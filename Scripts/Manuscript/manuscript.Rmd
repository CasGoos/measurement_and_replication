---
title             : "Measurement Reliability, Construct Validity, and Transparent Reporting in Original and Replication Research"
shorttitle        : "Measurement Reliability, Validity, and Reporting"

author: 
  - name          : Cas Goos
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Professor Cobbenhagenlaan 125, 5037 DB, Tilburg, The Netherlands"
    email         : "c.goos@tilburguniversity.edu"

  - name          : Marjan Bakker
    affiliation   : "1"

  - name          : Jelte M. Wicherts
    affiliation   : "1"

  - name          : Michèle B. Nuijten
    affiliation   : "1"


authornote: |
  The authors made the following contributions. CG: Conceptualization, Data curation, Formal Analysis, Investigation, Methodology, Project Administration, Software, Visualization, Writing - Original Draft Preparation, Writing - Review & Editing; MB: Conceptualization, Supervision, Writing - Review & Editing; JW: Conceptualization, Supervision, Writing - Review & Editing; MN: Conceptualization, Project Administration, Supervision, Validation, Writing - Review & Editing.


abstract: |
  To assess psychological phenomena we need reliable and valid measurement of the construct(s) of interest. Information should therefore be reported alongside the measure to enable the evaluation of reliability and validity. Additionally, to ensure that reliable and valid measures can be reused in future research, the measure should be documented in sufficient detail. Therefore, we investigated the measurement reporting in a sample of 77 measures within 56 Many Labs replications and related original articles (Ebersole et al., 2016, 2020; Klein et al., 2014, 2018). We found that the information relevant for the reuse of a measure was reported in full for around half the replication measures, and only 5.2% of the original studies. We also observed that only around a third of multi-item measures in original studies and 11.4% in replications reported a reliability coefficient, with comparable proportions for reporting any psychometric validity evidence. Besides reporting, we also performed checks of the reliability and construct validity using the openly available Many Labs item response data. We observed that the reliability and construct validity of the measures was rarely stable across contexts. These results corroborate existing findings that measurement reporting in published research lacks transparency, and that this transparency may in certain contexts cover up insufficient reliability and valdity. Finally, we offer suggestions on how to increase measurement reporting transparancy and the use of validated measures.



keywords          : "reliability, construct validity, measurement, reporting, transparent reporting, replications, psychology"
wordcount         : "225"

bibliography      : ["r-references.bib", "references.bib"]

floatsintext      : yes
linenumbers       : yes
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

header-includes:
  - | 
    \makeatletter
    \renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-1em}%
      {\normalfont\normalsize\bfseries\typesectitle}}
    
    \renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-\z@\relax}%
      {\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
    \makeatother
  - \renewcommand\author[1]{}
  - \renewcommand\affiliation[1]{}
  - \authorsnames[1, 1, 1, 1]{Cas Goos, Marjan Bakker, Jelte M. Wicherts, Michèle B. Nuijten}
  - \authorsaffiliations{{Department of Methodology and Statistics, Tilburg School of Social and Behavioral Sciences, Tilburg University, Tilburg, NL.}}

csl               : "vancouver.csl"
documentclass     : "apa7"

classoption       : man
output            : papaja::apa6_docx
knit              : worcs::cite_all
---

```{r setup, include = FALSE}
# loading R libraries
library(papaja)
library(worcs)
library(lavaan)
library(psych)
library(metafor)
library(forcats)
library(ggplot2)
library(grid)
library(patchwork)

# Code below loads all data. The raw data was loaded within the 'prepare_data.R script.
load_data()

# the code below removes the raw data to preserve disk space.
# If you want to rerun the data cleaning and preparations steps, DO NOT run
# this code.
# If you want to only rerun the analyses, YOU CAN run the code below.
rm(coded_data_initial_raw, coded_data_revised_raw, coded_data_vignette_raw,
   data_2.10.1, data_2.12, data_2.15, data_2.19.1, data_2.2, data_2.20, 
   data_2.23, data_2.3, data_2.4.1, data_2.4.2, data_2.8.2, data_3.5, data_5.1,
   data_5.4, data_5.5, data_5.7, data_5.9.1, data_ml1, data_ml3)

# creates a reference list for all used R packages and the installed R version 
# (does not include Rstudio)
r_refs("r-references.bib")
```

<!-- altering latex defaults to get better figure and table placement -->

\renewcommand{\arraystretch}{0.7}

<!-- reducing the line spacing within tables -->

\renewcommand{\topfraction}{.8}

<!-- max fraction of page for floats at top -->

\renewcommand{\bottomfraction}{.8}

<!-- max fraction of page for floats at bottom -->

\renewcommand{\textfraction}{.15}

<!-- min fraction of page for text -->

\renewcommand{\floatpagefraction}{.8}

<!-- min fraction of page that should have floats .66 -->

\setcounter{topnumber}{3} <!-- max number of floats at top of page -->

\setcounter{bottomnumber}{3} <!-- max number of floats at bottom of page -->

\setcounter{totalnumber}{4} <!-- max number of floats on a page -->

<!-- remember to use [htp] or [htpb] for placement -->

```{r analysis-preferences}
# Seed for random number generation
set.seed(26052025)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

```{r helper_functions}
# Function for storing data openly in the intermediate data folder
saving_to_intermediate_data <- function(data){
  open_data(data = data, filename = paste0(paste0(
    "Data/IntermediateData/", deparse(substitute(data))), ".csv"),
    codebook = NULL, value_labels = NULL) 
}

# Function for storing data openly in the analysis data folder
saving_to_analysis_data <- function(data){
  open_data(data = data, filename = paste0(paste0(
    "Data/AnalysisData/", deparse(substitute(data))), ".csv"),
    codebook = NULL, value_labels = NULL) 
}
```

<!-- Introduction -->

The challenge with measurement in quantitative psychological research is that unlike height or weight, psychological constructs cannot be directly measured, as they cannot be directly observed. Instead, we use scores from a measure as an indicator for a psychological construct. Yet, it cannot simply be assumed that any measure will form a good indicator of the constructs of interest. The measure has to be validated by establishing the reliability and construct validity of the measure. Following the definition of Crutzen and Peters, “[r]eliability refers to the consistency of scale scores when a construct is assessed multiple times or in multiple ways” [@crutzenScaleQualityAlpha2017b; pp. 244]. In turn, construct validity refers to whether the variance in the measured item scores between participants can be attributed to the variance in the unobserved construct(s) of interest between participants [@cronbachConstructValidityPsychological1955]. In practice, the measure itself is always validated within one or more use cases of a specific measurement procedure and the context in which that measurement took place, such as the sample characteristics, location, and time. 

However, psychological measures often lack reliability and construct validity evidence, either because the evidence is not reported, or because the measures are not validated at all [@beckmanHowReliableAre2004; @barryValidityReliabilityReporting2014; @flakeConstructValidationSocial2017; @maireadshawMeasurementPracticesLargescale2020; @flakeConstructValidityValidity2022]. Flake et al @flakeMeasurementSchmeasurementQuestionable2020 dubbed these such practices --- that raise doubts about the validity of a measurement --- “Questionable Measurement Practices” (QMPs), being a term analogous to Questionable Research Practices [QRPs; @simmonsFalsePositivePsychologyUndisclosed2011a; @johnMeasuringPrevalenceQuestionable2012a; @wichertsDegreesFreedomPlanning2016a]. A recent line of research has already shown QMPs to be relatively common in both original and replication psychology studies, damaging the trustworthiness and applicability of psychological research findings [@flakeConstructValidityValidity2022; @husseyAberrantAbundanceCronbachs2023].

The purpose of this study is to assess the measurement reporting practices of item-based measures in a diverse and seminal sample of original and replication studies in psychology. In addition, we will use the available item response data to calculate reliability and construct validity indices. Through the combination of assessing the measurement reporting and the calculated indices, we reflect on the apparent validity of the measurement based on the reported information, compared to the validity assessment when going beyond the reported information and checking the item response data.

## Measurement Reporting

One issue that has been observed with respect to measurement validation is that the reported measurement information is typically insufficient for readers to understand, evaluate, and reuse the measurement that was used. Flake et al @flakeConstructValidityValidity2022 documented the measurement reporting practices among 100 original psychology articles and their respective replications from the Reproducibility Project Psychology [@opensciencecollaborationEstimatingReproducibilityPsychological2015]. They coded the number of measures, the content of the measures, and information describing and justifying the measurement. They observed limited reporting of evidence demonstrating the reliability and validity of the measurement. Additionally, only eight of the 40 translated scales contained validity evidence for the translated version. A similar lack of reliability and validity reporting has been observed in other studies as well [@hoganEmpiricalStudyReporting2004; @flakeConstructValidationSocial2017]. If readers cannot infer the construct validity and reliability of a measure from the reported information, then readers lack the basic information needed to determine whether the measured variables are actually related to the constructs of interest. In turn, if we cannot determine whether the constructs of interest are captured in the measured variables or not, then the relation of the results of the statistical analysis using these variables and any psychological phenomena of interest is also tenuous. In short, if the reported information does not show that the measurement was valid, then any substantive conclusions from the data are left unsupported.

Further findings by Flake et al @flakeConstructValidityValidity2022 and others [@flakeConstructValidationSocial2017; @maireadshawMeasurementPracticesLargescale2020] show that measurement reporting was not only limited in terms of the reliability and validity evidence. A lack of clearly reported information was also observed for basic content descriptions such as the number of items, the response format, and the scoring of the scale. This creates challenges for future researchers who want to reuse the measure to assess the same construct. Specifically, replication studies attempting to reconstruct the measurement from these incomplete descriptions may end up with a measurement that assesses the constructs in a substantially different way from the original study, or even assess different constructs altogether. If different constructs are assessed, the replication cannot be seen as a test of the same phenomenon as in the original study. If the same constructs are assessed, but in a substantially different way, the estimated effects in the replication cannot be easily compared to the effects in the original study. In either case, substantial comparisons between original and replication are hindered.

Our first aim is to extend the existing findings on measurement reporting practices with a descriptive account of these practices for the Many Labs replications and the related original studies. We will evaluate to what extent the reporting of item-based scales in our sample are transparent enough to facilitate the evaluation and reuse of these measurements. The Many Labs replications are a series of large-scale collaborations, in which multiple labs across the world directly replicated classic and contemporary psychological studies [@kleinInvestigatingVariationReplicability2014a; @ebersoleManyLabs32016; @kleinManyLabs22018b; @ebersoleManyLabs52020d; @kleinManyLabs42022]. The original studies chosen for replication in the Many Labs were not only picked based on feasibility, but importantly for us the sample of effects to replicate was chosen to contain a diverse range of seminal effects. Furthermore, because the Many Labs projects used preregistered and documented structured protocols, we expected the measurement reporting for the replications to represent a high standard within the field. Any issues in measurement reporting here might suggest that other replications could face similar or greater challenges.

## Reliability

Two key aspects of measurement quality are reliability and construct validity. Psychometrics offers various methods for assessing these properties [@nunnallyOverviewPsychologicalMeasurement1978; @mellenberghConceptualIntroductionPsychometrics2011]. The results of such psychometric analyses often serve as essential evidence for evaluating a measure’s validity. 

Reliability serves as a pre-requisite for a valid measure in most cases, because a measure for which a participant’s response is not consistent, cannot capture a stable construct [Kaplan and Saccuzzo (2013, pp. 154–155)]. It is therefore concerning that --- as mentioned earlier --- the reporting of psychometric indicators of reliability is not common, and is often limited to Cronbach’s Alpha. Unfortunate, because Cronbach’s Alpha comes with strong assumptions that are rarely tested [@crutzenScaleQualityAlpha2017b]. Furthermore, research by Hussey et al @husseyAberrantAbundanceCronbachs2023 has shown evidence that the reporting of reliability may not only be uncommon, but also biased. They observed a disproportional number of Cronbach’s Alpha values at the common acceptably reliability threshold value of .70, and relatively low reporting below .70. Thus, the lack of reported measurement evidence may indicate psychometric skeletons hiding in the closet.

As a result, if we want to evaluate the reliability of a measure in our sample, we cannot rely on the reported information alone to paint us a representative picture. Therefore, we also evaluated the measurement reliability within the Many Labs original and replication studies by estimating it from the item response data. The data on the item responses are openly available per lab for our sample. Therefore, we can evaluate not only the reliability per measure, but also the variation in reliability across labs. The variation is relevant because Cronbach’s Alpha is an indicator of reliability within a particular sample and not of the reliability of the measure in general, as it is proportional to the total variance in the target variable in the sample. Still, many researchers report and interpret Cronbach’s Alpha as a universal quality of the measure [@cortinaWhatCoefficientAlpha1993; @schmittUsesAbusesCoefficient1996]. @maireadshawMeasurementPracticesLargescale2020 have already observed that for Many Labs 2 the overall sample Cronbach’s Alpha level across scales was below .5, a degree of reliability far below what many would consider acceptable for most research purposes. Our second research aim is therefore to extend these findings by empirically evaluating both the degree as well as the variation of reliability of the measures in Many Labs projects 1, 2, 3, and 5.

## Construct Validity and Unidimensionality

Besides reliability, construct validity is key to establish for any measure. A multi-stage process is required to statistically and substantially establish a link between the measurement and the construct of interest. For example, the way a measure's items relate to each other and to the underlying concept (the factor relations) can differ substantially across contexts, a concept known as measurement invariance [@hornPracticalTheoreticalGuide1992; @cheungDirectComparisonApproach2012]. Thus, we cannot assume that the measure is valid in each context it is used in or meant to be used in, it is something that has to be established for each context. Besides investigating the factor structure, the measure should also be backed up substantively and logically [@borgstedeMeaningfulMeasurementRequires2023]. However, what we typically observe in psychological research is that measures are rarely substantiated with validity evidence [@beckmanHowReliableAre2004; @barryValidityReliabilityReporting2014; @flakeConstructValidationSocial2017; @flakeConstructValidityValidity2022]. This may in part be because the sample sizes required in validating a scale can often be larger than many studies are able to obtain [@maccallumSampleSizeFactor1999, @gorsuchFactorAnalysis2013]. Alternatively, researchers may not recognize measurement validation as part of their research method. Yet what has also been observed in the literature is that many measures are created ad hoc and reused maybe once or not at all [@weidmanJingleJangleEmotion2017; @elsonPsychologicalMeasuresArent2023; @anvariFragmentedFieldConstruct2025; @anvariDefragmentingPsychology2025]. This makes it extremely unlikely that the validity of these measure will ever be assessed at all, let alone be thoroughly investigated by analyzing measurement invariance across multiple groups using substantial sample sizes [@meadePowerPrecisionConfirmatory2007; @frenchMeasurementInvarianceTechniques2016; @koziolImpactModelParameterization2018].

One preliminary psychometric check that could easily be conducted is to see if the number of constructs measured by the items of the measurement match the intended number of constructs. Most often the scores on a measure or subscales of a measure are aggregated and interpreted as a singular index. In that case, the measure should be unidimensional. Otherwise, the researcher is interpreting a set of scores to be one concept when in fact they are multiple distinct concepts. Additionally, most reliability indicators --- including Cronbach’s Alpha --- cannot properly estimate true variance when it is spread across distinct constructs, and will thus give an inaccurate estimate of reliability [@schmittUsesAbusesCoefficient1996; @crutzenScaleQualityAlpha2017b]. 

Mairead shaw et al @maireadshawMeasurementPracticesLargescale2020 checked the dimensionality of the measurements from Many Labs 2 [@kleinManyLabs22018b], and while some support for unidimensionality was present, they found that none of the scales in their sample met all of their criteria for unidimensionality. Because the Many Labs replications reused existing measures across contexts, Mairead shaw et al @maireadshawMeasurementPracticesLargescale2020 could assess validity across contexts. Our third research aim is to extend these findings. We will perform our own checks of unidimensionality on measures from Many Labs 1, 2, 3, and 5 per lab, and include a meta-analytic assessment of the variation in unidimensionality across labs. The combined goal of our three research aims is to provide a descriptive account of the reliability, construct validity, and reporting of measurement in the included Many Labs study pairs. 

# Disclosures

## Preregistration

We preregistered data collection, coding protocol, and planned analyses: <https://osf.io/jgxyu>. We deviated from our coding protocol as is explained further in the text. The results from our preregistered analyses are described in [Supplementary Analyses A](../../SupplementaryMaterials/SupplementaryAnalysesScripts/Supplementary_exploratory_version_pre-reg_analyses.Rmd). In the main text, we focus on the descriptive results.

## Data, Materials, and Online Resources

This manuscript was created in RStudio [*v`r rstudioapi::versionInfo()$version`*; @R-Rstudio] with R Version `r paste0(R.Version()$major, ".", R.Version()$minor)` [@R-base], and generated using the Workflow for Open Reproducible Code in Science [*v`r getNamespaceVersion("worcs")[[1]]`*; @R-worcs] to ensure reproducibility and transparency. All code and data used to generate this manuscript and its results are available at: <https://github.com/CasGoos/measurement_and_replication> and <https://osf.io/9r8yt/> (DOI: 10.17605/OSF.IO/9R8YT).

## Reporting

We report how we determined all data exclusions, all manipulations, and all measures in the study. Our sample size was predetermined by the number of studies in the Many Labs projects.

## Ethical Approval

This research was approved by the Tilburg University School of Social and Behavioral Sciences Ethical Review Board (nr. TSB_TP_REMA06).

# Method

## Data Source
The data consists of three main sources: replication datasets, replication protocols, and original study articles. We retrieved the data of Many Labs 1, 2, 3, & 5 [@kleinInvestigatingVariationReplicability2014a; @ebersoleManyLabs32016; @kleinManyLabs22018b; @ebersoleManyLabs52020d] from their respective OSF pages. Many Labs 4 [@kleinManyLabs42022] was excluded, as there were no publicly available replication protocols. Additionally, we excluded the replication of [@crosbyWhereWeLook2008a] in Many Labs 5, as it made use of videos and eye-tracking measures, which did not match this study’s focus on item-based measures.

## Unit of Analysis

Our unit of analysis is a measure of a single variable within a replication protocol or original article that was used in the main analysis that was replicated. For example, if conscientiousness and agreeableness were both measured using the Big 5 Personality Test, each of which was a variable in a replicated effect, then both the conscientiousness and agreeableness part of that questionnaire would be coded as their own unit of analysis. We also allowed for multiple variables to be measured per study. We used the replication protocols to identify the measure of each variable. We did not include acquiescence bias checks, manipulation checks, pilot test measures, and measures added for exploratory analyses. Our final sample size was `r nrow(coded_data_replications)` measures of unique variables for both original and replication studies. Initially, the original articles contained 3 more measures of unique variables than the replication protocols. This difference was due to the way that the moral foundations questionnaire was framed in the original article compared to in the replication protocol. In the original article it was framed as measuring five different moral foundations, while in the replication protocol the measure assessed the two overarching categories that were used to test the main effect in both the original and replication research. The measurement information reported was comparable across all five categories, and thus it was deemed that the measurement could be reduced to reflect two overarching categories to facilitate easier comparison between measurement in original and replication.

## Data Collection

The data on the preregistered replication protocols, and replication datasets from Many Labs 1, 2, 3, & 5 were all retrieved from their respective OSF pages: <https://osf.io/wx7ck/>, <https://osf.io/8cd4r/>, <https://osf.io/ct89g/>, & <https://osf.io/7a6rd/>. We skimmed both the replication protocols and replication datasets to ensure that they were the correct files to code measurement reporting information from. However, no coding or analysis of either of them had taken place before the analyses were preregistered. Further details on the search strategy can be found in the [coding protocol information file](../../SupplementaryMaterials/CodingProtocols/coding_protocol_information.Rmd) in the supplementary materials.

### Replication Datasets

The replication datasets refer to the publicly available datasets containing the data obtained from all labs of each Many Labs replication. For the analyses, we extracted the scores on the items of each previously identified measure that also met our inclusion criteria specified in the paragraph below. When scores could not be clearly identified based on the information available in the dataset and the replication protocol, any available codebooks, analysis scripts, or study materials were used to identify the relevant scores.

To be included as part of the subset of measures we analyzed in our reliability recalculations and factor analyses, the measure had to be a scale of multiple items. If cleaned data were available, we chose these over raw data, to ensure that variables were coded as intended (e.g., no reverse-coded items). We omitted pilot data from the analyses. These criteria combined with our inability to definitively determine which variables in the dataset corresponded to the items on the measure of interest resulted in a set of item score data from `r length(unique(calculated_reliability_lab_data$g))` replication sets spread across on average approximately `r round(mean(table(calculated_reliability_lab_data$g)), 0)` lab locations for our analyses.

```{r CleaningReplicationDatasetsData, include = FALSE, eval = FALSE}
##### This code can be used to rerun the data preparation to convert the raw
##### (input) data of the Many Labs replications into intermediate data.
##### However, because the intermediate data is also available in this project,
##### the manuscript can also be reproduced without running this code block.

### Below we extract the relevant data from the Many Labs' datasets. The first
### number in each dataset refers to the Many Labs project it is related to.
### The second number to which study the data is from in order of appearance in
### the Many Labs pre-registered protocols, or in the case of Many Labs 5, 
### within the OSF folder structure. If there is a third number, the study had 
### multiple relevant measures this refers to the order of appearance of the 
### measure where the data is from within the study's description in the Many 
### Labs protocol or OSF folder structure. A fourth number indicates which part
### of the data of this measure was taken, in case the measure assessed multiple
### constructs, as these were treated separately for some analyses.

## ML 1
# 1.3
data_1.3_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[22:29])
colnames(data_1.3_clean)[1] <- "g"
# 1.10
data_1.10_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[108:115])
colnames(data_1.10_clean)[1] <- "g"
# 1.11
data_1.11_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[73:76])
colnames(data_1.11_clean)[1] <- "g"
# 1.12.1
# not found
# 1.12.3
data_1.12.3.1_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[54:59])
colnames(data_1.12.3.1_clean)[1] <- "g"
data_1.12.3.2_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[60:65])
colnames(data_1.12.3.2_clean)[1] <- "g"

## ML 2
# 2.2
data_2.2_clean <- cbind(as.factor(data_2.2[[5]]), data_2.2[6:11])
colnames(data_2.2_clean)[1] <- "g"
# 2.3
# data does not appear suitable
# 2.4.1
data_2.4.1_clean <- cbind(as.factor(data_2.4.1[[5]]), data_2.4.1[6:11])
colnames(data_2.4.1_clean)[1] <- "g"
# 2.4.2
data_2.4.2_clean <- cbind(as.factor(data_2.4.2[[5]]), data_2.4.2[6:14])
colnames(data_2.4.2_clean)[1] <- "g"
# 2.8.2
data_2.8.2_clean <- cbind(as.factor(data_2.8.2[[6]]), data_2.8.2[9:13])
colnames(data_2.8.2_clean)[1] <- "g"
# 2.10.1
data_2.10.1_clean <- cbind(as.factor(data_2.10.1[[5]]), data_2.10.1[6:11])
colnames(data_2.10.1_clean)[1] <- "g"
# 2.12.1
data_2.12.1_clean <- cbind(as.factor(data_2.12[[5]]), data_2.12[c(6,7,8,9,10,31,32,33,34,35)]) 
data_2.12.1_clean[3465:6905,2:6] <- data_2.12.1_clean[3465:6905,7:11]
data_2.12.1_clean <- data_2.12.1_clean[1:6]
colnames(data_2.12.1_clean)[1] <- "g"
# 2.12.2
data_2.12.2_clean <- cbind(as.factor(data_2.12[[5]]), data_2.12[c(11,14,15,18,19,22,24,27,28,29,36,39,40,43,44,47,49,52,53,54)]) 
data_2.12.2_clean[3465:6905,2:11] <- data_2.12.2_clean[3465:6905,12:21]
data_2.12.2_clean <- data_2.12.2_clean[1:11]
colnames(data_2.12.2_clean)[1] <- "g"
# 2.12.3
data_2.12.3_clean <- cbind(as.factor(data_2.12[[5]]), data_2.12[c(12,13,16,17,20,21,23,25,26,30,37,38,41,42,45,46,48,50,51,55)]) 
data_2.12.3_clean[3465:6905,2:11] <- data_2.12.3_clean[3465:6905,12:21]
data_2.12.3_clean <- data_2.12.3_clean[1:11]
colnames(data_2.12.3_clean)[1] <- "g"
# 2.15
data_2.15_clean <- cbind(as.factor(data_2.15[[5]]), data_2.15[8:12])
colnames(data_2.15_clean)[1] <- "g"
# 2.19.1
# difficult to extract
# 2.19.2
# difficult to extract
# 2.20
data_2.20_clean <- cbind(as.factor(data_2.20[[5]]), data_2.20[6:45]) 
data_2.20_clean[3729:7396,2:21] <- data_2.20_clean[3729:7396,22:41]
data_2.20_clean <- data_2.20_clean[1:21] 
# coding so all 1's means somebody used rule-based grouping strategy
data_2.20_clean[,c(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)] <- ifelse(data_2.20_clean[,c(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)] == 1, 1, 0)
data_2.20_clean[,c(3, 5, 7, 9, 11, 13, 15, 17, 19, 21)] <- ifelse(data_2.20_clean[,c(3, 5, 7, 9, 11, 13, 15, 17, 19, 21)] == 2, 1, 0)
colnames(data_2.20_clean)[1] <- "g"
# 2.23
data_2.23_clean <- cbind(as.factor(data_2.23[[5]]), data_2.23[c(7,8,12,13,15)])
colnames(data_2.23_clean)[1] <- "g"


## ML 3
# 3.2.1
data_3.2.1 <- cbind(as.factor(data_ml3[[1]]), data_ml3[77:86] - 1)
data_3.2.1.1_clean <- na.omit(data_3.2.1[1:6])
colnames(data_3.2.1.1_clean)[1] <- "g"
data_3.2.1.2_clean <- na.omit(data_3.2.1[c(1, 7:11)])
colnames(data_3.2.1.2_clean)[1] <- "g"
# 3.5
# data appears unusable
# 3.7.1
data_3.7.1_clean <- na.omit(cbind(as.factor(data_ml3[[1]]), data_ml3[38:42]))
colnames(data_3.7.1_clean)[1] <- "g"
# 3.7.2
data_3.7.2_clean <- na.omit(cbind(as.factor(data_ml3[[1]]), data_ml3[89:94]))
colnames(data_3.7.2_clean)[1] <- "g"
# 3.8.1
# a single measure was reported
# 3.8.2
data_3.8.2_clean <- na.omit(cbind(as.factor(data_ml3[[1]]), data_ml3[29:30])) 
colnames(data_3.8.2_clean)[1] <- "g"


## ML 5
# 5.1.1
data_5.1.1_clean <- cbind(as.factor(data_5.1[[2]]), data_5.1[13:27])
colnames(data_5.1.1_clean)[1] <- "g"
# 5.1.2
data_5.1.2_clean <- cbind(as.factor(data_5.1[[2]]), data_5.1[28:33])
colnames(data_5.1.2_clean)[1] <- "g"
# 5.4
data_5.4_clean <- cbind(as.factor(data_5.4[[1]]), data_5.4[18:41])
colnames(data_5.4_clean)[1] <- "g"
# 5.5.1 & 5.5.2
# from this dataset it appears that this data will be difficult to use.
# 5.5.2
# also difficult to use
# 5.7 
data_5.7_clean <- cbind(as.factor(data_5.7[[3]]), data_5.7[c(25, 34, 35, 36, 37, 38, 39, 40, 41, 42)])
colnames(data_5.7_clean)[1] <- "g"
# 5.9.1
data_5.9.1_clean <- na.omit(cbind(as.factor(data_5.9.1[[4]]), data_5.9.1[c(79, 83, 87, 91, 95, 98, 101)]))
colnames(data_5.9.1_clean)[1] <- "g"


### Saving to Intermediate Data Folder
saving_to_intermediate_data(data_1.3_clean)
saving_to_intermediate_data(data_1.10_clean)
saving_to_intermediate_data(data_1.11_clean)
saving_to_intermediate_data(data_1.12.3.1_clean)
saving_to_intermediate_data(data_1.12.3.2_clean)
saving_to_intermediate_data(data_2.10.1_clean)
saving_to_intermediate_data(data_2.12.1_clean)
saving_to_intermediate_data(data_2.12.2_clean)
saving_to_intermediate_data(data_2.12.3_clean)
saving_to_intermediate_data(data_2.15_clean)
saving_to_intermediate_data(data_2.20_clean)
saving_to_intermediate_data(data_2.23_clean)
saving_to_intermediate_data(data_3.2.1.1_clean)
saving_to_intermediate_data(data_3.2.1.2_clean)
saving_to_intermediate_data(data_3.7.1_clean)
saving_to_intermediate_data(data_3.7.2_clean)
saving_to_intermediate_data(data_3.8.2_clean)
saving_to_intermediate_data(data_5.1.1_clean)
saving_to_intermediate_data(data_5.1.2_clean)
saving_to_intermediate_data(data_5.4_clean)
saving_to_intermediate_data(data_5.7_clean)
saving_to_intermediate_data(data_5.9.1_clean)

```

### Replication Protocols

The replication protocols refer to the publicly available protocols describing the background, methodology, and analysis of the replication of an original study. These were retrieved from the Open Science Framework (OSF) pages of the Many Labs projects (the search strategy and OSF file locations can be found in the [data retrieval information](../../SupplementaryMaterials/data_retrieval_information.Rmd) supplementary document; URL <https://github.com/CasGoos/measurement_and_replication/blob/master/SupplementaryMaterials/data_retrieval_information.Rmd>).

### Original Articles

We identified and retrieved all original study articles using the citations for these articles in each replication protocol.

```{r CleaningCodedData, include = FALSE, eval = FALSE}
##### This code can be used to rerun the data preparation to convert the raw 
##### (input) data of the coded data into intermediate data. However, because 
##### the intermediate data is also available in this project, the manuscript  
##### can also be reproduced without running this code block.

# Selecting the relevant rows and columns for the data
coded_data_initial_sel <- coded_data_initial_raw[3:160, 18:57]
coded_data_revised_sel <- coded_data_revised_raw[3:160, 18:38]
coded_data_vignette_sel <- coded_data_vignette_raw[3:160, 2]

# Combining the datasets
coded_data_full <- cbind(coded_data_initial_sel, 
                         cbind(coded_data_revised_sel, coded_data_vignette_sel))

# filtering out unnecessary double columns
coded_data_full <- cbind(coded_data_full[, 1:40], coded_data_full[, 45:62])


### data preparation
# renaming columns
colnames(coded_data_full) <- c("many_labs_version", "rep_org", "title", "measure_name", 
      "variable_name", "multi", "variable_order", "N", "N_items", 
      "hypothesis_support", "reliability_type", "reliability_type_text", 
      "reliability_coeff", "def_1", "op_version", "op_1", "op_2", "op_3", "op_4", 
      "op_5", "sel_existing", "sel_existing_text", "sel_1", "sel_2", "sel_3", 
      "sel_4", "sel_psychometric_evidence", "sel_psychometric_evidence_text", 
      "quant_1", "quant_2", "quant_3", "quant_4", "mod_check", "mod_1", "mod_2", 
      "mod_3", "mod_4", "mod_5", "mod_6", "mod_time", "op_1_REV", "op_2_REV",
      "op_5_REV", "sel_1_REV", "sel_3_REV", "sel_psychometric_evidence_REV", 
      "sel_psychometric_evidence_text_REV", "quant_1_REV", "quant_2_REV", 
      "quant_3_REV", "mod_check_REV", "mod_1_REV", "mod_2_REV", "mod_3_REV", 
      "mod_4_REV", "mod_5_REV", "mod_6_REV", "inseperable_material")
  
# renaming rows
rownames(coded_data_full) <- 1:nrow(coded_data_full)

# fixing some coding mistakes
coded_data_full$variable_name[79] <- "quote attribution effect"
coded_data_full$N[3] <- "5284"
coded_data_full$N[158] <- "1202"
coded_data_full$reliability_type[50] <- "Not Reported"
coded_data_full$reliability_type[127] <- "Not Reported"
coded_data_full$op_1[145] <- "False"
coded_data_full$op_3[121] <- "True"
coded_data_full$op_5[157] <- "True"
coded_data_full$sel_1[59] <- "True"
coded_data_full$quant_2[113] <- "True"
coded_data_full$mod_time[1] <- "Before"
coded_data_full$psychometric_evidence_text_REV[coded_data_full$sel_psychometric_evidence_text_REV == "convergent validitiy"] <- "convergent validity"


# removing missing entry 77
coded_data_full <- data.frame(coded_data_full)[-77,]

# Many labs 2.25 and 2.26, as well as 3.4 and 3.5 (for replications) were coded 
# in reverse order thus need to be swapped in right order. Additionally, some
# of the entries were included later than following their order, due to some
# minor coding oversights.
Coded_Data_Full_Restructured <- coded_data_full[c(1:18, 148, 19:23, 147, 24:28, 
                                                  153, 29:40, 154, 42, 41, 155, 
                                                  43:49, 156, 51, 50, 52:76, 
                                                  157, 77:88, 149:152, 89:146),]


# rename the rownames to match the new order
rownames(Coded_Data_Full_Restructured) <- 1:nrow(Coded_Data_Full_Restructured)

# changing the variable types for each column to better represent their 
# intended variable type
class(Coded_Data_Full_Restructured$many_labs_version) <- "numeric"
Coded_Data_Full_Restructured$many_labs_version <- as.factor(Coded_Data_Full_Restructured$many_labs_version)
Coded_Data_Full_Restructured$rep_org <- droplevels(as.factor(Coded_Data_Full_Restructured$rep_org))
Coded_Data_Full_Restructured$multi <- droplevels(as.factor(Coded_Data_Full_Restructured$multi))
Coded_Data_Full_Restructured$variable_order <- droplevels(as.factor(Coded_Data_Full_Restructured$variable_order))
class(Coded_Data_Full_Restructured$N) <- "numeric"
Coded_Data_Full_Restructured$N_items <- droplevels(as.factor(Coded_Data_Full_Restructured$N_items))
Coded_Data_Full_Restructured$hypothesis_support <- droplevels(as.factor(Coded_Data_Full_Restructured$hypothesis_support))
levels(Coded_Data_Full_Restructured$hypothesis_support) <- c("No", "Unclear", "Yes")
Coded_Data_Full_Restructured$reliability_type <- droplevels(as.factor(Coded_Data_Full_Restructured$reliability_type))
class(Coded_Data_Full_Restructured$reliability_coeff) <- "numeric"
Coded_Data_Full_Restructured$def_1 <- as.logical(Coded_Data_Full_Restructured$def_1)
Coded_Data_Full_Restructured$op_1 <- as.logical(Coded_Data_Full_Restructured$op_1)
Coded_Data_Full_Restructured$op_2 <- as.logical(Coded_Data_Full_Restructured$op_2)
Coded_Data_Full_Restructured$op_3 <- as.logical(Coded_Data_Full_Restructured$op_3)
Coded_Data_Full_Restructured$op_4 <- as.logical(Coded_Data_Full_Restructured$op_4)
Coded_Data_Full_Restructured$op_5 <- as.logical(Coded_Data_Full_Restructured$op_5)
Coded_Data_Full_Restructured$sel_existing <- droplevels(as.factor(Coded_Data_Full_Restructured$sel_existing))
Coded_Data_Full_Restructured$sel_1 <- as.logical(Coded_Data_Full_Restructured$sel_1)
Coded_Data_Full_Restructured$sel_2 <- as.logical(Coded_Data_Full_Restructured$sel_2)
Coded_Data_Full_Restructured$sel_3 <- as.logical(Coded_Data_Full_Restructured$sel_3)
Coded_Data_Full_Restructured$sel_4 <- as.logical(Coded_Data_Full_Restructured$sel_4)
Coded_Data_Full_Restructured$sel_psychometric_evidence <- droplevels(as.factor(Coded_Data_Full_Restructured$sel_psychometric_evidence))
Coded_Data_Full_Restructured$quant_1 <- as.logical(Coded_Data_Full_Restructured$quant_1)
Coded_Data_Full_Restructured$quant_2 <- as.logical(Coded_Data_Full_Restructured$quant_2)
Coded_Data_Full_Restructured$quant_3 <- as.logical(Coded_Data_Full_Restructured$quant_3)
Coded_Data_Full_Restructured$quant_4 <- as.logical(Coded_Data_Full_Restructured$quant_4)
Coded_Data_Full_Restructured$mod_check <- droplevels(as.factor(Coded_Data_Full_Restructured$mod_check))
Coded_Data_Full_Restructured$mod_1 <- as.logical(Coded_Data_Full_Restructured$mod_1)
Coded_Data_Full_Restructured$mod_2 <- as.logical(Coded_Data_Full_Restructured$mod_2)
Coded_Data_Full_Restructured$mod_3 <- as.logical(Coded_Data_Full_Restructured$mod_3)
Coded_Data_Full_Restructured$mod_4 <- as.logical(Coded_Data_Full_Restructured$mod_4)
Coded_Data_Full_Restructured$mod_5 <- as.logical(Coded_Data_Full_Restructured$mod_5)
Coded_Data_Full_Restructured$mod_6 <- as.logical(Coded_Data_Full_Restructured$mod_6)
Coded_Data_Full_Restructured$mod_time <- droplevels(as.factor(Coded_Data_Full_Restructured$mod_time))
Coded_Data_Full_Restructured$op_1_REV <- as.logical(Coded_Data_Full_Restructured$op_1_REV)
Coded_Data_Full_Restructured$op_2_REV <- as.logical(Coded_Data_Full_Restructured$op_2_REV)
Coded_Data_Full_Restructured$op_5_REV <- as.logical(Coded_Data_Full_Restructured$op_5_REV)
Coded_Data_Full_Restructured$sel_1_REV <- as.logical(Coded_Data_Full_Restructured$sel_1_REV)
Coded_Data_Full_Restructured$sel_3_REV <- as.logical(Coded_Data_Full_Restructured$sel_3_REV)
Coded_Data_Full_Restructured$sel_psychometric_evidence_REV <- droplevels(as.factor(Coded_Data_Full_Restructured$sel_psychometric_evidence_REV))
Coded_Data_Full_Restructured$quant_1_REV <- as.logical(Coded_Data_Full_Restructured$quant_1_REV)
Coded_Data_Full_Restructured$quant_2_REV <- as.logical(Coded_Data_Full_Restructured$quant_2_REV)
Coded_Data_Full_Restructured$quant_3_REV <- as.logical(Coded_Data_Full_Restructured$quant_3_REV)
Coded_Data_Full_Restructured$mod_check_REV <- droplevels(as.factor(Coded_Data_Full_Restructured$mod_check_REV))
Coded_Data_Full_Restructured$mod_1_REV <- as.logical(Coded_Data_Full_Restructured$mod_1_REV)
Coded_Data_Full_Restructured$mod_2_REV <- as.logical(Coded_Data_Full_Restructured$mod_2_REV)
Coded_Data_Full_Restructured$mod_3_REV <- as.logical(Coded_Data_Full_Restructured$mod_3_REV)
Coded_Data_Full_Restructured$mod_4_REV <- as.logical(Coded_Data_Full_Restructured$mod_4_REV)
Coded_Data_Full_Restructured$mod_5_REV <- as.logical(Coded_Data_Full_Restructured$mod_5_REV)
Coded_Data_Full_Restructured$mod_6_REV <- as.logical(Coded_Data_Full_Restructured$mod_6_REV)
Coded_Data_Full_Restructured$inseperable_material <- droplevels(as.factor(Coded_Data_Full_Restructured$inseperable_material)

                                                                
                                                                

# the moral foundations questionnaire in original 2.4 is reported using all 5 of 
# its factors, whereas in replication 2.4 only the two overarching groups of 
# binding and individualizing foundations are described. For that reason a 
# shortened original dataset will be used for any direct comparisons between 
# original and replication coding.

Coded_Data_Full_Shortened <- Coded_Data_Full_Restructured[c(1:94, 96, 99:157),] 
Coded_Data_Full_Shortened[c(95,96),5] <- c("individualizing moral foundations", "binding moral foundations")
Coded_Data_Full_Shortened[95,13] <- NA
Coded_Data_Full_Shortened[96,13] <- NA


# separating the replication and original data from each other.
coded_data_replications <- Coded_Data_Full_Shortened[1:77,]
coded_data_original <- Coded_Data_Full_Shortened[78:154,]

# exporting cleaned data
saving_to_intermediate_data(coded_data_replications)
saving_to_intermediate_data(coded_data_original)

```

## Measures

### Measurement Reporting

We evaluated the transparency of the measurement reporting practices within the original articles and replication protocols using our preregistered coding protocol containing items assessing twenty three reporting practices. The items were based on Table \@ref(tab:QMPCodingInfoTable) presented in Flake et al @flakeMeasurementSchmeasurementQuestionable2020, listing what measurement information to report. We coded an item as “true” if the relevant measurement information was clearly reported, “false” if it was missing or unclear, and “not applicable” if it was irrelevant for that measure (e.g., reporting factor analysis results for single-item measures). These items thus indicate transparent reporting practices, which can be seen as the opposite of QMPs. We grouped the items into five categories, each representing a different element in measurement reporting. Example items are listed in Table \@ref(tab:QMPCodingInfoTable).

```{r QMPCodingInfoTable, warning = FALSE}
QMP_info_dataframe <- data.frame(Category = c("Definition", "", "", "Operationalisation", "", "", "", "", "Selection/Creation", "", "", "Quantification", "Modification", "", "", "", "", ""),
           'N Items' = c("1", "", "", "5", "", "", "", "", "7", "", "", "4", "6", "", "", "", "", ""),
           'Example Item' = c("A psychological/sociological definition",
            "is given to the name of the measured",
            "variable within the paper.", 
            "The administration format (pen-and-",
            "paper/computer) and environment (in",
            "public/in a lab) are described (Note:",
            "both should be present for this item to be", 
            "rated as “true”).", 
            "The source of the scale is provided",
            "(in case the scale was newly developed",
            "this should be clearly stated).", 
            "The number of items are described.", 
            "Any format changes are mentioned",
            "(paper-and-pencil <–> computer), if no",
            "changes were made to the format, and",
            "this was mentioned then code as No",
            "modification. If it is not clear, then code",
            "as False."))

# making the column names look less robot speak-y.
colnames(QMP_info_dataframe) <- c("Category", "N Items", "Example Item")


# transfer the data to an APA table for printing
apa_table(
  QMP_info_dataframe, align = c("l", "r", "l")
  , caption = "Example items used to code transparent measurement reporting practices across five categories linking to different aspects of measurement"
  , note = "Selection and creation of a measure share a category because the justifications and requirements in selecting a measure are similar to those for creating a new measure."
  , escape = FALSE, placement = "htp", booktabs = TRUE)

```

Included within the list of measurement reporting information is crucially also the reported reliability coefficient and type of index (Cronbach’s Alpha, test-retest correlation, inter-rater reliability coefficient, etc.) when present. As well as the presence of any psychometric construct validity evidence for the measure, such as the results from a factor analysis.

After the initial coding, we made minor alterations from the preregistered coding protocol for fourteen of the twenty three measurement reporting practice items. These items were changed after familiarizing ourselves with the way measurement was reported, and it became clear that our criteria were too stringent. For example, in the initial protocol, an example item of the measure had to be present within the article or protocol itself, for the measurement practice to be considered clearly reported. In the revised protocol, references to online appendices with example items were also considered sufficient for this item. The analyses, tables, and figures presented in this article are all based on the revised coding protocol. The equivalent measurement reporting descriptives obtained with the initial protocol can be found in [Supplementary Analyses B](../../SupplementaryMaterials/SupplementaryAnalysesScripts/Supplementary_initial_QMP_ratio_table.rmd).

We initially intended to construct an index from the transparent measurement reporting items and perform regression analyses on this index and associated replication outcomes. However, this index could not be validated properly, due to a lack of reported information and because we do not have any theorized outcome that could function as a suitable index for predictive validity. Results based on this index would be misleading to present here in the main article. A more detailed explanation of what was omitted and why, as well as the results from these preregistered analyses can be found in [Supplementary Analyses A](../../SupplementaryMaterials/SupplementaryAnalysesScripts/Supplementary_exploratory_version_pre-reg_analyses.Rmd).

```{r Calculating_QMP_Data, include = FALSE, eval = FALSE}
# function for changing QMP data to their ratio equivalent
Create_QMP_descriptive_case <- function(practice_name, original_coded_var, replication_coded_var){
  # add practice name to first row
  Practice <- practice_name
  
  # Sum all QMP/GMP (Good Measurement Practice), keeping NA items out of
  # the equation
  original_n_QMPs <- sum(original_coded_var == "FALSE", na.rm = TRUE)
  replication_n_QMPs <- sum(replication_coded_var == "FALSE", na.rm = TRUE)
  
  original_n_GMPs <- sum(original_coded_var == "TRUE", na.rm = TRUE)
  replication_n_GMPs <- sum(replication_coded_var == "TRUE", na.rm = TRUE)
  
  # calculating the N of applicable items for originals
  N_applicable_original <- original_n_QMPs + original_n_GMPs
  
  # checking that we are not dividing by 0, and then calculating the ratio of 
  # QMPs for applicable items for originals
  if(N_applicable_original != 0){
    QMP_percentage_original <- round(original_n_QMPs / (N_applicable_original), 2)
  } else{
    QMP_percentage_original <- 0
  }                               
  
  # calculating the N of applicable items for replications
  N_applicable_replication <- replication_n_QMPs + replication_n_GMPs
  
  # checking that we are not dividing by 0, and then calculating the ratio of 
  # QMPs for applicable items for replications
  if(N_applicable_replication != 0){
    QMP_percentage_replication <- round(replication_n_QMPs / N_applicable_replication, 2)
  } else{
    QMP_percentage_replication <- 0
  }     
  
  # calculating the Phi coefficient to estimate the relation between variables
  Phi <- phi(matrix(c(original_n_QMPs, original_n_GMPs, replication_n_QMPs, 
                      replication_n_GMPs), nrow = 2, byrow = TRUE))
  
  return(c(Practice, QMP_percentage_original, N_applicable_original, QMP_percentage_replication, N_applicable_replication, Phi))
}



# loading the relevant items
original_QMP_data <- coded_data_original[c("def_1", "sel_2", "op_4", "reliability_type", "sel_existing", "op_version", "op_1_REV", "sel_1_REV", "sel_3_REV", "sel_4", "sel_psychometric_evidence", "op_2_REV", "op_3", "quant_1_REV", "quant_2_REV", "quant_3_REV", "quant_4", "op_5_REV", "mod_1_REV", "mod_2_REV", "mod_3_REV", "mod_4_REV", "mod_5_REV", "mod_6_REV")]

replication_QMP_data <- coded_data_replications[c("def_1", "sel_2", "op_4", "reliability_type", "sel_existing", "op_version", "op_1_REV", "sel_1_REV", "sel_3_REV", "sel_4", "sel_psychometric_evidence", "op_2_REV", "op_3", "quant_1_REV", "quant_2_REV", "quant_3_REV", "quant_4", "op_5_REV", "mod_1_REV", "mod_2_REV", "mod_3_REV", "mod_4_REV", "mod_5_REV", "mod_6_REV")]



# create an empty dataset to add a recoded TRUE and FALSE response for QMPs to 
QMP_data <- data.frame(delete_this = rep(NA, 77))


QMP_data$reliability_reported_org <- ifelse(coded_data_original$N_items == "multiple item measure", ifelse(original_QMP_data$reliability_type != "Not Reported" & original_QMP_data$reliability_type != "" & !is.na(original_QMP_data$reliability_type), TRUE, ifelse(original_QMP_data$reliability_type == "Not Reported", FALSE, NA)), NA)

QMP_data$reliability_reported_rep <- ifelse(coded_data_replications$N_items == "multiple item measure", ifelse(replication_QMP_data$reliability_type != "Not Reported" & replication_QMP_data$reliability_type != "" & !is.na(replication_QMP_data$reliability_type), TRUE, ifelse(replication_QMP_data$reliability_type == "Not Reported", FALSE, NA)), NA)


# add if clearly specified if measure existed or not
QMP_data$select_or_create_clarity_org <- original_QMP_data$sel_existing == "Not Clearly Stated"

QMP_data$select_or_create_clarity_rep <- replication_QMP_data$sel_existing == "Not Clearly Stated"


# add if version was clearly specified
QMP_data$version_clarity_org <- original_QMP_data$op_1_REV == FALSE | original_QMP_data$op_version == "" & original_QMP_data$sel_existing == "True, namely:"

QMP_data$version_clarity_rep <- replication_QMP_data$op_1_REV == FALSE | replication_QMP_data$op_version == "" & replication_QMP_data$sel_existing == "True, namely:"


# add if factor structure was analysed
QMP_data$factor_analysis_org <- ifelse(original_QMP_data$sel_psychometric_evidence == "None", FALSE, ifelse(original_QMP_data$sel_psychometric_evidence != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)", TRUE, NA))

QMP_data$factor_analysis_rep <- ifelse(replication_QMP_data$sel_psychometric_evidence == "None", FALSE, ifelse(replication_QMP_data$sel_psychometric_evidence != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)", TRUE, NA))

# direct copies
QMP_data$justified_definition_org <- original_QMP_data$def_1
QMP_data$justified_definition_rep <- replication_QMP_data$def_1
QMP_data$justified_selection_org <- original_QMP_data$sel_2
QMP_data$justified_selection_rep <- replication_QMP_data$sel_2
QMP_data$justified_operationalisation_org <- original_QMP_data$op_4
QMP_data$justified_operationalisation_rep <- replication_QMP_data$op_4
QMP_data$source_org <- original_QMP_data$sel_1_REV
QMP_data$source_rep <- replication_QMP_data$sel_1_REV
QMP_data$in_psychometric_org <- original_QMP_data$sel_3_REV
QMP_data$in_psychometric_rep <- replication_QMP_data$sel_3_REV
QMP_data$out_psychometric_org <- original_QMP_data$sel_4
QMP_data$out_psychometric_rep <- replication_QMP_data$sel_4
QMP_data$admin_format_org <- original_QMP_data$op_2_REV
QMP_data$admin_format_rep <- replication_QMP_data$op_2_REV
QMP_data$admin_procedure_org <- original_QMP_data$op_3
QMP_data$admin_procedure_rep <- replication_QMP_data$op_3
QMP_data$N_items_org <- original_QMP_data$quant_1_REV
QMP_data$N_items_rep <- replication_QMP_data$quant_1_REV
QMP_data$N_response_org <- original_QMP_data$quant_2_REV
QMP_data$N_response_rep <- replication_QMP_data$quant_2_REV
QMP_data$recoding_org <- original_QMP_data$quant_3_REV
QMP_data$recoding_rep <- replication_QMP_data$quant_3_REV
QMP_data$aggregation_org <- original_QMP_data$quant_4
QMP_data$aggregation_rep <- replication_QMP_data$quant_4
QMP_data$example_items_org <- original_QMP_data$op_5_REV
QMP_data$example_items_rep <- replication_QMP_data$op_5_REV
QMP_data$mod_admin_format_org <- original_QMP_data$mod_1_REV
QMP_data$mod_admin_format_rep <- replication_QMP_data$mod_1_REV
QMP_data$mod_admin_format_support_org <- original_QMP_data$mod_2_REV
QMP_data$mod_admin_format_support_rep <- replication_QMP_data$mod_2_REV
QMP_data$mod_language_org <- original_QMP_data$mod_3_REV
QMP_data$mod_language_rep <- replication_QMP_data$mod_3_REV
QMP_data$mod_language_support_org <- original_QMP_data$mod_4_REV
QMP_data$mod_language_support_rep <- replication_QMP_data$mod_4_REV
QMP_data$mod_N_items_or_response_org <- original_QMP_data$mod_5_REV
QMP_data$mod_N_items_or_response_rep <- replication_QMP_data$mod_5_REV
QMP_data$mod_N_items_or_response_support_org <- original_QMP_data$mod_6_REV
QMP_data$mod_N_items_or_response_support_rep <- replication_QMP_data$mod_6_REV


QMP_data <- QMP_data[,!(names(QMP_data) %in% "delete_this")]



### QMP ratio data
# create the empty QMP ratio dataset
QMP_ratio_data <- data.frame(Practice = rep("", ncol(QMP_data)/2),
                   QMP_percentage_original = rep(0, ncol(QMP_data)/2),
                   N_applicable_original = rep(0, ncol(QMP_data)/2),
                   QMP_percentage_replication = rep(0, ncol(QMP_data)/2),
                   N_applicable_replication = rep(0, ncol(QMP_data)/2),
                   Phi = rep(0, ncol(QMP_data)/2))


# we loop through the length of the number of Measurement Practices (MPs) 
# ignoring doubles due to having both original and replication
for(i in 1:(ncol(QMP_data)/2)){
  # we create the name of the variable by taking the name and removing the 
  # _org suffix. Then we for each column except the NA initial column
  # we take the original MP and the related replication MP, and calculate
  # their QMP
  QMP_ratio_data[i,] <- Create_QMP_descriptive_case(substr(names(QMP_data)[i*2], 1, nchar(names(QMP_data)[i*2]) - 4), QMP_data[[(i*2)-1]], QMP_data[[i*2]])
  }

# store QMP ratio data in the analysis data folder
saving_to_analysis_data(QMP_ratio_data)
```

### Calculating Reliability

We calculated reliability from the item responses on each measure for each lab the measure was used in based on the replication datasets with suitable data. We calculated both Cronbach’s Alpha, as well as its standard error using formulas 2 & 3 from Duhachek and Lacobucci @duhachekAlphasStandardError2004a. We used these values to conduct a meta-analysis of the measure’s reliability, also referred to as a Reliability Generalization (RG) Meta-Analysis [@botellaManagingHeterogeneityVariance2012; @lopez-ibanezReliabilityGeneralizationMetaanalysis2024; @vacha-haaseReliabilityGeneralizationExploring1998]. We performed the RG Meta-Analysis using the rma function from the metafor R package [*v`r getNamespaceVersion("metafor")[[1]]`*; @R-metafor] and default settings. We then used the results from the meta-analysis to evaluate the heterogeneity via the tau statistic and the Cochran’s Q-test [@cochranCombinationEstimatesDifferent1954]. These indicators have received criticism as indicators of heterogeneity, especially when within study sample sizes are small and power to detect heterogeneity is low [@hoaglinMisunderstandingsCochransTest2016; @pereiraCriticalInterpretationCochrans2010]. Therefore, we also present the 95% prediction interval (the interval within which a the measure’s Cronbach’s Alpha is expected to fall if used again within the same population) and implore that the heterogeneity results should be viewed critically [@borensteinAvoidingCommonMistakes2024]. We implemented no correction for publication bias, because the Many Labs replications were guaranteed to be published regardless of their outcomes.

Our analyses will focus on Cronbach’s Alpha, because it is the most commonly reported reliability indicator, which also allows us to make comparisons between calculated and reported reliabilities. Furthermore, we were able to calculate the standard error of alpha to be used in the RG meta-analysis. However, Cronbach’s Alpha comes with strong assumptions on the underlying factor structure, including that the measure is unidimensional, which means that there should a single factor underlying the measurement scores. Therefore, we also estimated McDonald’s Omega for each lab from the same set of replications as for Cronbach’s Alpha, since it has been argued to be a more informative measure of reliability than Cronbach’s Alpha with less strict assumptions [@crutzenScaleQualityAlpha2017b; @dengTestingDifferenceReliability2017]. The results based on McDonald’s Omega can be found in [Supplementary Analyses D](../../SupplementaryMaterials/SupplementaryAnalysesScripts/Supplementary_omega_analyses.Rmd).

```{r calculated_reliability_across_labs, include = FALSE, eval = FALSE}
# creating an empty data frame to insert all the responses into
calculated_reliability_lab_data <- data.frame(alpha = 0, omega.tot = 0, 
                                          omega.hier = 0, ASE = 0, g = 0)

# Combining the data together 1.3, & 5.4 were omitted, because data was not
# recorded in usable numeric format
extracted_score_data <- list(data_1.10_clean, data_1.11_clean, 
    data_1.12.3.1_clean, data_1.12.3.2_clean, data_2.12.1_clean, 
    data_2.12.2_clean, data_2.12.3_clean, data_2.15_clean, data_2.20_clean, 
    data_2.23_clean, data_3.2.1.1_clean, data_3.2.1.2_clean, data_3.7.1_clean,
    data_3.7.2_clean, data_3.8.2_clean, data_5.1.1_clean, data_5.1.2_clean, 
    data_5.7_clean, data_5.9.1_clean)


# obtaining the omega and alpha values for a measure in one lab.
get_omega_and_alpha_values <- function(Data){
  # first we calculate alpha and ase separately in case the omega function goes haywire
  alpha <- psych::alpha(Data)$total[["std.alpha"]]
  ase <- psych::alpha(Data)$total[["ase"]]
  
  # try to calculate the omega
  result <- c(NA, NA)
  tryCatch({
      result <- omega(Data)
    }, error = function(e) {
      result <- c(NA, NA)
    }, warning = function(w) {
      result <- c(NA, NA)
    })
  
  # combine the information in one vector
  omega_and_alpha_vec <- as.numeric(c(alpha, result[c(4, 1)], ase))
  
  return(omega_and_alpha_vec)
}


# calculate the alpha, omega.tot, omega.hier, & ASE for all relevant datasets
# for each lab.
for (i in 1:length(extracted_score_data)){
  calculated_reliability_instance <- tapply(extracted_score_data[[i]][-1], 
                                            extracted_score_data[[i]]$g, 
                                            get_omega_and_alpha_values)
  
  calculated_reliability_instance <- data.frame(matrix(unlist(
    calculated_reliability_instance), ncol = 4, byrow = TRUE))
  
  # make sure the var names match the complete dataframe
  colnames(calculated_reliability_instance) <- c("alpha", "omega.tot", 
                                                 "omega.hier", "ASE")
  # adding the measure as a group (g) indicator
  calculated_reliability_instance$g <- i
  
  # adding this measure's data to the total
  calculated_reliability_lab_data <- rbind(calculated_reliability_lab_data, 
                                       calculated_reliability_instance)
}


# removing the empty first row
calculated_reliability_lab_data <- calculated_reliability_lab_data[-1,]

# making sure group (g) is a factor
calculated_reliability_lab_data$g <- as.factor(calculated_reliability_lab_data$g)

# indexing the meta-analysis results with a specific index relating to a 
# row (measure) in coded_data_replications
calculated_reliability_lab_data$coded_data_index <- c(rep(10, 36), rep(11, 36), 
  rep(14, 36), rep(14, 36), rep(29, 74), rep(30, 74), rep(31, 74), rep(34, 61), 
  rep(39, 60), rep(42, 58), rep(51, 21), rep(51, 21), rep(59, 20), rep(60, 21), 
  rep(61, 21), rep(65, 4), rep(66, 4), rep(74, 8), rep(76, 5))

# changing the group variable to reflect measure descriptions from the text.
# checked using:
# coded_data_replications[unique(calculated_reliability_data$reporting_index), 3], and
# coded_data_replications[unique(calculated_reliability_data$reporting_index), 5]
levels(calculated_reliability_lab_data$g) <- c("Caruso et al. (2012)", 
    "Husnu & Crisp (2010)", "Nosek et al. (2002), Math", "Nosek et al. (2002), Art", 
    "Anderson et al. (2012), SWL", "Anderson et al. (2012), PA", 
    "Anderson et al. (2012), NA", "Giessner & Schubert, (2007)", 
    "Norenzayan et al. (2002)", "Zhong & Lijenquist (2006)", 
    "Monin & Miller (2001), most", "Monin & Miller (2001), some", 
    "Cacioppo et al. (1983), arg",  "Cacioppo et al. (1983), nfc", 
    "De Fruyt et al. (2000)", "Albarracín et al. (2008), exp 5 verb", 
    "Albarracín et al. (2008), exp 5 math", "Shnabel & Nadler (2008)",
    "Vohs & Schooler (2008)")
  
calculated_reliability_lab_data$g <- factor(calculated_reliability_lab_data$g, 
    labels = c("Caruso et al. (2012)", 
    "Husnu & Crisp (2010)", "Nosek et al. (2002), Math", 
    "Nosek et al. (2002), Art", "Anderson et al. (2012), SWL", 
    "Anderson et al. (2012), PA", "Anderson et al. (2012), NA", 
    "Giessner & Schubert, (2007)", "Norenzayan et al. (2002)", 
    "Zhong & Lijenquist (2006)", "Monin & Miller (2001), most", 
    "Monin & Miller (2001), some", "Cacioppo et al. (1983), arg",  
    "Cacioppo et al. (1983), nfc", "De Fruyt et al. (2000)", 
    "Albarracín et al. (2008), exp 5 verb", 
    "Albarracín et al. (2008), exp 5 math", "Shnabel & Nadler (2008)", 
    "Vohs & Schooler (2008)"))

# adding whether or not an effect replicated based on what was coded from the 
# replication report.
calculated_reliability_lab_data$replication_success <- c(coded_data_replications[
    calculated_reliability_lab_data$coded_data_index, "hypothesis_support"])



# store per lab calculated reliability data in the analysis data folder
saving_to_analysis_data(calculated_reliability_lab_data)
```

```{r averaged_reliability, include = FALSE, eval = FALSE}
# function to assess the heterogeneity in the calculated Cronbach's Alpha values
# and get Cronbach's Alpha prediction intervals
assess_heterogeneity<- function(data_on_alpha){
  # run a random effects meta-analysis
  rma_model <- rma(yi = data_on_alpha$alpha, sei = data_on_alpha$ASE, 
                   method = "REML", control = list(stepadj = 0.5, maxiter = 1000))
  
  # extract the relevant heterogeneity information
  temp_tau <- sqrt(rma_model$tau2)
  temp_QEp <- rma_model$QEp
  
  # get prediction intervals for alpha
  rma_prediction <- predict(rma_model)
  temp_pi.lb <- rma_prediction$pi.lb
  temp_pi.ub <- rma_prediction$pi.ub
  
  return(c(temp_tau, temp_QEp, temp_pi.lb, temp_pi.ub))
}

# function to convert the lab specifc reliability to averaged
convert_reliability_data_to_avg <- function(reliability_data){
  # conducting the reliability-generalization meta-analysis for each measure
  heterogeneity_results <- assess_heterogeneity(reliability_data[c(1, 4)])
  
  # getting the avearage reliability scores for each measure
  avg_reliabilities <- colMeans(reliability_data[1:4], na.rm = TRUE)
  
  # indicates the measure
  g <- reliability_data$g[[1]]
  
  # we need one of the indices per measure as they are all the same across labs
  coded_data_index <- reliability_data$coded_data_index[[1]]
  
  # we need one of the indices per measure as they are all the same across labs
  replication_success <- reliability_data$replication_success[[1]]
  
  # extracting the reported reliability coefficient 
  coefficient_reported <- coded_data_original$reliability_coeff[coded_data_index]
  
  # calculating the difference between reported and calculated average 
  # reliability coefficient
  coeficient_difference <- coefficient_reported - avg_reliabilities[[1]]
  
  # testing whether or not (for those studies that had a reported alpha) if
  # it was out of the 95% bounds around the mean calculated alpha
  population_95_bounds <- quantile(reliability_data$alpha, probs = c(0.025, 0.975))
  
  significance_reported_coefficient <- coefficient_reported < population_95_bounds[1] | 
                                          coefficient_reported > population_95_bounds[2]
  
  # return all the data as a single row in the dataframe
  return(data.frame(alpha = avg_reliabilities[[1]], omega.tot = avg_reliabilities[[2]], 
                    omega.hier = avg_reliabilities[[3]], ASE = avg_reliabilities[[4]], 
                    tau = heterogeneity_results[[1]], QEp = heterogeneity_results[[2]],
                    pi.lb = heterogeneity_results[[3]], pi.ub = heterogeneity_results[[4]], 
                    g = g, coded_data_index = coded_data_index, 
                    replication_success, reported_coefficient = coefficient_reported, 
                    coefficient_difference = coeficient_difference, 
                    significance_reported_coefficient = significance_reported_coefficient))
}

# calculate the average reliability data + heterogeneity test + reported and calculated
# reliability coefficient comparison
avg_reliability_list <- tapply(calculated_reliability_lab_data, calculated_reliability_lab_data$g, convert_reliability_data_to_avg)

# convert data output to a dataframe
measure_reliability_data <- do.call(rbind.data.frame, avg_reliability_list)


# store per measure  reliability data in the analysis data folder
saving_to_analysis_data(measure_reliability_data)
```

### Unidimensionality

All measurement items for which we checked unidimensionality were used to form a singular index of one latent variable in their respective replications. Therefore, unidimensionality presents a valuable indication of the construct validity of a measure in our dataset. To test for unidimensionality, we fit a single-factor model on the item responses of each lab with suitable data. Our inference of unidimensionality is based on a set of four model fit indices and their commonly used thresholds indicating adequate fit: RMSEA (threshold: < .08), SRMR (< .08), CFI (> .90), and the exact fit test (statistically significant at .05). We consider the use of these thresholds sufficient for our descriptive aim, even though we understand apprehension against rules of thumb when used to evaluate measurement in individual studies. As a fifth additional fit index we ran a parallel analysis. A parallel analysis runs multiple Exploratory Factor Analyses where the number of factors in the model is increased by one until the number of factors is one less than the number of items. It then compares the eigenvalues (an indication of how much variance is explained by that factor) of each factor to the eigenvalues for that factor if the data matrix was effectively random. If only the first factor has an eigenvalue that is significantly higher than the eigenvalue when the data matrix is random, the test is passed. We chose a combination of indices to test unidimensionality, since each one has their own limitations, and combining them gives us a more robust picture of the unidimensionality of the measures.

These unidimensionality checks are not intended as a complete validation of the measures in our sample. That would require additional steps that are beyond the scope of this study. Instead, the unidimensionality is checked as a prerequisite for validity. Practically speaking this means that if we observe a lack of unidimensional fit for our measures, then we have reason to question their validity. On the other hand, if we observe good undimensional fit, then based on this preliminary evidence we conclude that we do not observe an issue with this prerequisite for validity. We also evaluated the variation in the unidimensionality indices across labs, to observe how consistently measures showed undimensional fit, or if this varried considerably across context.

```{r unidimensionality_tests, include = FALSE, eval = FALSE}
# function that runs all our checks for Cronbach's alpha. Also functions as some basic validity checks
validity_and_alpha_assumption_tests <- function(data){
  ### Tests for Unidimensionality
  # Test 1a: obtaining single factor model cfa RMSEA, CFI, SRMR, and exact fit test results
  RMSEA_values <- c(NA, NA)
  CFI <- NA
  SRMR <- NA
  fit_results <- c(NA, NA, NA)
  tryCatch({
    # creating the base function for the model (it is unidimensional)
    model_free <- "Factor =~ " 

    for (item_name in names(data)[-1]){
      # adding all of the items from the scale to the model formula
      model_free <- paste0(model_free, item_name, " + ")
    }
      
    # trimming the excess " + "
    model_free <- substring(model_free, 1, nchar(model_free) - 3)

    # fitting the free coefficient estimated single dimensional model
    fit.modelfree <- cfa(model_free, 
                              data = data, 
                              std.lv = TRUE, 
                              estimator = "MLM")

    # extracting the fit measures
    fit_measures <- fitMeasures(fit.modelfree, c("chisq", "df", "pvalue", "rmsea", "rmsea.ci.upper", "cfi", "srmr"))
    
    # extracting RMSEA and its standard error
    rmsea <- fit_measures["rmsea"] 
    rmsea.ci.upper <- fit_measures["rmsea.ci.upper"]
    rmsea.se <- (rmsea.ci.upper - rmsea) / 1.645
    RMSEA_values <- c(rmsea, rmsea.se)
    
    # extracting CFI
    CFI <- fit_measures["cfi"] 
    
    # extracting SRMR
    SRMR <- fit_measures["srmr"] 
    
    # extracting exact fit test measures
    fit_chisq <- fit_measures["chisq"]
    fit_df <- fit_measures["df"]
    fit_pvalue <- fit_measures["pvalue"]
    fit_results <- c(fit_chisq, fit_df, fit_pvalue)
    
    
  }, error = function(e) {
    RMSEA_values <- c(NA, NA)
    CFI <- NA
    SRMR <- NA
    fit_results <- c(NA, NA, NA)
  })

  
  
  # Test 1b: conducting parallel test to check if one factor solution is best.
  parallel_N_factors <- NA
  tryCatch({
      parallel_N_factors <- suppressWarnings(fa.parallel(data[-1], fa = "fa", plot = FALSE))$nfact

    }, error = function(e) {
      parallel_N_factors <- NA
  })
  
  # adding a single factor check count
  if(is.null(RMSEA_values[[1]]) | is.null(parallel_N_factors)){
    unidimensional_check_count <- NA
  } else{
    unidimensional_check_count <- sum(RMSEA_values[[1]] < .08, CFI > .90, SRMR < .08, fit_results[[3]] < .05, parallel_N_factors == 1)
  }
  
  
  ### Test 2: Test for Tau equivalence
  Tau_results <- c(NA, NA, NA)
  tryCatch({
    # creating the base function for the fixed model (it is unidimensional)
    model_tau_restrict <- "Factor =~ " 
  
    for (item_name in names(data)[-1]){
      # adding all of the items from the scale to the model formula
      # estimated with the same coefficient a to emulate a Tau equivalent model
      model_tau_restrict <- paste0(model_tau_restrict, "a*", item_name, " + ")
    }
    
    # trimming the excess " + "
    model_tau_restrict <- substring(model_tau_restrict, 1, nchar(model_tau_restrict) - 3)
    
    # fitting the tau restricted model
    fit.modeltaurestrict <- cfa(model_tau_restrict, 
                                     data = data, 
                                     std.lv = TRUE, 
                                     estimator = "MLM")
    
    #summary(fit.modelfree, fit.measures = TRUE)
    #summary(fit.modeltaurestrict, fit.measures = TRUE)
    
    # conducting the fit test between the free coefficient model and the tau
    # restricted model
    fit.test <- anova(fit.modelfree, fit.modeltaurestrict)
    
    # extract the relevant results
    Tau_results <- fit.test[2, c("Chisq diff", "Df diff", "Pr(>Chisq)")]
    }, error = function(e) {
      Tau_results <- c(NA, NA, NA)
  })
  
  ### Test 3: Assessment of uncorrelated errors
  error_cor_results <- rep(NA, 4)
  
  
  tryCatch({
    # copying the tau restricted model as the baseline.
    model_freed_errors <- model_tau_restrict
    fit.modelfreed <- fit.modeltaurestrict
    
    # create empty vector to put data into
    spec.all.vec <- rep(NA, 5)
    
    # looping through five instances of freeing the highest mod index error 
    # covariance parameter estimation. We loop 5 times rather than adding the top 5
    # highest mod index, because when one parameter is freed it will affect the extent 
    # to which other error covariances impact the fit since (part of) their additional 
    # explained variance may have already been captured in the parameter just freed.
    for (i in 1:5) {
      modindex_highest <- suppressWarnings(modindices(fit.modelfreed, sort = TRUE))[1,]
      
      # store the fully standardized estimated coefficient change for the highest
      # mod index freed parameter
      spec.all.vec[i] <- modindex_highest[[7]]
      
      # add the freeing of the parameter into the model code
      model_freed_errors <- paste(model_freed_errors, "\n", modindex_highest[[1]], modindex_highest[[2]], modindex_highest[[3]])
      
      # rerun the model
      fit.modelfreed  <- cfa(model_freed_errors, 
                             data = data, 
                             std.lv = TRUE, 
                             estimator = "MLM")
    }
  
    # perform a model comparison between the model with the top 5 error covariances
    # freely estimated and the model with Tau equivalence.
    fit.test2 <- anova(fit.modeltaurestrict, fit.modelfreed)
    
    error_cor_test_results <- fit.test2[2, c("Chisq diff", "Df diff", "Pr(>Chisq)")]
    
    # calculate the mean of the fully standardized estimated coefficient change for
    # all the highest mod index freed parameters
    spec.all.mean <- mean(spec.all.vec)
    
    # store all error related results
    error_cor_results <- unlist(c(spec.all.mean, error_cor_test_results))  
    
    }, error = function(e) {
      error_cor_results <- NA
  })
  
  ### storing all of the data
  FA_data <- c(RMSEA_values, CFI, SRMR, fit_results, parallel_N_factors, 
               unidimensional_check_count, Tau_results, error_cor_results)
  
  # returning the FA data
  return(FA_data)
}


extracted_data_list <- list(data_1.10_clean, data_1.11_clean, data_1.12.3.1_clean, data_1.12.3.2_clean, data_2.12.1_clean, data_2.12.2_clean, data_2.12.3_clean, data_2.15_clean, data_2.20_clean, data_2.23_clean, data_3.2.1.1_clean, data_3.2.1.2_clean, data_3.7.1_clean, data_3.7.2_clean, data_3.8.2_clean, data_5.1.1_clean, data_5.1.2_clean, data_5.7_clean, data_5.9.1_clean)




# list to store the data in
FA_list <- list(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
                NA, NA, NA, NA, NA, NA, NA, NA, NA)

# running the validity/alpha assumption checks for all of the extracted datasets
for (i in 1:length(extracted_data_list)){
  # applying the function across each individual lab
  FA_data <- tapply(extracted_data_list[[i]], extracted_data_list[[i]]$g,
                    validity_and_alpha_assumption_tests)

  # transforming the returned list to a dataframe
  FA_data_frame <- do.call(rbind.data.frame, FA_data)
  
  # Setting the column names
  names(FA_data_frame) <- c("RMSEA", "RMSEA_se", "CFI", "SRMR", "fit_chisq", "fit_df", 
                            "fit_pvalue", "N_factors", "unidimensional_check_count", "Tau_chisq_diff", 
                            "Tau_Df_diff", "Tau_p_diff", "mean_spec", "err_chisq_diff", 
                            "err_Df_diff", "err_p_diff")
  
  # append results to the overarching FA results list
  FA_list[[i]] <- FA_data_frame
}




# store all dataframes with the list of the assumption test data in 
# the analysis data folder
fa_Caruso_2012 <- FA_list[[1]]
saving_to_analysis_data(fa_Caruso_2012)
fa_Husnu_2010 <- FA_list[[2]]
saving_to_analysis_data(fa_Husnu_2010)
fa_Nosek_2002_Math <- FA_list[[3]]
saving_to_analysis_data(fa_Nosek_2002_Math)
fa_Nosek_2002_Art <- FA_list[[4]]
saving_to_analysis_data(fa_Nosek_2002_Art)
fa_Anderson_2012_SWL <- FA_list[[5]]
saving_to_analysis_data(fa_Anderson_2012_SWL)
fa_Anderson_2012_PA <- FA_list[[6]]
saving_to_analysis_data(fa_Anderson_2012_PA)
fa_Anderson_2012_NA <- FA_list[[7]]
saving_to_analysis_data(fa_Anderson_2012_NA)
fa_Giessner_2007 <- FA_list[[8]]
saving_to_analysis_data(fa_Giessner_2007)
fa_Norenzayan_2002 <- FA_list[[9]]
saving_to_analysis_data(fa_Norenzayan_2002)
fa_Zhong_2006 <- FA_list[[10]]
saving_to_analysis_data(fa_Zhong_2006)
fa_Monin_2001_most <- FA_list[[11]]
saving_to_analysis_data(fa_Monin_2001_most)
fa_Monin_2001_some <- FA_list[[12]]
saving_to_analysis_data(fa_Monin_2001_some)
fa_Cacioppo_1983_arg <- FA_list[[13]]
saving_to_analysis_data(fa_Cacioppo_1983_arg)
fa_Cacioppo_1983_nfc <- FA_list[[14]]
saving_to_analysis_data(fa_Cacioppo_1983_nfc)
fa_De_Fruyt_2000 <- FA_list[[15]]
saving_to_analysis_data(fa_De_Fruyt_2000)
fa_Albarracin_2008_verb <- FA_list[[16]]
saving_to_analysis_data(fa_Albarracin_2008_verb)
fa_Albarracin_2008_math <- FA_list[[17]]
saving_to_analysis_data(fa_Albarracin_2008_math)
fa_Shnabel_2008 <- FA_list[[18]]
saving_to_analysis_data(fa_Shnabel_2008)
fa_Vohs_2008 <- FA_list[[19]]
saving_to_analysis_data(fa_Vohs_2008)


rm(extracted_data_list)
```

# Results

## Measurement Reporting

### Transparency and Reusability

Table \@ref(tab:QMPTable) lists the prevalence of different measurement reporting practices in original and replication studies. Specifically, we calculated the proportion of measures that transparently reported on the item compared to the total number of measures to which the item was applicable. A higher proportion indicates more transparent measurement reporting.

```{r QMPTable}
# create informative Measurement Practice Labels 
MPractice <- c("Reliability is reported", "It is clear if the measure existed or is newly created",    "Exact version of the measure is specified", "Validity evidence from a factor analysis is presented",   "Measured variable is defined", "Choice of measure selection or creation is justified",    "Implemented operationalisation is justified", "Source of the measure is provided",    "Psychometric evidence from the study is given", "Psychometric evidence from an earlier study is given",    "Administration format and environment are described", "Administration procedure is described",   "N items are described", "N response options are described", "Recoding of responses is described", "Creation of the index is described", "Text or supplement has example items", "Administration format changes are mentioned",    "Administration format changes are justified/validated", "Translations are mentioned",    "Translated measures are justified/validated", "Changes in N items or response options are mentioned",    "Changes in N items or response options are justified/validated")   


# construct the QMP table to be printed 
QMP_table <- data.frame("Item" = MPractice, 
                        "Original" = paste0(format(round(1 - qmp_ratio_data$QMP_percentage_original, digits = 2), nsmall = 2), " (", qmp_ratio_data$N_applicable_original, ")"), 
                        "Replication" = paste0(format(round(1 - qmp_ratio_data$QMP_percentage_replication, digits = 2), nsmall = 2), " (", qmp_ratio_data$N_applicable_replication, ")"))   

QMP_table_formatted <- rbind(rbind(rbind(rbind(rbind(c("Definition", "", ""), QMP_table[5,]),
                                               c("Operationalisation", "", ""), QMP_table[c(3, 11, 12, 7, 17),]),
                                         c("Selection/Creation", "", ""), QMP_table[c(2, 6, 8, 9, 10, 1, 4),]),
                                   c("Quantification", "", ""), QMP_table[c(13, 14, 15, 16),]),
                             c("Modification", "", ""), QMP_table[18:23,])

# set the zero N Items to "- (0)"
QMP_table_formatted[23, 2] <- "- (0)"
QMP_table_formatted[24, 2] <- "- (0)"


# print the table in apa formatting 
apa_table(QMP_table_formatted, align = c("l", "r", "r"), 
          caption = "Proportion of measures that met each of the item criteria, for both original and replication studies. Proportions are calculated based on the total number of measures to which an item was applicable (in brackets).", 
          note = "The first number in both Original and Replication is the ratio of measures for which that reporting practice contained sufficient information. The number between brackets indicates the number of measures for which the practice was applicable.", escape = FALSE, placement = "htp", booktabs = TRUE)
```

```{r ReuseCompleteInfoPercent}
reuse_complete_percent_original <- round((sum((coded_data_original$op_1_REV | is.na(coded_data_original$op_1_REV)) & (coded_data_original$op_2_REV | is.na(coded_data_original$op_2_REV)) & (coded_data_original$op_3 | is.na(coded_data_original$op_3)) & (coded_data_original$op_4 | is.na(coded_data_original$op_4)) & (coded_data_original$op_5_REV | is.na(coded_data_original$op_5_REV)) & coded_data_original$sel_existing != "Not Clearly Stated" & (coded_data_original$sel_1_REV | is.na(coded_data_original$sel_1_REV)) & (coded_data_original$quant_1_REV | is.na(coded_data_original$quant_1_REV)) & (coded_data_original$quant_2_REV | is.na(coded_data_original$quant_2_REV)) & (coded_data_original$quant_3_REV | is.na(coded_data_original$quant_3_REV)) & (coded_data_original$quant_4 | is.na(coded_data_original$quant_4))) / nrow(coded_data_original)) * 100, 1)

reuse_complete_percent_replication <- round((sum((coded_data_replications$op_1_REV | is.na(coded_data_replications$op_1_REV)) & (coded_data_replications$op_2_REV | is.na(coded_data_replications$op_2_REV)) & (coded_data_replications$op_3 | is.na(coded_data_replications$op_3)) & (coded_data_replications$op_4 | is.na(coded_data_replications$op_4)) & (coded_data_replications$op_5_REV | is.na(coded_data_replications$op_5_REV)) & coded_data_replications$sel_existing != "Not Clearly Stated" & (coded_data_replications$sel_1_REV | is.na(coded_data_replications$sel_1_REV)) & (coded_data_replications$quant_1_REV | is.na(coded_data_replications$quant_1_REV)) & (coded_data_replications$quant_2_REV | is.na(coded_data_replications$quant_2_REV)) & (coded_data_replications$quant_3_REV | is.na(coded_data_replications$quant_3_REV)) & (coded_data_replications$quant_4 | is.na(coded_data_replications$quant_4))) / nrow(coded_data_replications)) * 100, 1)
```

For the description of the results, we highlight the reporting practices most relevant for reusing the measure in future research, as well as the modification reporting practices to understand the relation between original and replication studies. It was not always clear which measures were used  (`r round(sum(coded_data_original$sel_existing == "Not Clearly Stated") / nrow(coded_data_original) * 100, 1)`% original; `r round(sum(coded_data_replications$sel_existing == "Not Clearly Stated") / nrow(coded_data_replications) * 100, 1)`% replication), which also implies that in these cases we could not be sure that the same measure was used in original and replication studies. While the number of items (`r round((1 - qmp_ratio_data$QMP_percentage_original[13]) * 100, 1)`% original (n = `r qmp_ratio_data$N_applicable_original[13]`); `r round((1 - qmp_ratio_data$QMP_percentage_replication[13]) * 100, 1)`% replications (n = `r qmp_ratio_data$N_applicable_replication[13]`)) and response options (`r round((1 - qmp_ratio_data$QMP_percentage_original[14]) * 100, 1)`% original (n = `r qmp_ratio_data$N_applicable_original[14]`); `r round((1 - qmp_ratio_data$QMP_percentage_replication[14]) * 100, 1)`% replications (n = `r qmp_ratio_data$N_applicable_replication[14]`)) were usually reported, even these were not always clear. How the responses should be recoded if at all (`r round((1 - qmp_ratio_data$QMP_percentage_original[15]) * 100, 1)`% original (n = `r qmp_ratio_data$N_applicable_original[15]`); `r round((1 - qmp_ratio_data$QMP_percentage_replication[15]) * 100, 1)`% replication (n = `r qmp_ratio_data$N_applicable_replication[15]`)), and how an index was created from the items (`r round((1 - qmp_ratio_data$QMP_percentage_original[16]) * 100, 1)`% original (n = `r qmp_ratio_data$N_applicable_original[16]`); `r round((1 - qmp_ratio_data$QMP_percentage_replication[16]) * 100, 1)`% replication (n = `r qmp_ratio_data$N_applicable_replication[16]`)) was clear a lot less. The operationalisation of the measure was usually reported clearly in the replication studies (format: `r round((1 - qmp_ratio_data$QMP_percentage_replication[11]) * 100, 1)`, procedure: `r round((1 - qmp_ratio_data$QMP_percentage_replication[12]) * 100, 1)`, justification: `r round((1 - qmp_ratio_data$QMP_percentage_replication[7]) * 100, 1)`, example items: `r round((1 - qmp_ratio_data$QMP_percentage_replication[17]) * 100, 1)`), but less so in original studies (format: `r round((1 - qmp_ratio_data$QMP_percentage_original[11]) * 100, 1)`, procedure: `r round((1 - qmp_ratio_data$QMP_percentage_original[12]) * 100, 1)`, justification: `r round((1 - qmp_ratio_data$QMP_percentage_original[7]) * 100, 1)`, example items: `r round((1 - qmp_ratio_data$QMP_percentage_original[17]) * 100, 1)`). If we look across all of the items mentioned above, the measures for which all applicable items were clearly reported on was `r reuse_complete_percent_original`% of all measures in original studies and `r reuse_complete_percent_replication`% in replications.

Modifications to the number of items compared to preceding research were rare but did occur for `r round((qmp_ratio_data$QMP_percentage_original[22]) * 100, 1)`% of measures in original studies (n = `r qmp_ratio_data$N_applicable_original[22]`) and `r round((1 - qmp_ratio_data$QMP_percentage_replication[22]) * 100, 1)`% for replications (n = `r qmp_ratio_data$N_applicable_replication[22]`)). Furthermore, similar to the findings of Flake et al @flakeConstructValidityValidity2022 (original: 92%; replication 85.6%), most studies reported the number of items for their measures (original: `r round((1 - qmp_ratio_data$QMP_percentage_original[13]) * 100, 1)`%; replications: `r round((1 - qmp_ratio_data$QMP_percentage_replication[13]) * 100, 1)`%). The total proportion of measures for which the number of items was changed from original to replication was also similar (`r round(sum(!is.na(coded_data_replications$mod_5_REV)) / nrow(coded_data_replications) * 100, 1)`)% to Flake et al @flakeConstructValidityValidity2022 (16%). In general, it was uncommon for measures in original studies to have been modified. In replication studies, we noted that not all modifications were validated or justified (language: `r round((1 - qmp_ratio_data$QMP_percentage_replication[21]) * 100, 1)`% (n = `r qmp_ratio_data$N_applicable_replication[21]`); administration format: `r round((1 - qmp_ratio_data$QMP_percentage_replication[19]) * 100, 1)`% (n = `r qmp_ratio_data$N_applicable_replication[19]`)), especially for changes in items and response options, the modification was validated or justified for only a little more than half of the measures (`r round((1 - qmp_ratio_data$QMP_percentage_replication[23]) * 100, 1)`% (n = `r qmp_ratio_data$N_applicable_replication[23]`)). In general, we observed that measurement was modified in some way from original to replication for `r round((sum(coded_data_replications$mod_check == "True") / nrow(coded_data_replications)) * 100, 1)`% of measures, and it was unclear whether modification occurred for `r round((sum(coded_data_replications$mod_check == "None Reported") / nrow(coded_data_replications)) * 100, 1)`%.

### Reliability Reporting

Figure \@ref(fig:ReliabilityReportingFlowDiagram) depicts a flowchart of measure types and reliability reporting in original and replication studies. First, of the total of 77 measures, it shows that almost half of the measures in both replication (N = `r sum(coded_data_replications$N_items == "1 item measure")`) and original research (N = `r sum(coded_data_original$N_items == "1 item measure")`) were single-item measures. Second, for only `r apa_num(sum(coded_data_original$reliability_type != "Not Reported" & coded_data_original$reliability_type != ""), numerals = FALSE)` measures out of `r sum(coded_data_original$N_items == "multiple item measure")` (`r round((sum(coded_data_original$reliability_type != "Not Reported" & coded_data_original$reliability_type != "") / sum(coded_data_original$N_items == "multiple item measure")) * 100, 1)` %) multi-item scales in original and `r apa_num(sum(coded_data_replications$reliability_type != "Not Reported" & coded_data_replications$reliability_type != ""), numerals = FALSE)` out of `r sum(coded_data_replications$N_items == "multiple item measure")` (`r round((sum(coded_data_replications$reliability_type != "Not Reported" & coded_data_replications$reliability_type != "") / sum(coded_data_replications$N_items == "multiple item measure")) * 100, 1)`%) in replications, a reliability indicator was reported. Flake et al @flakeConstructValidityValidity2022 observed a much higher degree of reliability coefficient reporting in both original (60.8%) and replications (37.1%). However, similarly to Flake et al @flakeConstructValidityValidity2022, we observed that reliability reporting was more common in original studies as compared to replication studies, and that Cronbach’s Alpha was the most commonly reported reliability indicator in our sample as well.

```{r ReliabilityReportingFlowDiagram, fig.cap = "Reliability reporting flow diagram. Figure shows the number of measures as reported in both the replication protocols and original article, which meet the criterion in the box within the diagram and those criteria before it.", out.height = "60%"}
knitr::include_graphics(path = "../../SupplementaryMaterials/reliability_reporting_flow_diagram.png")
```

### Validity Reporting

For validity evidence the pattern was similar. Psychometric validity evidence was reported for only, `r apa_num(sum(coded_data_original$sel_psychometric_evidence_REV != "None" & coded_data_original$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)" | coded_data_original$sel_3_REV == TRUE, na.rm = TRUE), numerals = FALSE)` (`r round((sum(coded_data_original$sel_psychometric_evidence_REV != "None" & coded_data_original$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)" | coded_data_original$sel_3_REV == TRUE, na.rm = TRUE) / sum(coded_data_original$N_items == "multiple item measure")) * 100, 1)`%) multiple item measures in original studies and `r apa_num(sum(coded_data_replications$sel_psychometric_evidence_REV != "None" & coded_data_replications$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)" | coded_data_replications$sel_3_REV == TRUE, na.rm = TRUE), numerals = FALSE)` (`r round((sum(coded_data_replications$sel_psychometric_evidence_REV != "None" & coded_data_replications$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)" | coded_data_replications$sel_3_REV == TRUE, na.rm = TRUE) / sum(coded_data_replications$N_items == "multiple item measure")) * 100, 1)`%) in replications. These proportions align with the results from Flake et al @flakeConstructValidityValidity2022 for both original (9.3%) and replication studies (6.2%), and again, similar to Flake et al @flakeConstructValidityValidity2022 we observe that validity evidence is also more commonly reported in original compared to replication studies. Exploratory factor analyses (original: `r apa_num(sum(coded_data_original$sel_psychometric_evidence_REV == "Exploratory Factor Analysis"), numerals = FALSE)`, replications: `r apa_num(sum(coded_data_replications$sel_psychometric_evidence_REV == "Exploratory Factor Analysis"), numerals = FALSE, zero_string = "zero")`) and convergent validity evidence (original: `r apa_num(sum(coded_data_original$sel_psychometric_evidence_text_REV == "convergent validity"), numerals = FALSE)`, replications: `r apa_num(sum(coded_data_replications$sel_psychometric_evidence_text_REV == "convergent validity"), numerals = FALSE)`) --- in which a measure is correlated to a measure of a theoretically related construct --- were the most commonly reported validity indicators. `r apa_num(sum(coded_data_original$sel_4 == TRUE, na.rm = TRUE), numerals = FALSE)` original studies and `r apa_num(sum(coded_data_replications$sel_4 == TRUE, na.rm = TRUE), numerals = FALSE)` replications reported psychometric validity evidence from previous studies. For single-item measures, validity evidence was reported in `r apa_num(sum(coded_data_original$sel_psychometric_evidence_text_REV[coded_data_original$N_items == "1 item measure"] != ""), numerals = FALSE, zero_string = "none")` of the original studies, and only `r apa_num(sum(coded_data_replications$sel_psychometric_evidence_text_REV[coded_data_replications$N_items == "1 item measure"] != ""), numerals = FALSE)` replication protocols (in both cases evidence of convergent validity). 

```{r validity_evidence_reporting, include = FALSE, eval = FALSE}
table(coded_data_original$sel_psychometric_evidence_REV)
table(coded_data_replications$sel_psychometric_evidence_REV)

# how much psychometric validity evidence was reported
sum(coded_data_original$sel_psychometric_evidence_REV != "None" & coded_data_original$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)" | coded_data_original$sel_3_REV == TRUE, na.rm = TRUE)
sum(coded_data_replications$sel_psychometric_evidence_REV != "None" & coded_data_replications$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)" | coded_data_replications$sel_3_REV == TRUE, na.rm = TRUE)

table(coded_data_original$sel_psychometric_evidence_text_REV)
table(coded_data_replications$sel_psychometric_evidence_text_REV)


# internal
table(coded_data_original$sel_3_REV)
table(coded_data_replications$sel_3_REV)
# external
table(coded_data_original$sel_4)
table(coded_data_replications$sel_4)

sum(coded_data_original$sel_4 == TRUE, na.rm = TRUE)
sum(coded_data_replications$sel_4 == TRUE, na.rm = TRUE)


# how many existing measures were used
sum(coded_data_original$sel_existing == "True, namely:")
sum(coded_data_replications$sel_existing == "True, namely:")

# for existing measures, how many had the specific version reported
sum(coded_data_original$op_1_REV[coded_data_original$sel_existing == "True, namely:"] == TRUE, na.rm = TRUE)
sum(coded_data_replications$op_1_REV[coded_data_replications$sel_existing == "True, namely:"] == TRUE , na.rm = TRUE)


# for measures how much exploratory factor analysis validity evidence was reported
sum(coded_data_original$sel_psychometric_evidence_REV == "Exploratory Factor Analysis")
sum(coded_data_replications$sel_psychometric_evidence_REV == "Exploratory Factor Analysis")


# for measures how much convergent validity evidence was reported
sum(coded_data_original$sel_psychometric_evidence_text_REV == "convergent validity")
sum(coded_data_replications$sel_psychometric_evidence_text_REV == "convergent validity")


# for single item measures was any validity evidence reported
apa_num(sum(coded_data_replications$sel_psychometric_evidence_text_REV[coded_data_replications$N_items == "1 item measure"] != ""), numerals = FALSE)

apa_num(sum(coded_data_original$sel_psychometric_evidence_text_REV[coded_data_original$N_items == "1 item measure"] != ""), numerals = FALSE)


# if an existing measure was used, how did that affect the reporting of reliability
# and validity evidence
table(coded_data_original$sel_existing, coded_data_original$reliability_type) # reliability does appear to indicate more reporting for existing reliabilities

table(coded_data_original$sel_existing, coded_data_original$sel_psychometric_evidence_REV) # not much to go on for psychometric validity evidence

# overall not substantive enough data too highlight in the article (would require a test to evaluate)
```

## Analysis of Item Responses

### Calculated Reliability Coefficients

We could calculate Cronbach’s Alpha for `r length(levels(calculated_reliability_lab_data$g))` measures across on average `r apa_num(mean(table(calculated_reliability_lab_data$g)))` labs, for which the required raw data was provided. The average Cronbach’s Alpha coefficient across measures was `r apa_num(mean(measure_reliability_data$alpha))` with a standard deviation of `r apa_num(sd(measure_reliability_data$alpha))`. Figure \@ref(fig:PlotAlphaDistributions) displays the distributions of the calculated Cronbach’s Alpha scores from each lab for each measure, separated by successful and unsuccessful replication, based on the meta-analytic p-value (alpha = .05) retrieved from the Many Labs reports.

```{r alpha_distributions_data_prep}
### Preparation code for the alpha distribution figure
# creating an editable copy of the calculated_reliability_lab_data
calculated_alpha_plot_data <- calculated_reliability_lab_data

# calculating the average alpha per group
calculated_alpha_plot_data$avg.alpha <- ave(calculated_reliability_lab_data$alpha, 
                                            calculated_reliability_lab_data$g)

# making replication success a Boolean, so we can order the dataset by it.
calculated_alpha_plot_data$replication_success <- ifelse(
  calculated_alpha_plot_data$replication_success == "Yes", TRUE, FALSE)

# reordering the plot from succesfully replicated to unsuccesfully replicated, and then from least to most reliable.
calculated_alpha_plot_data <- calculated_alpha_plot_data[order(-calculated_alpha_plot_data$replication_success, -calculated_alpha_plot_data$avg.alpha),]

# Removing calculated alpha's that fell below 0 to make everythign fit nicely in the plot.
calculated_alpha_plot_data <- calculated_alpha_plot_data[calculated_alpha_plot_data$alpha > 0,]

# reordering the grouping variable factor levels
calculated_alpha_plot_data$g <- fct_inorder(as.factor(calculated_alpha_plot_data$g), ordered = NA)


# adding an index to which row in the plot data the row in measure_reliability_data belongs to
measure_reliability_data$graph_index <- NA

for (i in 1:nrow(measure_reliability_data)){
  measure_reliability_data$graph_index[which(unique(calculated_alpha_plot_data$g)[i] == measure_reliability_data$g)] <- i
}

```

```{r PlotAlphaDistributions, warning = FALSE,  fig.cap = "Distributions of calculated Cronbach’s Alpha coefficients calculated for the responses on a measure at each lab location, across the nineteen measures for which the required raw data was available. Cronbach’s Alpha values that fell below 0 were excluded. The green lines indicate the meta-analytic 95% prediction interval lower and upper bound. The blue triangles indicate the reported Cronbach’s Alpha coefficient for that measure from the original article, when reported. The N column indicates the number of labs that the measure was used in. The Tau column besides the figure shows the tau heterogeneity estimate based on a meta-analysis of the calculated reliabilities for each measure. Meta-analyses for which the Q-test for heterogeneity was significant at alpha = .05 are marked by a *. The Diff column shows the difference between reported reliability and the average reliability calculated from the Many Labs data for the applicable measures. The reported reliabilities that fell outside the 95% quantile of calculated reliability scores are marked by a *."}

# plot for distribution of alpha
ggplot(calculated_alpha_plot_data, aes(x = alpha, y = g)) +
  geom_boxplot(outlier.shape = NA) +
  geom_hline(yintercept = 6.5, color = "red", size = 1) +
  geom_point(alpha = 0.1) +
  
  # adding in the number of labs per measures
  geom_text(data = measure_reliability_data, label = as.numeric(table(calculated_alpha_plot_data$g)), x = 1.1, y = measure_reliability_data$graph_index, size = 2.8) +
  
  # adding in the tau values
  geom_text(data = measure_reliability_data, label = ifelse(measure_reliability_data$QEp < .05, paste0(format(measure_reliability_data$tau, digits = 1), "*"), paste0(format(measure_reliability_data$tau, digits = 1), " ")), x = 1.22, y = measure_reliability_data$graph_index, size = 2.8) +
  
  # adding in the difference in alpha coefficients
  geom_text(data = measure_reliability_data[c(1:8, 18),], label = ifelse(measure_reliability_data$significance_reported_coefficient[c(1:8, 18)], paste0(format(measure_reliability_data$coefficient_difference[c(1:8, 18)], digits = 2), "*"), paste0(format(measure_reliability_data$coefficient_difference[c(1:8, 18)], digits = 2), " ")), x = 1.35, y = measure_reliability_data[!is.na(measure_reliability_data$coefficient_difference),]$graph_index, size = 2.8) +
  
  # setting the theme
  theme_minimal() +
  theme(legend.position = "none", plot.margin = unit(c(1, 6.5, 1, 1), "lines")) +
  
  # adding the necessary indicative texts
  annotation_custom(grob = textGrob(label = "Not Replicated", hjust = 0, gp = gpar(fontsize = 10)), ymin = 7.25, ymax = 7.25, xmin = 0.01, xmax = 0.01) +
  annotation_custom(grob = textGrob(label = "Replicated", hjust = 0, gp = gpar(fontsize = 10)), ymin = 6, ymax = 6, xmin = 0.01, xmax = 0.01) +
  annotation_custom(grob = textGrob(label = "N", hjust = 0, gp = gpar(fontsize = 12)), ymin = 20.2, ymax = 20.2, xmin = 1.08, xmax = 1.1) +
  annotation_custom(grob = textGrob(label = "Tau", hjust = 0, gp = gpar(fontsize = 12)), ymin = 20.2, ymax = 20.2, xmin = 1.24, xmax = 1.1) +
  annotation_custom(grob = textGrob(label = "Diff", hjust = 0, gp = gpar(fontsize = 12)), ymin = 20.2, ymax = 20.2, xmin = 1.37, xmax = 1.24) +
  
  coord_cartesian(xlim = c(0, 1), clip = "off") +
  
  
  # adding the blue triangles for reported reliability and green prediction intervals
  geom_point(data = measure_reliability_data, mapping = aes(x = reported_coefficient, y = graph_index), color = "blue", shape = 17, size = 3) +
  # we remove 14 & 15 lower bound because they are below 0
  geom_point(data = measure_reliability_data[-c(14, 15),], mapping = aes(x = pi.lb, y = graph_index), color = "green", shape = 124, size = 2.5) + 
  geom_point(data = measure_reliability_data, mapping = aes(x = pi.ub, y = graph_index), color = "green", shape = 124, size = 2.5) + 
  
  ylab("") +
  xlab("Cronbach's alpha") 

```

We found statistically significant indication of heterogeneity in Cronbach’s Alpha for `r apa_num(sum(measure_reliability_data$QEp < .05), numerals = FALSE)` of the `r apa_num(nrow(measure_reliability_data), numerals = FALSE)` measures. However, as noted earlier, a more accurate indication of the heterogeneity can be observed in the prediction intervals. The prediction intervals generally show larger indications of heterogeneity for some of the lower reliability measures and less for higher reliability measures when compared to the Tau test results. When comparing reported reliability in the original study when present to the calculated reliability averages, we first of all can see that reported reliabilities were generally lower than the average calculated reliabilities. Second, we can observe that reliabilities were reported more often in the original study if the measures had higher average calculated reliabilities (\>.80) in the replications.

### Unidimensionality Test

```{r PlotData1Factor, warning = FALSE}
# load in the FA_list data
FA_list <- list(fa_caruso_2012, fa_husnu_2010, fa_nosek_2002_math, fa_nosek_2002_art,
                fa_anderson_2012_swl, fa_anderson_2012_pa, fa_anderson_2012_na,
                fa_giessner_2007, fa_norenzayan_2002, fa_zhong_2006, fa_monin_2001_most,
                fa_monin_2001_some, fa_cacioppo_1983_arg, fa_cacioppo_1983_nfc, 
                fa_de_fruyt_2000, fa_albarracin_2008_verb, fa_albarracin_2008_math,
                fa_shnabel_2008, fa_vohs_2008)



# create the base dataframe
unidimensionality_graph_data <- data.frame(RMSEA = NA,
                                           RMSEA_se = NA,
                                           CFI = NA,
                                           SRMR = NA,
                                           fit_chisq = NA,
                                           fit_df = NA,
                                           fit_pvalue = NA,
                                           N_factors = NA,
                                           unidimensional_check_count = NA,
                                           g = NA)

measured_variables <- c("caruso_2012", "husnu_2010", "nosek_2002_math", "nosek_2002_art", "anderson_2012_swl", "anderson_2012_pa", "anderson_2012_na", "giessner_2007", "norenzayan_2002", "zhong_2006", "monin_2001_most", "monin_2001_some", "cacioppo_1983_arg", "cacioppo_1983_nfc", "de_fruyt_2000", "albarracin_2008_verb", "albarracin_2008_math", "shnabel_2008", "vohs_2008")


# adding the info from the FA_list to our data.
for (i in 1:19){

  g = measured_variables[i]
  
  unidimensionality_graph_data <- rbind(unidimensionality_graph_data, cbind(FA_list[[i]][1:9], g))

}

# removing placeholder first row
unidimensionality_graph_data <- unidimensionality_graph_data[-1, ]

# ordering the levels of g to match the order in the reliability graph
unidimensionality_graph_data$g <- factor(unidimensionality_graph_data$g, levels = c("nosek_2002_art", "nosek_2002_math", "shnabel_2008", "husnu_2010", "norenzayan_2002", "vohs_2008", "cacioppo_1983_arg", "anderson_2012_pa", "anderson_2012_na", "giessner_2007", "anderson_2012_swl", "caruso_2012", "zhong_2006", "monin_2001_some", "cacioppo_1983_nfc", "monin_2001_most", "albarracin_2008_verb", "albarracin_2008_math", "de_fruyt_2000"), labels = c("Nosek et al. (2002), Art", "Nosek et al. (2002), Math", "Shnabel & Nadler (2008)", "Husnu & Crisp (2010)", "Norenzayan et al. (2002)", "Vohs & Schooler (2008)", "Cacioppo et al. (1983), arg", "Anderson et al. (2012), PA", "Anderson et al. (2012), NA", "Giessner & Schubert, (2007)", "Anderson et al. (2012), SWL", "Caruso et al. (2012)", "Zhong & Lijenquist (2006)", "Monin & Miller (2001), some", "Cacioppo et al. (1983), nfc", "Monin & Miller (2001), most",  "Albarracín et al. (2008), exp 5 verb", "Albarracín et al. (2008), exp 5 math", "De Fruyt et al. (2000)"))

# adding a capped of version of the N_factors variable as a category variable
N_factors_capped <- rep(NA, nrow(unidimensionality_graph_data))

N_factors_capped[unidimensionality_graph_data$N_factors == 1] <- "1"
N_factors_capped[unidimensionality_graph_data$N_factors == 2] <- "2"
N_factors_capped[unidimensionality_graph_data$N_factors >= 3] <- "3+"

unidimensionality_graph_data$N_factors_capped <- as.factor(N_factors_capped)


### meta-analyzing RMSEA
assess_undidimensionality_heterogeneity <- function(data_on_RMSEA){
  temp_beta <- NA
  temp_tau <- NA
  temp_QEp <- NA
  rma_prediction <- NA
  temp_pi.lb <- NA
  temp_pi.ub <- NA
  
  tryCatch({
    # run a random effects meta-analysis
    rma_model <- rma(yi = data_on_RMSEA$RMSEA, sei = data_on_RMSEA$RMSEA_se, 
                     method = "REML", control = list(stepadj = 0.5, maxiter = 1000))
    # estimated coefficient
    temp_beta <- rma_model$beta
    
    # extract the relevant heterogeneity information
    temp_tau <- sqrt(rma_model$tau2)
    temp_QEp <- rma_model$QEp
    
    # get prediction intervals for alpha
    rma_prediction <- predict(rma_model)
    temp_pi.lb <- rma_prediction$pi.lb
    temp_pi.ub <- rma_prediction$pi.ub

    
  }, error = function(e) {
    
  })
  
  
  return(c(temp_beta, temp_tau, temp_QEp, temp_pi.lb, temp_pi.ub, data_on_RMSEA$g[1]))
}


unidimensionality_meta_list <- tapply(unidimensionality_graph_data, unidimensionality_graph_data$g, assess_undidimensionality_heterogeneity)

# convert data output to a dataframe
unidimensionality_meta_data <- do.call(rbind.data.frame, unidimensionality_meta_list)

names(unidimensionality_meta_data) <- c("beta", "tau", "QEp", "pi.lb", "pi.ub", "g")

# add the count of significant p_values for the exact test to the meta_dataframe

count_significant_results <- function(p){
  significance <- sum(p > .05, na.rm = TRUE)
  number_of_converged <- sum(!is.na(p))
  return(c(significance, number_of_converged))
}

p_meta_list <- tapply(unidimensionality_graph_data$fit_pvalue, unidimensionality_graph_data$g, count_significant_results)

p_meta_data <- do.call(rbind.data.frame, p_meta_list)

names(p_meta_data) <- c("p_count", "converged_count")

unidimensionality_meta_data <- cbind(unidimensionality_meta_data, p_meta_data)


# added the reporting or lack of reporting of psychometric validity evidence in the original to the meta data
unidimensionality_meta_data$reported_psych_val <- coded_data_original$sel_psychometric_evidence_REV[measure_reliability_data$coded_data_index] != "None" & coded_data_original$sel_psychometric_evidence_REV[measure_reliability_data$coded_data_index] != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)" | coded_data_original$sel_3_REV[measure_reliability_data$coded_data_index] == TRUE

# adding the number of non-converged factor models
non_converged_list <- tapply(unidimensionality_graph_data$RMSEA, unidimensionality_graph_data$g, is.na)

non_converged_vec <- rep(0, 19)

for (i in 1:19){
  non_converged_vec[i] <- sum(non_converged_list[[i]])
}

# creating a string that shows the ratio of nonconverged to total
convergence_rate_vec <- paste0(unidimensionality_meta_data$converged_count, "/", unidimensionality_meta_data$converged_count + non_converged_vec)

# adding a star for De Fruyt et al. (2000)
convergence_rate_vec <- c(convergence_rate_vec[1:18], paste0(convergence_rate_vec[19], "*"))

```

Figure \@ref(fig:Plot1FactorRMSEA) shows the result of our unidimensionality test. The mean obtained RMSEAs were below the threshold of .08 for `r apa_num(sum(as.numeric(tapply(unidimensionality_graph_data$RMSEA, unidimensionality_graph_data$g, mean, na.rm = TRUE)) < .08, na.rm = TRUE), numerals = FALSE)` of the `r apa_num(nrow(unidimensionality_meta_data), numerals = FALSE)` measures (`r apa_num(sum(as.numeric(tapply(unidimensionality_graph_data$RMSEA, unidimensionality_graph_data$g, mean, na.rm = TRUE)) < .08, na.rm = TRUE) / nrow(unidimensionality_meta_data) * 100)`%). Furthermore, the RMSEA shows considerable variation within measures, with the standard deviation ranging from `r apa_num(min(tapply(unidimensionality_graph_data$RMSEA, unidimensionality_graph_data$g, sd, na.rm = TRUE)[!is.na(tapply(unidimensionality_graph_data$RMSEA, unidimensionality_graph_data$g, sd, na.rm = TRUE))]))` to `r apa_num(max(tapply(unidimensionality_graph_data$RMSEA, unidimensionality_graph_data$g, sd, na.rm = TRUE)[!is.na(tapply(unidimensionality_graph_data$RMSEA, unidimensionality_graph_data$g, sd, na.rm = TRUE))]))` (mean SD = `r apa_num(mean(tapply(unidimensionality_graph_data$RMSEA, unidimensionality_graph_data$g, sd, na.rm = TRUE)[!is.na(tapply(unidimensionality_graph_data$RMSEA, unidimensionality_graph_data$g, sd, na.rm = TRUE))]))`).

The parallel analysis results similarly showed inconsistency both across and within measures. While `r apa_num((sum(unidimensionality_graph_data$N_factors_capped == "1", na.rm = TRUE) / nrow(unidimensionality_graph_data)) * 100)`% of labs returned a single factor solution, `r apa_num((sum(unidimensionality_graph_data$N_factors_capped == "2", na.rm = TRUE) / nrow(unidimensionality_graph_data)) * 100)`% returned a two-factor, and `r apa_num((sum(unidimensionality_graph_data$N_factors_capped == "3+", na.rm = TRUE) / nrow(unidimensionality_graph_data)) * 100)`% returned three or more factors. The remainder did not converge. The lack of convergence was also an issue for the CFA used for the RMSEA check. The CFA failed to converge for `r apa_num((sum(is.na(unidimensionality_graph_data$RMSEA)) / nrow(unidimensionality_graph_data)) * 100)`% of labs. In most cases, the convergence problems were the result of either the severe misfit of our factor models or a lack of sample size to stably estimate these models. Furthermore, the results from the exact fit tests showed varying dimensionality across labs. In total, the factor analyses converged for eighteen measures. For `r apa_num(sum((unidimensionality_meta_data$p_count[1:18] / unidimensionality_meta_data$converged_count[1:18]) >= .75), numerals = FALSE)` of these measures, there was substantive evidence for a one-factor solution (statistical fit in >75% of labs), whereas for `r apa_num(sum((unidimensionality_meta_data$p_count[1:18] / unidimensionality_meta_data$converged_count[1:18]) <= .25), numerals = FALSE)` measures, there was low evidence for a one-factor solution (statistical fit in <25% of labs). The evidence for a one-factor solution for the remaining measures was mixed. 

```{r Plot1FactorRMSEA, warning = FALSE, fig.cap = "Distributions of calculated RMSEA of a single-factor model fit calculated for the responses on a measure at each lab location, across the nineteen measures for which raw data was available on which a factor model could be fitted. The red horizontal line separates the replicated from the non-replicated measures. The red vertical line indicates our .08 RMSEA cutoff value. The first number in the Conv. column besides the graph shows the number of labs per measure for which the single-factor CFA converged, with second number showing the total number of labs. The Sig. Prop. column indicates the proportion of converged studies for which the 1-factor CFA exact fit test returned a statistically significant p-value at Alpha = .05. The color of each dot shows the number of factors that were selected for that measure for that lab location based on the parallel analyses. Psychometric validity evidence was reported in the original Husnu & Crips (2010) and Shnabel & Nadler (2008) studies. * The Conscientiousness measure used in De Fruyt et al. (2000) consisted of two items, which are too few items to fit a factor model on. Therefore, none of these models converged."}

# plot for distribution of RMSEA and exact fit test
ggplot(unidimensionality_graph_data, aes(x = RMSEA, y = g, colour = as.factor(N_factors_capped))) +
  geom_boxplot(outlier.shape = NA, colour = "black") +
  geom_hline(yintercept = 6.5, color = "red", size = 1) +
  geom_vline(xintercept = 0.08, color = "red", size = 1) +
  geom_point(alpha = 0.3) +
  
  
  # adding in the convergence rate
  geom_text(data = unidimensionality_meta_data, label = format(convergence_rate_vec), x = 0.51, y = 1:19, size = 2.8, colour = "black") +
  
  # adding in the proportion of significant p-values
  geom_text(data = unidimensionality_meta_data, label = format(c(round(unidimensionality_meta_data$p_count[1:18] / unidimensionality_meta_data$converged_count[1:18], 2), "")), x = 0.58, y = 1:19, size = 2.8, colour = "black") +
  
  
  # setting the theme
  theme_minimal() +
  theme(legend.position = "bottom", plot.margin = unit(c(2, 5, 1, 1), "lines")) +
  
  
  # adding the necessary indicative texts
  annotation_custom(grob = textGrob(label = "Not Replicated", hjust = 0, gp = gpar(fontsize = 10)), ymin = 7.25, ymax = 7.25, xmin = 0.33, xmax = 0.33) +
  annotation_custom(grob = textGrob(label = "Replicated", hjust = 0, gp = gpar(fontsize = 10)), ymin = 6, ymax = 6, xmin = 0.37, xmax = 0.37) +
  annotation_custom(grob = textGrob(label = "1-Fac\nConv.", hjust = 0, gp = gpar(fontsize = 11)), ymin = 20.9, ymax = 20.9, xmin = 0.48, xmax = 0.48) +
  annotation_custom(grob = textGrob(label = "Sig.\nProp.", hjust = 0, gp = gpar(fontsize = 11)), ymin = 20.9, ymax = 20.9, xmin = 0.55, xmax = 0.56) +
  
  coord_cartesian(xlim = c(0, 0.45), clip = "off") +
  
  # we remove 7 & 15 lower bound because they are below 0
  # geom_point(data = unidimensionality_meta_data[c(1:6, 8:14, 16:17),], mapping = aes(x = pi.lb, y = c(1:6, 8:13, 15:17) ), color = "green", shape = 124, size = 2.5) + 
  # geom_point(data = unidimensionality_meta_data, mapping = aes(x = pi.ub, y = 1:19), color = "green", shape = 124, size = 2.5) + 
  
  scale_colour_manual(name = "N Factors (Parallel Analysis)", values = c("#0049b8", "#02bd8a", "#f2e30f"), na.value = "#000000") + 
  guides(colour = guide_legend(override.aes = list(alpha = 1))) +
  ylab("") +
  xlab("RMSEA (unidimensional CFA model)") 


```

Figure \@ref(fig:Plot1FactorSRMRCFI) shows the SRMR, and CFI fit indices for the same single factor model as was used to estimate the RMSEA and exact fit test. We found that, the mean SRMR was below .08 for `r apa_num((sum(tapply(unidimensionality_graph_data$SRMR, unidimensionality_graph_data$g, mean, na.rm = TRUE) < .08, na.rm = TRUE)), numerals = FALSE)` measures `r apa_num(sum(tapply(unidimensionality_graph_data$SRMR, unidimensionality_graph_data$g, mean, na.rm = TRUE) < .08, na.rm = TRUE) / length(levels(unidimensionality_graph_data$g)) * 100)`%, while the mean CFI was above .90 for `r apa_num((sum(tapply(unidimensionality_graph_data$CFI, unidimensionality_graph_data$g, mean, na.rm = TRUE) > .90, na.rm = TRUE)), numerals = FALSE)` measures `r apa_num(sum(tapply(unidimensionality_graph_data$CFI, unidimensionality_graph_data$g, mean, na.rm = TRUE) > .90, na.rm = TRUE) / length(levels(unidimensionality_graph_data$g)) * 100)`%. The mean standard deviation of the SRMR fit index across measures is `r apa_num(mean(tapply(unidimensionality_graph_data$SRMR, unidimensionality_graph_data$g, sd, na.rm = TRUE), na.rm = TRUE))`. For CFI this was `r apa_num(mean(tapply(unidimensionality_graph_data$CFI, unidimensionality_graph_data$g, sd, na.rm = TRUE), na.rm = TRUE))`. Overall, compared to the RMSEA, the SRMR and CFI fit indices show a more stable unidimensional fit. Especially the SRMR index shows considerably better fit compared to the RMSEA. When combining the results from all five unidimensionality checks, we observe that at least one of the five unidimensionality checks was passed for `r round((sum(unidimensionality_graph_data$unidimensional_check_count >= 1, na.rm = TRUE) / sum(!is.na(unidimensionality_graph_data$unidimensional_check_count))) * 100, 2)`% of the `r sum(!is.na(unidimensionality_graph_data$unidimensional_check_count))` labs with converged factor models, at least two for `r round((sum(unidimensionality_graph_data$unidimensional_check_count >= 2, na.rm = TRUE) / sum(!is.na(unidimensionality_graph_data$unidimensional_check_count))) * 100, 2)`%, at least three for `r round((sum(unidimensionality_graph_data$unidimensional_check_count >= 3, na.rm = TRUE) / sum(!is.na(unidimensionality_graph_data$unidimensional_check_count))) * 100, 2)`%, at least four for `r round((sum(unidimensionality_graph_data$unidimensional_check_count >= 4, na.rm = TRUE) / sum(!is.na(unidimensionality_graph_data$unidimensional_check_count))) * 100, 2)`%, and all five for `r round((sum(unidimensionality_graph_data$unidimensional_check_count == 5, na.rm = TRUE) / sum(!is.na(unidimensionality_graph_data$unidimensional_check_count))) * 100, 2)`%. The mean number of unidimensionality check passed was `r round(mean(unidimensionality_graph_data$unidimensional_check_count, na.rm = TRUE), 2)`.

```{r Plot1FactorSRMRCFI, warning = FALSE, fig.cap = "Distributions of calculated SRMR and calculated CFI of a single-factor model fit calculated for the responses on a measure at each lab location, across the nineteen measures for which raw data was available on which a factor model could be fitted. The left figure shows the SRMR result, where the red vertical line indicates our .08 RMSR cutoff value. The right figure shows the CFI results, where the red vertical line indicates our .90 CFI cutoff value. In both figures the red horizontal line separate the replicated from the non-repicated measures."}

# plot for distribution of SRMR
SRMR_plot <- ggplot(unidimensionality_graph_data, aes(x = SRMR, y = g, colour = as.factor(N_factors_capped))) +
  geom_boxplot(outlier.shape = NA, colour = "black") +
  geom_point(alpha = 0.3) +
  geom_hline(yintercept = 6.5, color = "red", size = 1) +
  geom_vline(xintercept = 0.08, color = "red", size = 1) +

  # setting the theme
  theme_minimal() +
  theme(legend.position = "bottom", plot.margin = unit(c(1, .5, 1, 1), "lines")) +
  scale_x_continuous(breaks = round(c(0, 0.1, 0.2),1)) +
  
  # adding the necessary indicative texts
  annotation_custom(grob = textGrob(label = "Not Replicated", hjust = 0, gp = gpar(fontsize = 8)), ymin = 7.25, ymax = 7.25, xmin = 0.18, xmax = 0.18) +
  annotation_custom(grob = textGrob(label = "Replicated", hjust = 0, gp = gpar(fontsize = 8)), ymin = 6, ymax = 6, xmin = 0.18, xmax = 0.18) +

  coord_cartesian(xlim = c(0, 0.2), clip = "off") +

  scale_colour_manual(name = "N Factors (Parallel Analysis)", values = c("#0049b8", "#02bd8a", "#f2e30f"), na.value = "#000000") + 
  guides(colour = guide_legend(override.aes = list(alpha = 1))) +
  ylab("") +
  xlab("SRMR (unidimensional\nCFA model)") 


# plot for distribution of CFI
CFI_plot <- ggplot(unidimensionality_graph_data, aes(x = CFI, y = g, colour = as.factor(N_factors_capped))) +
  geom_boxplot(outlier.shape = NA, colour = "black") +
  geom_hline(yintercept = 6.5, color = "red", size = 1) +
  geom_vline(xintercept = 0.9, color = "red", size = 1) +
  geom_point(alpha = 0.3) +

  # setting the theme
  theme_minimal() +
  theme(legend.position = "none", plot.margin = unit(c(1, .5, 1, 1), "lines"), axis.text.y = element_blank()) +
  scale_x_continuous(breaks = round(c(0, 0.2, 0.4, 0.6, 0.8, 1),1)) +
  
  coord_cartesian(xlim = c(0, 1), clip = "off") +
  
  scale_colour_manual(name = "N Factors (Parallel Analysis)", values = c("#0049b8", "#02bd8a", "#f2e30f"), na.value = "#000000") + 
  guides(colour = guide_legend(override.aes = list(alpha = 1))) +
  ylab("") +
  xlab("CFI (unidimensional\nCFA model)") 


(SRMR_plot + CFI_plot)

```

# Discussion

To assess psychological phenomena we need valid and reliable measurement, but previous research has raised doubts regarding the quality of measurement and its reporting. In this study, we extracted measurement reporting information from `r nrow(coded_data_replications)` measures, reported in original and replication studies from the Many Labs projects, and where possible, calculated indices for reliability and construct validity based on item response data. We found that reliability and validity evidence was often not reported. Additionally, the information relevant for reconstructing the measure in another study was rarely reported in full, especially among original studies. Finally, based on the Cronbach's Alpha and Unidimensionality indices we recalculated from the item response data. We observed inconsistent support for the reliability and construct validity across labs.

Similar to Flake et al @flakeConstructValidityValidity2022, we observed that many studies lacked the complete information needed to ensure the measurement could be reconstructed fully. In original studies, the administration format and procedure for each was not properly described for around a third of the applicable measures. The number of items and response options were each unclear for around an eighth of the applicable measures, while how the index was calculated, how the items were recoded and example items were presented for each a little under half of the applicable measures. One notable difference compared to Flake et al @flakeConstructValidityValidity2022 is that in our results replications did have more complete and transparent measurement reporting on this information compared to original research. We believe that this is in part due to the structured way in which the Many Labs protocols were written up. Several Many Labs protocols even contained specific sections to declare any deviations from the original methodology. The other reason may be due to the more lenient coding of transparent measurement reporting practices in our revised coding protocol. We primarily checked whether each piece of measurement information was reported at all, rather than the actual completeness or depth of that reporting. It is therefore possible that even if we coded a practice as being clearly reported, it may not be sufficiently detailed to fully facilitate reuse. However, if we coded a practice as unclear then --- given our lenient coding --- we can be fairly certain that that particular reporting practice was reported in insufficient detail. Therefore, it is concerning to note that even within our lenient coding scheme many key measurement information could not be traced in the reported information.

In line with existing research, we also found that the majority of the measures in original and replication studies do not report a reliability coefficient nor psychometric validity evidence [@beckmanHowReliableAre2004; @barryValidityReliabilityReporting2014; @flakeConstructValidationSocial2017; @maireadshawMeasurementPracticesLargescale2020; @flakeConstructValidityValidity2022]. The lack of reliability reporting is in part due to the fact that around half of the measures in original and replication studies were single item measures. While it is possible to check the reliability and validity of single item measures, these methods often require multiple measures or studies to allow for cross-validation [@leppinkWeNeedMore2017; @sarstedtSelectingSingleItems2016]. The additional effort required for this type of cross-validation, as we observed here is rarely taken. Still, even among multiple item measures reliability was reported for about one third of the measures in original studies, and only about a tenth in replication studies. Psychometric validity evidence was only checked for multiple item measures, and for these was reported even less commonly than reliability. Again, reporting was lower in replication studies. This could be because replications implicitly defer the responsibility of reporting psychometric validity evidence to the original study. However, as we noted before, and as our analyses of the reliability and unidimensionality reiterate, the reliability and validity of a measure varies across contexts. It is therefore still important for the reliability and validity to be described for the replications studies too. However, because the replication protocols were written before data collection, it makes sense that this information was not reported there. Still, we could also not find such information in both the available supplementary materials that we checked and the Many Labs reports.

The fact that the reliability and validity of a measurement was rarely reported is particularly concerning, because our analysis of the item response data revealed that the reliability and the unidimensionality of measures in our sample typically varied considerably across samples. While some measures did show consistently high reliability and stable unidimensionality, a similar number showed low average reliability and low unidimensionality. Most measures however fell somewhere in between, such that for the same measure reliability and unidimensionality met standards in some of the labs, while they were clearly below par in others. We also noted that some measures had convergence issues. Overall the inconsistent fit --- and in extreme cases lack of convergence ---  of the unidimensional factor model either indicates that these measures were not unidimensional, and thus not construct valid in this use case, or because our simplistic factor model itself was not an appropriate choice for the given data. Besides this split in potential explanations for our results, low sample size and a small number of items --- common with the replication measures --- likely also played a role in the lack of convergence. Combining all these points together, we stress that our results are not meant to be regarded as validity assessments of individual measures. This lies beyond the scope of our article and does not align with our analytical approach. Instead, our claim based on our results is that while for the measures we checked unidimensionality forms a prerequisite for them being construct valid, we observe that this prerequisite is not universally met across labs, for which we question the assurance of valid measurement within our sample.

Besides difference across contexts, reliability and validity of the measures in our sample were also impacted because many of them were modified from the original study. In our sample we noted that about two-thirds of the measures in replications were modified in some way from the original study. With modifications being this common it is best to be explicit in reporting these modifications and also to document the reliability and validity in replications. Minor differences between replication and original study are inevitable, but substantial changes between original and replication study raises the question whether it is still justified to label them as "direct" replication. For example, the conscientiousness measure used for the replication of De Fruyt et al @defruytCloningersPsychobiologicalModel2000 was reduced from the original NEO-PI-R 48-item measure to a two-item measure in the replication. It is highly unlikely that a measure with such a significant change would have a comparable validity and reliability to the original, and we did unsurprisingly observe convergence issues with validity for this measure as well as low reliability.

We observed some patterns that may indicate bias in measurement reporting. If a Cronbach’s Alpha in our sample was reported for a measure in an original study, then their average Cronbach’s Alpha observed among the replications were consistently relatively high. Meanwhile, if no Cronbach’s Alpha was reported in the original study, the reliability was often relatively low. If we take our replication results to be representative, it may indicate that authors are more inclined to report reliability coefficients when measures are above an acceptable threshold, but less so when they are below. This aligns with existing research indicating bias in reported Cronbach’s Alpha values, with an excess of reported values at the common acceptably reliable threshold value of .70, and low reporting just below .70 [@husseyAberrantAbundanceCronbachs2023]. In combination with our inconsistent reliability and validity evidence, this reporting bias creates a false sense of security when readers judge the reliability and construct validity of a measure  based on the reported information in the literature.

Our intent is not to specifically criticize the Many Labs replications nor these specific original studies, but rather use them as illustrative examples. Regardless, we would argue that full transparent measurement reporting is a responsibility of both replications and original studies. Otherwise, based on our results and the results of existing research, there is reason to distrust the reliability and validity of many psychological measures, especially when used in a new context. The validation required to ensure that we can trust that a measure is valid across contexts is simply not reported for most measures, and based on our own analyses of the item response data, there is reason to doubt that a measure is likely to be valid in any context. Even though validating measures is challenging and resource intensive, its necessity cannot be ignored. It is questionable to draw substantive interpretations from results based on responses to an unvalidated measure.

## Limitations & Future Research

Our main indicator of construct validity obtained from the response data was our unidimensionality test from a factor analysis. However, our factor analyses were not informed by theory, and were not specific to each measure. Instead the structure of the model and the test was the same for all measures. Even though we believe this is a reasonable proxy for construct validity --- as all measures we checked were used to form a singular index of one latent variable --- it is quite possible that a measure fails at our checks and is still a valid and reliable measure in its specific use case. These checks are designed as preliminary checks of construct validity that provide a general overview, not a complete thorough investigation of construct validity. Future research may want to deep dive into construct validity for one or a few measures and conduct thorough theory informed construct validity tests using openly available measurement response data to provide a complementary qualitative assessment of measurement validity in psychological research.

One of the most impactful limitations to our analyses was that we were unable to conduct informative inferential tests on our data. This was in large part because there were simply too few measures with the relevant information for our purposes. We initially planned to test the difference in reported reliability between original and replication studies, and the relation between reliability and replication outcome. However, due to the small number of reported reliabilities and measures for which reliability could be calculated we were unable to perform these tests. However, this limitation also highlights one of our most important findings. While we were unable to perform our preregistered tests, the fact that we could not is telling of the lack of measurement information reporting in our sample.

As another limitation, it is important to reflect on whether or not replication protocols and research articles can be fairly compared in terms of measurement reporting practices. A study’s description within a replication protocol is generally shorter than an article. The protocol may therefore lack the space needed to report on the measurement in full detail. There are three reasons why we believe protocols and articles remain mostly comparable. Firstly, articles are often also restricted in the amount of space they have available to devote to measurement [@gardinerEditorialMethodsPapers2019; @zogmaisterAssessingTransparencyMethods2024]. Second, in the revised protocol some measurement reporting items, such as whether example items were reported or not, also allowed for reporting this information in supplementary materials to count as good practice. Third, while we also allowed supplementary materials, few other files besides the protocol described the measurement details for the replication in any details, as such the protocol represent most of the available information on the measurement just like the original article does. If the information could not be found in the protocol, it was unlikely to be found anywhere else. A difference that was not resolved between articles and protocols was that the protocols were written before the measurement was conducted. Therefore we cannot directly compare protocol and replication on measurement information that is derived from the data – such as reliability coefficients and psychometric validity information. Still, future research should look if possible at replications that report on measurement information derived from the item response data.

There were additional sources of information we could have included. Data from original studies could have been used to recalculate reliability and validity indicators from to compare with the replication results. However, it is unlikely that a substantial number would have shared their data (many were conducted before the OSF and other Open Science initiatives were launched). This is likely different for more recent research [@hardwickeEstimatingPrevalenceTransparency2022; @hamiltonPrevalencePredictorsData2023a]. The original studies and replications of the Many Labs projects may also not be representative of typical original or replication research in psychology. Although we maintain the Many Labs projects are a relevant and high-quality source of studies, it may represent a standard that is not common throughout the field. Future research may wish to look at more representative replications and replications of more recent research.

## Recommendations

There are two main issues that we believe should be addressed to improve measurement practices: the common use of measures without established validity, and the lack of transparent measurement reporting. Unfortunately, fixing these issues requires considerable time and resources. Establishing validity would for a start require that the reliability and construct validity of each measure is analysed  within each context it is used in, a practice that currently occurs only rarely beyond a handful of common (clinical and personality) questionnaires [@anvariFragmentedFieldConstruct2025]. Meanwhile, improving transparency in reporting is an issue that many have tried to address for other aspects of academic articles such as preregistration deviations [@willrothBestLaidPlans2024], and constraints on generality [@simonsConstraintsGeneralityCOG2017], but widespread success has remained elusive. Despite these challenges, we remain optimistic that even small interventions can accelerate improvement. First of all, these two issues are interdependent. Improving transparency in measurement reporting can in turn facilitate validation efforts. Moreover, even incremental progress, such as establishing validity for a subset of frequently used measures, can meaningfully enhance the overall quality and credibility of psychological research.

Thus, our first recommendation is that reporting standards for measurement deserve a more central place in psychological science. We found that reporting reliability and validity information was the exception rather than the rule, and even basic information such as how many items were in the measure was reported with little consistency in our sample. The American Educational Research Association @americaneducationalresearchassociationStandardsEducationalPsychological2014 guidelines exist to guide researchers in the creation and validation of scales, but not how to report the measurement details when in use. The APA guidelines [@AmericanPsychologicalAssociation2020] do address many of the same reporting practices we assessed, however our findings and those of other research shows that uptake of these standards is still minimal. One aspect where reporting standards has seen some changes is in statistical reporting [@hardwickeStatisticalGuidanceAuthors2023]. Statistical reporting standards and guidelines have received, including recommendations from the American Statistical Association [@wassersteinASAStatementPValues2016]. The uptake of statistical reporting guidelines is even seen among journals. Although, in practice the guidelines vary in their content and enforcement [@hardwickeStatisticalGuidanceAuthors2023]. Still, we believe that for measurement any increase in reporting standards will be a welcome change compared to no push, and that a similar high-profile push is at least as relevant for measurement reporting standards as it is for improving scientific robustness as it is for statistical result reporting. After all, the data obtained through measurement is the foundation for nearly all empirical quantitative psychological research results.

As a related recommendation that could become a standard, we suggest that researchers should report and evaluate their measures using more informative indicators. Cronbach’s Alpha on its own gives only limited information the quality of a scale, and comes with strong assumptions [@cortinaWhatCoefficientAlpha1993; @sijtsmaUseMisuseVery2009]. Factor analytical evidence and McDonald’s Omega are more informative indicators that should be presented in favor of or in tandem with Cronbach’s Alpha.

Our second recommendation is that, as a scientific community, we need to systematically validate a greater proportion of our measures. But to able to do so, the measures should first of all be reusable --- here, transparent measurement reporting is crucial. Then, researchers should actually reuse existing validated measures, rather than make new ones on the fly and use these unvalidated measures. To encourage and enable the reuse of existing measures, Elson et al @elsonPsychologicalMeasuresArent2023 have proposed an open repository of measurement protocols to facilitate the discovery of measures and building an evidence base. Furthermore, they suggested that journals should implement the Standardisation Of BEhavior Research (SOBER) guidelines to specifically address issues of flexibility and norming in measurement so that measures remain comparable across studies. If researchers then reuse a measure across different context its reliability and validity needs to be compared across multiple occasions to assess in which contexts, if any, the measure is valid. 

If more systematic measurement validation is to take place, it is equally important that the resulting evidence becomes an integral and respected part of the scientific literature. We therefore suggest that the scientific literature should include a dedicated space for systematic and continuous measurement validation studies, similar to the space proposed for replication studies to assess the robustness of effects. Moreover, replication studies themselves could contribute to measurement validation: by paying closer attention to measurement evaluation and reporting, replication efforts could simultaneously serve as a robustness of an effect and a validation test of the measure. Finally, to enable cumulative progress, a greater proportion of item response data should be made available to the scientific community. Ideally, this includes item-level responses and relevant participant characteristics, allowing future researchers to (re-)evaluate a measure’s validity and reliability in relation to theory. When full data sharing is not possible, providing summary statistics (such as variance–covariance matrices, means, and standard deviations) can still allow others to conduct factor-analytic assessments of measurement quality and even meta-analytic evaluation across contexts. 

Finally, we recommend to researchers seeking to replicate a study that they should first evaluate the measurement of the original study before replicating. Reliable and valid measurement is essential for an informative replication. If the original measures are unreliable, discrepancies between the original and replication results become more likely, thereby reducing the interpretability and value of both studies. Moreover, repeating a study that relies on invalid measures does little to advance substantive knowledge and instead risks perpetuating misleading or meaningless findings. Furthermore, to make meaningful replication possible, original studies must report sufficient measurement details to allow others to reconstruct the instruments and procedures used. Without such transparency, it is impossible to know whether differences between the original and replication studies reflect true effects or merely differences in measurement. When the original measures are unreliable, invalid, or insufficiently documented, we recommend that researchers instead use their resources to conduct a replication of a study with reliable, valid and well-documented measurement. When replicating another study is not an option, we advise the replicating researcher to first attempt a conceptual replication using a validated measurement. Afterwards, a direct replication can be performed based on the conceptual replication to further assess the robustness of the effect.

# Conclusion

Cumulative knowledge on psychological phenomena starts with our ability to accurately measure the constructs of interest. For this we need valid and reliable measurement. Yet, in our sample of Many Labs replications and original studies we observed that the construct validity and reliability of psychological measurements is rarely reported and based on our analyses were seldom sufficient enough across contexts to assume that constructs are generally measures as intended. Furthermore, the lack of transparency in measurement reporting hampers the reuse of existing measures. A change in the use and reporting of measurement is necessary. Otherwise, psychological effects lack the measurement fundamental to establish that they are related to true psychological phenomena. Fortunately, even small steps towards improved reporting practices, data and materials sharing, and normalizing measurement validation can spark the proliferation of validated measurement.

## Conflicts of Interest

The author(s) declare that there were no conflicts of interest with respect to the authorship or the publication of this article.

## ORCID iDs

Cas Goos <https://orcid.org/0009-0005-3792-4148>

Marjan Bakker <https://orcid.org/0000-0001-9024-337X>

Jelte M. Wicherts <https://orcid.org/0000-0003-2415-2933>

Michèle B. Nuijten <https://orcid.org/0000-0002-1468-8585>

## Funding

The preparation of this article was supported by the Vici grant VI.C.221.100 awarded to Jelte M. Wicherts from the Dutch Research Council (NWO).

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
