---
title             : "The Impact of Measurement Practices and Measurement Error on Construct Validity and Replication Outcomes in Psychological Science"
shorttitle        : "Measurement Quality and Replicability"

author: 
  - name          : "Goos, C."
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Professor Cobbenhagenlaan 125, 5037 DB, Tilburg, The Netherlands"
    email         : "casgoos99@gmail.com"
    role: # Contributorship roles (e.g., CRediT, https://credit.niso.org/)
      - "Conceptualization"
      - "Data curation"
      - "Formal Analysis"
      - "Investigation"
      - "Methodology"
      - "Visualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
  - name          : "Nuijten, M.B."
    affiliation   : "1"
    role:
      - "Project Administration"
      - "Writing - Review & Editing"
      - "Supervision"

affiliation:
  - id            : "1"
    institution   : "Tilburg University"

authornote: |
  1 Department of Methodology and Statistics, Tilburg School of Social and Behavioral Sciences, Tilburg University, Tilburg, NL. 

  Enter author note here. Caspar?

abstract: |
  Recently, the influence of measurement related issues on replicability has received more investigations. 
  In particular, studies have indicated that low reliability, and poor measurement reporting can negatively influence the replicability of an effect. 
  Building on this new line of research, this thesis assessed these influences in large scale replication projects. 
  Protocols from the Many Labs replication projects, and the original articles that the replications were based on were used to assess the  reporting of the measures, while publicly available data from the Many Labs replications was used to assess the reliability of the measures. Reliability did not relate to replicability, and was generally consistent between labs for each measure in the Many Labs projects. 
  The quality of measurement reporting in replication protocols did however relate to replicability.
  Additionally, tentative evidence was found for a relation between quality of measurement reporting in the protocol of a replication and the quality of measurement reporting in its related original study.
  Recommendations are given for researchers and replicators in order to improve measurement practices in replication, and ward its negative impact on replicability. 
  Additionally, directions are given for future research investigating the relationship between measurement issues and replicability.


keywords          : "reliability, measurement, measurement reporting, replicability, construct validity"
wordcount         : "X"

bibliography      : ["r-references.bib", "references.bib"]

floatsintext      : yes
linenumbers       : no
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : "man"
output            : papaja::apa6_pdf
knit              : worcs::cite_all
---

```{r setup, include = FALSE}
# loading R libraries
library(papaja)
library(worcs)
library(psych)

# loading source script
source(file = "../source_script.R")

# Code below loads the processed data. The raw data was prepared for analysis in 'prepare_data.R.
load_data() 

# creates a reference list for all used R packages and the installed R version (does not include Rstudio)
r_refs("r-references.bib")

### DON'T FORGET THAT I SHOULD REMOVE HERE THE R PACKAGE REFERENCES FROM MY REFERENCES BIB AND SHOULD HAVE THE R=REFERENCES BIB TAKE CARE OF THAT INSTEAD.
```


This is an example of a non-essential citation [\@ @vanlissaWORCS2021]. If you change the rendering function to `worcs::cite_essential`, it will be removed.

```{r analysis-preferences}
# Seed for random number generation
set.seed(17042023)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Introduction

For many years, methodologists and statisticians have raised concerns about the way that research practices [@cumming2014new; @wicherts2016degrees], reporting practices [@bakker2011mis; @nuijten2016prevalence], and publication decisions [@sterling1959publication; @bakker2012rules; @giner2012science] may undermine the credibility of psychology's scientific claims. Replicability of a finding is seen as one crucial step in establishing its credibility [@nosek2022replicability]. In order to investigate the replicability of psychological science, the large scale Reproducibility Project: Psychology (RPP) was set-up [@osc2015estimating]. Replications of 100 published studies were performed using similar materials, procedures, and analyses, while using a new sample of participants for each study. They found however, that in many instances the results did no match those of the original studies. This lack of replicability of psychological findings illustrated that psychology was facing a so-called 'replication crisis' [@hughes2018psychology; @giner2019crisis].

As a response, the demand to verify the credibility of psychological findings through replications rose. In order to attempt replications of various psychological findings, numerous large scale projects were set-up [@klein2014data; @camerer2016evaluating; @camerer2018evaluating; @ebersole2016many; @ebersole2020many; @klein2018many; @klein2022many]. Among these were the Many Labs projects, which operated similarly to the RPP, with the additional feature that each original article was replicated multiple times across different labs in various locations around the world. The results from these replication projects generally showed smaller effects than those reported in the original studies. Similarly, effects that had reached statistical significance in the original no longer did across the aggregate results of its replications.

Several explanations for this discrepancy between original and replication findings have been suggested and investigated, with many being centred around the concept of Questionable Research Practices [QRPs, @john2012measuring]. QRPs are practices, which either misrepresent or omit essential methodological information that is necessary for the evaluation of the research. Without this information, a reader will not know whether the result was in line with the author's expectation or simply the result of repeated trial and error until a suitable result was found [@simmons2011false]. Consequently, much of the discourse and the subsequent proposed solutions have been centred around preventing these QRPs through increased transparency in scientific reporting. Preregistrations are one method for preventing QRPs. Preregistrations provide a clear description of the originally planned methodology of a study, which remains publicly available alongside the final report of the study. As a result, the author's original expectations and intentions can be accessed by the reader [@nosek2022replicability].

However, many researchers have also raised concerns that other factors, such as low power [@stanley2018what], lack of strong theoretical foundations [@eronen2021theory], and validity [@finkel2017replicability], which have not been given as much attention, are just as if not more responsible for the lack of successful replications found in psychology. An idea which has gained more traction recently is that measurement related issues have a large problematic influence on replicability. In particular, studies have focused on the effects of issues such as measurement error and questionable measurement practices on replicability [@stanley2014expectations; @shaw2020measurement; @flake2017construct; @flake2020measurement].

Measurement in psychological science is defined in large part by one overarching issue: psychological constructs, such as affective state and intelligence, cannot be measured directly. As a result, psychologists face two primary challenges in the use of measurement: First of all, the measure has to assess the construct it intends to measure, even though it can only do so indirectly. Secondly, because the measurement is indirect, it also comes with some degree of error surrounding its assessment of the construct. The first issue relates to the concept of validity, and specifically construct validity. Validity refers to the overall extent to which a measure measures what it is supposed to [@borsboom2004concept], while construct validity specifically refers to the substantial relation between the item scores on a psychological measure and the psychological construct it intends to measure. In order for indirect assessment of psychological constructs to be a viable approach, the measure needs to have construct validity [@cook2002experimental]. However, both original and replication research are often not reported with sufficient details on construct validity [@shaw2020measurement; @flake2022construct]. The second issue relates to the concept of measurement error and reliability. Measurement error indicates the amount of variation in item scores that is the result of the used measures inaccuracies, and not due to differences in the underlying psychological construct of interest. While what should be considered acceptable levels of measurement error is a subject of debate, and dependent upon context [@cho2015cronbach], it is generally agreed upon that too much measurement error is problematic. If measurement error is too large, the item scores relate to a construct no more than they do to random noise. As a result, the interpretability of these scores as indicators of a psychological construct is limited.

@stanley2014expectations demonstrated, using a simulation study, the drastic effect measurement error can have on attempts to verify or falsify original findings. In practice, measurement error is commonly indicated using score-reliability. Score-reliability is an index of consistency of a measure, showing how much the items converge to the same point, with the point being the measured variable. Thus, @stanley2014expectations generated scalar item data with levels of score-reliability that mimicked what is standard in psychological research, to ensure that the resulting degree of measurement error was representative. Using this data, they found that the degree of measurement error can explain a large amount of the variation in observed effects found in replication research. It could therefore be that a failed replication was simply the result of measurement of the true effect varying from the original, rather than the true effect itself being different from what was originally reported. As a result, interpretations of replications might not be based on tests of the true effect, if the impact of measurement error is not attenuated for.

Another key aspect in evaluating the use of measures is how they are reported. Questionable Measurement Practices (QMPs) have been coined as a conjugate term to QRPs. QMPs are practices that decrease the information necessary to evaluate the measurement of a study. They range from lack of transparency and unclear motivation in choice of measure, to poor justification for modifications of measure and procedure of any sourced measures [@flake2020measurement]. @flake2022construct coded the presence of a series of QMPs in the replication report in both the replication protocols of each replication in the RPP, as well as the original articles the replications were based on. They found that QMPs linked to issues for replicators in recreating the measurement to closely match the measurement in the original. As a result, there was a lack of measurement related information in both replication reports, and original articles. Without this information, it becomes difficult to establish that original and replication are measuring the same construct(s), which has detrimental consequences for direct replication attempts.

This thesis aims to investigate the influence of measurement error and QMPs on replicability, using data and reports from the large scale Many Labs replication projects. One goal is to expand the research by [@stanley2014expectations] with a practical example. Whereas they obtained their findings using a simulation study, this thesis intended to investigate the scope of their findings in practice. QMPs in the context of replication research were already investigated by [@flake2022construct]. However, the investigations remained mostly descriptive. This thesis aims to expand upon the findings of [@flake2022construct] through the use of a new set of replication data, and by investigating the relation between QMP and replicability.

## Measurement Error

As a first step it is worth investigating what the state of measurement error is in original and psychological research, in order to assess the scope of @stanley2014expectations's findings in practice. Thus, this thesis will investigate the following:

-   RQ1a. What is the degree of score reliability in replications of psychological research?

-   RQ1b. What is the degree of score reliability in original psychological research?

It is difficult to suggest an expected or informed cut-off value for what could be considered a sufficient score reliability coefficient for both original and replication studies. While suggested cut-off points for reliability do exist [@nunnaly1994assessment], they are not without their criticism [@cho2015cronbach; @crutzen2017scale]. Instead a comparison of scores between original and their associated replication studies should be of more substantive interest. Since direct replications intend to investigate effects in nearly identical scenarios with nearly identical measures as the original study, it is expected that measurement errors are as present in replication research as in original psychological research.

-   H1. No difference is present in the degree of score reliability between replication research and original psychological research.

While reliability estimates are generally considered a measure specific feature, this is false. They are context dependent, and as a result the estimates of an effect can differ across contexts [@cho2015cronbach; @pauly2018resampling]. With large enough differences across contexts, the effect estimates between different replication attempts could deviate from each other beyond direct comparison. For instance, suppose that we have a measure of multiple items assessing agreeableness in relation to both parents as well as peers. In a non-traditional society we might find high reliability for this measure, because participants that score high on one item, score high on another item since they are generally agreeable, while participants that score low one item, score low for all the other items. On the other hand, in a more traditional society we might find that reliability is low, because agreeableness towards parents is normative in that society, and as a result items relating to agreeableness towards parents may not be indicative of general agreeableness as a personality trait. In this example, a high score on the measure in the first context relates more strongly to a single agreeableness construct compared to the second context. The findings across these two populations would then no longer be directly comparable. Consequently, the value of any direct replication would become limited. To investigate this potential problem, the variation in score reliability is assessed for each measure when used in different contexts.

-   RQ2. To what degree do reliability estimates differ within a replication set of the same original study?

Even though reliability estimates are context dependent, sufficient protocol structure can counteract the variance in reliability estimates. An example of the standardising impact a structured protocol can have on reducing variability within an effect can be found in investigations into the closely related issue of heterogeneity [@patacchini2007unobserved]. Heterogeneity refers to the true variation in the true effect size due to minor but key differences in context, environment, and procedure. Heterogeneity is yet another proposed explanation for failed replications. This concern even prompted a Many Labs centred around the testing of heterogeneity in replications [@klein2018many]. However analyses from Many Labs 2, and @olsson2020heterogeneity showed little empirical evidence for widespread heterogeneity in effects amongst replications. These findings demonstrate that it is possible to reduce variability as the result of context using structured protocols, as was done in the Many Labs projects this thesis analyzed.

-   H2. There is no significant variation in the reliability estimates of replications of the same original study.

This thesis investigated whether there is something in the measures used for replicated findings that sets them apart from those used for non-replicated findings. In particular, this thesis extends research by @stanley2014expectations on the relationship between reliability and replicability, by using data from real life replication projects.

-   RQ3. What is the association between replication study score reliability and replication outcome?

Greater score-reliability means greater consistency in estimation as variance around the estimate of the true effect is decreased. Consequently measures across different occasions, including between original and replication, should also become more consistent [@nunnaly1994assessment]. When original and replication research share consistent result, and as long as the studied effect is true, then on average their statistical conclusions should converge.

-   H3. Score reliability in replication research is positively associated with successful replication of an original finding.

Reliability is only one aspect by which a measure can be judged, with validity being another important way to determine the scientific rigour of a measure [@roberts2006reliability]. Therefore as part of the exploratory analyses, validity was assessed for the same data source used to test the aforementioned hypotheses. Specifically, the unidimensionality of the measure was determined, meaning whether or not the items of a measure all related to a single variable.

Validity of measures can be assessed not only through features found in the data. The measure must also be chosen and implemented in a way that enables proper measurement of the variables [@flake2020measurement]. Any scientific report should therefore clearly motivate the measure and procedure that was used. Without clear reporting on these aspects, the validity of measurement will remain uncertain. Therefore, the other tests of this paper focused on the reporting aspects of measurement, assessed through QMPs.

## Questionable Measurement Practices

@flake2022construct descriptively investigated QMPs within the reproducibility project psychology [@osc2015estimating]. This thesis seeks to conceptually replicate some of their findings within the Many Labs replication projects.

-   RQ4a. To what degree are QMPs present in replications of psychological research?
-   RQ4b. To what degree are QMPs present in original psychological research?

The investigations by @flake2022construct have shown not only that QMPs are common in psychological research, they can be even more common in replications reports. However, the reporting of the Many Labs projects differ from the RPP data used in @flake2022construct. The structured and preregistered protocols used for the Many Labs projects were hypothesized to lead to consistent reporting of important measurement details, and as a result contain less QMPs.

-   H4. QMPs are expected to be more frequent in original psychological research than in replication research.

QMPs put the validity of a study into question. If there is a lack of transparency and the used measures are invalid, replication results become difficult to interpret [@flake2020measurement]. With a lack of transparency, closely mimicking the original's measurement becomes challenging. Furthermore, if a replication does not properly measure the variables of the original effect, the effect analysed within the replication may not be the same as the original. For these reasons, establishing a link between QMPs and non-replicability should confirm suspicions of their detrimental effects on replications [@flake2017construct; @flake2020measurement; @flake2022construct].

-   RQ5. What is the association between QMPs in replication studies and replication outcome?

Fewer QMPs means that measurement decisions in a report are transparent with the number arbitrary methodological decisions being limited as well. Fewer arbitrary decisions means less ways that a replicator can deviate from the original's intended effect. One of the Many Labs projects even touched on this concept [@ebersole2020many]. In this project, peer-review of replication protocols by original authors was used to ensure that approved methods were used. Additionally, it has been observed that authors who promote greater transparency in their work, have on average stronger evidence for their hypotheses [@wicherts2011willingness]. Finally, the results shown in @protzko2020high illustrate that when original studies are transparent, preregistered, and have adequate sample size, subsequent replication attempts find nearly the same effect size as the original. All of these findings point towards a positive relation between successful replication and various types of good research practices. A similar relation is expected for good measurement practices.

-   H5. QMPs in replication research are negatively associated with replication of an original finding.

@flake2022construct have already demonstrated the issues replicators face in transparently reporting the appropriate evidence for the measures they used. That study identified among other issues, a lack of available information in original articles. If essential information on a measure is not available in an original article, it may be difficult to know the specifics of that measure when used in a replication. In that case, the lack of proper reporting on measurement would impact the transparency and validity of future replications. Consequently, the article's findings become difficult to verify. To provide insight into this issue, an investigation of the relationship between original and replication QMPs was included.

-   RQ6. What is the association between QMPs in original research and QMPs in replications of psychological research?

If the relationship exists, it is expected that questionable measurement practices in an original paper might cause a spill-over effect of QMPs into the replication. If the original paper does not specify how a measure was used, the replication may implement it differently. In that case, the replication can no longer be considered a direct replication. Additionally, investigations by @shaw2020measurement have shown that more validity evidence in an original article is associated with better psychometric properties in the replication sample. Based on these arguments, hypotheses are expected to be associated across original and replication studies.

-   H6. Total number of QMPs in original psychological research is positively related to total number of QMPs in replication research.

# Method

Data collection, coding protocol, and planned analyses were all preregistered. The preregistration and supplementary materials can be found on this thesis' OSF page: <https://osf.io/9r8yt/>. Any deviations from the preregistration are explicitly mentioned in the text. This manuscript was generated using the Workflow for Open Reproducible Code in Science [WORCS version 0.1.1, @vanlissaWORCS2021] to ensure reproducibility and transparency. All code and XXX data are available at <https://github.com/CasGoos/measurement_and_replication.git>.

## Sample

The data used for the analyses in this thesis consists of three main sources: replication datasets, preregistered replication protocols, and original study articles. The data came from the Many Labs series of studies. In particular, data from Many Labs 1, 2, 3, & 5 was used [@klein2014data; @klein2018many; @ebersole2016many; @ebersole2020many]. Data from Many Labs 4 [@klein2022many] was not part of the sample, as there was no publicly available protocol which contains the information needed to code QMPs. Additionally, the replication of [@crosby2008we] in Many Labs 5 did not contain contain data suitable for this project. The replication made use of videos and eye-tracking as measures, which does not match this thesis' focus on scale and item measures.

The total number of psychological findings for which replication was attempted across the four included Many Labs projects amounts to a total of 60: 13 in Many Labs 1; 28 in Many Labs 2; 10 in Many Labs 3; & finally 9 out of the 10 tested findings in Many Labs 5. Since each replication set is related to an original study, the number of effects in original studies matches the number of effects in replication research.

However, the unit of measurement was not a single study or replication. Instead each measure of a unique variable within a Many Labs project was treated as the unit of measurement. The reason being that studies could have and did use multiple measures. Additionally, measures can assess multiple dimensions. These dimensions, meaning unique variables, were separated into different units whenever possible, otherwise the measure was discarded from analysis. However, not all measures of unique variables were used. Only data from measures of primary variables, meaning those variables included in the primary test of the replication, were used. This was done to ensure that the analyses could link directly to the replicability of a study. Acquiescence bias checks, manipulation checks, measures used for pilot testing, and measures added for exploratory analyses in replication studies were not included. The result was a total sample size of 77 measures: 14 in Many Labs 1; 35 in Many Labs 2; 15 in Many Labs 3; & 13 in Many Labs 5. Again, the sample was equivalent between replications and original studies.

### Replication Datasets

The replication datasets refer to the data of each of the replication attempts for each of the original psychological effects undertaken as part of a Many Labs study. The specific data was extracted from the replication datasets for each relevant measure. The measure had to consist of multiple scalar items to be considered relevant for the related analyses. Single item measures were not usable for the analyses on score reliability [@wanous1996estimating] as they do not translate directly to the findings of @stanley2014expectations, which this thesis is following up on.

For Many Labs 1, all the relevant replication data was available in a single csv file. For Many Labs 2 & 3 the situation was similar, only here the files were split across two csv files for each project. For Many Labs 5, the data was made available for each of the ten replication sets separately. Three out of the ten datasets were furthermore split across the various locations at which they were taken. If cleaned data was available this was chosen over raw data. Pilot data were omitted from the analyses entirely. This resulted in suitable measure data from eighteen replication sets spread across on average 40 lab locations for the analyses of hypotheses 2 & 3.

```{r Cleaning Replication Datasets Data}
##### Data already available, code does not need to be run!
### ML 1
# 1.3
data_1.3_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[22:29])
colnames(data_1.3_clean)[1] <- "g"
# 1.10
data_1.10_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[108:115])
colnames(data_1.10_clean)[1] <- "g"
# 1.11
data_1.11_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[73:76])
colnames(data_1.11_clean)[1] <- "g"
# 1.12.1
# not found
# 1.12.3
data_1.12.3.1_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[54:59])
colnames(data_1.12.3.1_clean)[1] <- "g"
data_1.12.3.2_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[60:65])
colnames(data_1.12.3.2_clean)[1] <- "g"

### ML 2
# 2.2
data_2.2_clean <- cbind(as.factor(data_2.2[[5]]), data_2.2[6:11])
colnames(data_2.2_clean)[1] <- "g"
# 2.3
# data does not appear suitable
# 2.4.1
data_2.4.1_clean <- cbind(as.factor(data_2.4.1[[5]]), data_2.4.1[6:11])
colnames(data_2.4.1_clean)[1] <- "g"
# 2.4.2
data_2.4.2_clean <- cbind(as.factor(data_2.4.2[[5]]), data_2.4.2[6:14])
colnames(data_2.4.2_clean)[1] <- "g"
# 2.8.2
data_2.8.2_clean <- cbind(as.factor(data_2.8.2[[6]]), data_2.8.2[9:13])
colnames(data_2.8.2_clean)[1] <- "g"
# 2.10.1
data_2.10.1_clean <- cbind(as.factor(data_2.10.1[[5]]), data_2.10.1[6:11])
colnames(data_2.10.1_clean)[1] <- "g"
# 2.12.1
data_2.12.1_clean <- cbind(as.factor(data_2.12[[5]]), data_2.12[c(6,7,8,9,10,31,32,33,34,35)]) 
data_2.12.1_clean[3465:6905,2:6] <- data_2.12.1_clean[3465:6905,7:11]
data_2.12.1_clean <- data_2.12.1_clean[1:6]
colnames(data_2.12.1_clean)[1] <- "g"
# 2.12.2
data_2.12.2_clean <- cbind(as.factor(data_2.12[[5]]), data_2.12[c(11,14,15,18,19,22,24,27,28,29,36,39,40,43,44,47,49,52,53,54)]) 
data_2.12.2_clean[3465:6905,2:11] <- data_2.12.2_clean[3465:6905,12:21]
data_2.12.2_clean <- data_2.12.2_clean[1:11]
colnames(data_2.12.2_clean)[1] <- "g"
# 2.12.3
data_2.12.3_clean <- cbind(as.factor(data_2.12[[5]]), data_2.12[c(12,13,16,17,20,21,23,25,26,30,37,38,41,42,45,46,48,50,51,55)]) 
data_2.12.3_clean[3465:6905,2:11] <- data_2.12.3_clean[3465:6905,12:21]
data_2.12.3_clean <- data_2.12.3_clean[1:11]
colnames(data_2.12.3_clean)[1] <- "g"
# 2.15
data_2.15_clean <- cbind(as.factor(data_2.15[[5]]), data_2.15[8:12])
colnames(data_2.15_clean)[1] <- "g"
# 2.19.1
# difficult to extract
# 2.19.2
# difficult to extract
# 2.20
data_2.20_clean <- cbind(as.factor(data_2.20[[5]]), data_2.20[6:45] - 1) 
data_2.20_clean[3729:7396,2:21] <- data_2.20_clean[3729:7396,22:41]
data_2.20_clean <- data_2.20_clean[1:21] 
colnames(data_2.20_clean)[1] <- "g"
# 2.23
data_2.23_clean <- cbind(as.factor(data_2.23[[5]]), data_2.23[c(7,8,12,13,15)])
colnames(data_2.23_clean)[1] <- "g"


### ML 3
# 3.2.1
data_3.2.1 <- cbind(as.factor(data_ml3[[1]]), data_ml3[77:86] - 1)
data_3.2.1.1_clean <- na.omit(data_3.2.1[1:6])
colnames(data_3.2.1.1_clean)[1] <- "g"
data_3.2.1.2_clean <- na.omit(data_3.2.1[c(1, 7:11)])
colnames(data_3.2.1.2_clean)[1] <- "g"
# 3.5
# data appears unusable
# 3.7.1
data_3.7.1_clean <- na.omit(cbind(as.factor(data_ml3[[1]]), data_ml3[38:42]))
colnames(data_3.7.1_clean)[1] <- "g"
# 3.7.2
data_3.7.2_clean <- na.omit(cbind(as.factor(data_ml3[[1]]), data_ml3[89:94]))
colnames(data_3.7.2_clean)[1] <- "g"
# 3.8.1
# a single measure was reported
# 3.8.2
data_3.8.2_clean <- na.omit(cbind(as.factor(data_ml3[[1]]), data_ml3[29:30])) 
colnames(data_3.8.2_clean)[1] <- "g"


### ML 5
# 5.1.1
data_5.1.1_clean <- cbind(as.factor(data_5.1[[2]]), data_5.1[13:33])
colnames(data_5.1.1_clean)[1] <- "g"
# 5.1.2
# difficult to separate .1 from .2
# 5.4
data_5.4_clean <- cbind(as.factor(data_5.4[[1]]), data_5.4[18:41])
colnames(data_5.4_clean)[1] <- "g"
# 5.5.1 & 5.5.2
# from this dataset it appears that this data will be difficult to use.
# 5.5.2
# also difficult to use
# 5.7 
data_5.7_clean <- cbind(as.factor(data_5.7[[3]]), data_5.7[c(25, 34, 35, 36, 37, 38, 39, 40, 41, 42)])
colnames(data_5.7_clean)[1] <- "g"
# 5.9.1
data_5.9.1_clean <- na.omit(cbind(as.factor(data_5.9.1[[4]]), data_5.9.1[c(79, 83, 87, 91, 95, 98, 101)]))
colnames(data_5.9.1_clean)[1] <- "g"

### to summarize
# total number of likely usable: 15 (1.10, 1.11, 1.12.3.1, 1.12.3.2, 2.10.1, 
# 2.12.1, 2.12.2, 2.12.3, 2.15, 2.23, 3.7.1, 3.7.2, 3.8.2, 5.7, 5.9.1)
# total number of maybe usable: 6 (1.3, 2.20, 3.2.1.1, 3.2.1.2, 5.1.1, 5.4)
# total alpha coefficient comparison usable: 3 
#                                           1.11: 0.82
#                                           3.7.2: 0.67
#                                           5.9.1: 0.84


### Saving to Intermediate Data Folder
closed_data(data = data_1.3_clean, filename = "Data/IntermediateData/data_1.3_clean.csv",
            codebook = "Data/IntermediateData/codebook_data_1.3_clean.Rmd", 
            value_labels = "Data/IntermediateData/value_labels_data_1.3_clean.yml", 
            synthetic = FALSE) 

closed_data(data = data_1.10_clean, filename = "Data/IntermediateData/data_1.10_clean.csv",
            codebook = "Data/IntermediateData/codebook_data_1.10_clean.Rmd", 
            value_labels = "Data/IntermediateData/value_labels_data_1.10_clean.yml", 
            synthetic = FALSE) 

closed_data(data = data_1.11_clean, filename = "Data/IntermediateData/data_1.11_clean.csv",
            codebook = "Data/IntermediateData/codebook_data_1.11_clean.Rmd", 
            value_labels = "Data/IntermediateData/value_labels_data_1.11_clean.yml", 
            synthetic = FALSE) 

closed_data(data = data_1.12.3.1_clean, filename = "Data/IntermediateData/data_1.12.3.1_clean.csv",
            codebook = "Data/IntermediateData/codebook_data_1.12.3.1_clean.Rmd", 
            value_labels = "Data/IntermediateData/value_labels_data_1.12.3.1_clean.yml", 
            synthetic = FALSE) 

closed_data(data = data_1.12.3.2_clean, filename = "Data/IntermediateData/data_1.12.3.2_clean.csv",
            codebook = "Data/IntermediateData/codebook_data_1.12.3.2_clean.Rmd", 
            value_labels = "Data/IntermediateData/value_labels_data_1.12.3.2_clean.yml", 
            synthetic = FALSE) 

closed_data(data = data_2.10.1_clean, filename = "Data/IntermediateData/data_2.10.1_clean.csv",
            codebook = "Data/IntermediateData/codebook_data_2.10.1_clean.Rmd", 
            value_labels = "Data/IntermediateData/value_labels_data_2.10.1_clean.yml", 
            synthetic = FALSE) 

closed_data(data = data_2.12.1_clean, filename = "Data/IntermediateData/data_2.12.1_clean.csv",
            codebook = "Data/IntermediateData/codebook_data_2.12.1_clean.Rmd", 
            value_labels = "Data/IntermediateData/value_labels_data_2.12.1_clean.yml", 
            synthetic = FALSE) 

closed_data(data = data_2.12.2_clean, filename = "Data/IntermediateData/data_2.12.2_clean.csv",
            codebook = "Data/IntermediateData/codebook_data_2.12.2_clean.Rmd", 
            value_labels = "Data/IntermediateData/value_labels_data_2.12.2_clean.yml", 
            synthetic = FALSE) 

closed_data(data = data_2.12.3_clean, filename = "Data/IntermediateData/data_2.12.3_clean.csv",
            codebook = "Data/IntermediateData/codebook_data_2.12.3_clean.Rmd", 
            value_labels = "Data/IntermediateData/value_labels_data_2.12.3_clean.yml", 
            synthetic = FALSE) 

closed_data(data = data_2.15_clean, filename = "Data/IntermediateData/data_2.15_clean.csv",
            codebook = "Data/IntermediateData/codebook_data_2.15_clean.Rmd", 
            value_labels = "Data/IntermediateData/value_labels_data_2.15_clean.yml", 
            synthetic = FALSE) 

closed_data(data = data_2.20_clean, filename = "Data/IntermediateData/data_2.20_clean.csv",
            codebook = "Data/IntermediateData/codebook_data_2.20_clean.Rmd", 
            value_labels = "Data/IntermediateData/value_labels_data_2.20_clean.yml", 
            synthetic = FALSE) 

closed_data(data = data_2.23_clean, filename = "Data/IntermediateData/data_2.23_clean.csv",
            codebook = "Data/IntermediateData/codebook_data_2.23_clean.Rmd", 
            value_labels = "Data/IntermediateData/value_labels_data_2.23_clean.yml", 
            synthetic = FALSE) 

closed_data(data = data_3.2.1.1_clean, filename = "Data/IntermediateData/data_3.2.1.1_clean.csv",
            codebook = "Data/IntermediateData/codebook_data_3.2.1.1_clean.Rmd", 
            value_labels = "Data/IntermediateData/value_labels_data_3.2.1.1_clean.yml", 
            synthetic = FALSE) 

closed_data(data = data_3.2.1.2_clean, filename = "Data/IntermediateData/data_3.2.1.2_clean.csv",
            codebook = "Data/IntermediateData/codebook_data_3.2.1.2_clean.Rmd", 
            value_labels = "Data/IntermediateData/value_labels_data_3.2.1.2_clean.yml", 
            synthetic = FALSE) 

closed_data(data = data_3.7.1_clean, filename = "Data/IntermediateData/data_3.7.1_clean.csv",
            codebook = "Data/IntermediateData/codebook_data_3.7.1_clean.Rmd", 
            value_labels = "Data/IntermediateData/value_labels_data_3.7.1_clean.yml", 
            synthetic = FALSE) 

closed_data(data = data_3.7.2_clean, filename = "Data/IntermediateData/data_3.7.2_clean.csv",
            codebook = "Data/IntermediateData/codebook_data_3.7.2_clean.Rmd", 
            value_labels = "Data/IntermediateData/value_labels_data_3.7.2_clean.yml", 
            synthetic = FALSE) 

closed_data(data = data_3.8.2_clean, filename = "Data/IntermediateData/data_3.8.2_clean.csv",
            codebook = "Data/IntermediateData/codebook_data_3.8.2_clean.Rmd", 
            value_labels = "Data/IntermediateData/value_labels_data_3.8.2_clean.yml", 
            synthetic = FALSE) 

closed_data(data = data_5.1.1_clean, filename = "Data/IntermediateData/data_5.1.1_clean.csv",
            codebook = "Data/IntermediateData/codebook_data_5.1.1_clean.Rmd", 
            value_labels = "Data/IntermediateData/value_labels_data_5.1.1_clean.yml", 
            synthetic = FALSE) 

closed_data(data = data_5.4_clean, filename = "Data/IntermediateData/data_5.4_clean.csv",
            codebook = "Data/IntermediateData/codebook_data_5.4_clean.Rmd", 
            value_labels = "Data/IntermediateData/value_labels_data_5.4_clean.yml", 
            synthetic = FALSE) 

closed_data(data = data_5.7_clean, filename = "Data/IntermediateData/data_5.7_clean.csv",
            codebook = "Data/IntermediateData/codebook_data_5.7_clean.Rmd", 
            value_labels = "Data/IntermediateData/value_labels_data_5.7_clean.yml", 
            synthetic = FALSE) 

closed_data(data = data_5.9.1_clean, filename = "Data/IntermediateData/data_5.9.1_clean.csv",
            codebook = "Data/IntermediateData/codebook_data_5.9.1_clean.Rmd", 
            value_labels = "Data/IntermediateData/value_labels_data_5.9.1_clean.yml", 
            synthetic = FALSE) 

```

### Preregistered Replication Protocols

The preregistered replication protocols refer to the publicly available protocols describing the background, methodology, and analysis of each replication set. These protocols were available through the OSF pages of each Many Labs project. These preregistered replication protocols, as well as the replication datasets mentioned earlier were scanned through to ensure that the planned analyses were feasible. However, no coding or analysis of either of them had taken place before the analyses were preregistered. This includes any systematic coding of the QMPs present in the report. Furthermore, no overview of these and the qualitative topics were available at the time of preregistration.

Finally, the reports of the Many Labs projects, meaning the published articles [@klein2014data; @klein2018many; @ebersole2016many; @ebersole2020many], were used to determine whether not the replication was considered successful.

### Original Study

The original study articles are the published articles of the original effect on which the Many Labs replication sets were based. In the case of Many Labs 5 the original studies refer to the studies which first tested the effect to be replicated, not the earlier replication attempts of the same effects in the RPP [@osc2015estimating].

```{r Cleaning Coded Data}
##### Data already available, code does not need to be run!
# Selecting the relevant rows and columns for the data
coded_data_initial_sel <- coded_data_initial_raw[3:160, 18:57]
coded_data_revised_sel <- coded_data_revised_raw[3:160, 18:38]
coded_data_vignette_raw <- coded_data_vignette_raw[3:160, 2]

# Combining the datasets
coded_data_full <- cbind(coded_data_initial_sel, 
                         cbind(coded_data_revised_sel, coded_data_vignette_raw))

# filtering out unnecessary double columns
coded_data_full <- cbind(coded_data_full[, 1:40], coded_data_full[, 45:62])


### creating the cleaned dataset using the functions in the source code 
coded_data_clean <- calculating(recoding(restructuring(fixing(
  renaming(coded_data_full)))))



### Saving to Intermediate Data Folder
# Splitting data into replication and original
coded_data_replications <- coded_data_clean[1:77,]
coded_data_original <- coded_data_clean[78:157,]

# difference in dataset row number is due to the fact that the moral foundations
# questionnaire in original 2.4 is reported using all 5 of its factors, whereas
# in replication 2.4 only the two overarching groups of binding and 
# individualizing foundations are described.
# For that reason a shortened original dataset will be used for any direct
# comparisons between original and replication coding.
coded_data_original_shortened <- coded_data_original[c(1:17, 19, 22:80),] 
coded_data_original_shortened[c(18,19),5] <- 
  c("individualizing moral foundations", "binding moral foundations")
coded_data_original_shortened[18,13] <- NA
coded_data_original_shortened[19,13] <- NA


# exporting cleaned data
open_data(data = coded_data_replications, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(coded_data_replications))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(coded_data_replications))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(coded_data_replications))), ".yml")) 

open_data(data = coded_data_original, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(coded_data_original))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(coded_data_original))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(coded_data_original))), ".yml")) 

open_data(data = coded_data_original_shortened, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(coded_data_original_shortened))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(coded_data_original_shortened))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(coded_data_original_shortened))), ".yml")) 


```


## Data Collection

The data on the preregistered replication protocols, and replication datasets from Many Labs 1, 2, 3, & 5 were all retrieved from their respective OSF pages: <https://osf.io/wx7ck/>, <https://osf.io/8cd4r/>, <https://osf.io/ct89g/>, & <https://osf.io/7a6rd/>. When available, codebooks and study materials were used to identify the specific data for each measure.

The publicly available datasets of the replication attempt were accessed through their OSF page and loaded into Rstudio [@Posit2023RStudio] with R version 4.2.2 [@R2021R]. Using the replication protocols, codebooks, and analysis scripts from each replication set as a guide, the variables associated with each measure were extracted into their own data frame containing the data from all studies in one replication set. These pieces of information formed the replication datasets in the analysis.

The OSF page for each Many Labs project was combed through, in order to identify the components of the research which contained information on the measurement practice and reported reliability. The focus was on finding the preregistered protocols which contained information on the entirety of the replication sets in each Many Labs project. This was successful for Many Labs 2, & 3. For Many Labs 1 the Proposal_V1.1 was selected as the proper candidate for the QMP analysis. Despite the difference in name, it is the equivalent in Many Labs 1 to the protocols of Many Labs 2 & 3. For Many Labs 5 the protocols were made accessible for each replication set separately. The relevant OSF page of each replication set was searched through for a file which contained protocol in its name. If available, a version of the protocol labelled as revised, post-review, peer-review, or endorsed was selected. In case there was doubt about which was most appropriate, the latest uploaded relevant protocol was selected. Only regular protocols were taken, meaning those not labelled as data collection, analysis protocol or anything similar[^1].

[^1]: The steps taken to access the replication protocol data from Many Labs 5 were not preregistered, and only became clear upon further investigation of the accessible information. The specific names and locations of the files that were used as the datasets can be found under the supplementary materials of the OSF page of this thesis <https://osf.io/gft46/%7D>

The original articles were identified through the citations in each replication protocol, using Web of Science, Google Scholar, and PsychInfo (in that order). An issue was encountered while trying to access @hyman1950current. However, after contacting the first author of @klein2014data a suitable version of the article was obtained for use in the analyses.

The measures of primary variables were identified by combing through the entire methods section of each preregistered replication protocols. Then using the structured coding protocol (made using Qualtrics) available on the OSF page for this thesis (<https://osf.io/9r8yt/>), each measure of a unique variable was coded on measurement reporting and reported reliability.

The structured coding protocol was used for both replication protocols and original studies. A series of questions relating to measurement practices and measurement error were used to extract the information. Descriptions of the extracted variables can be found in the Measures section below. The scope of the coding was limited to the abstract, method, procedure, materials, measure, and initial parts of the results sections. Additionally, a keyword search using the name of the measured variable was used to search for any potentially missed information. An exception to this procedure was a question asking about the presence of example items. For this question, information from supplementary materials was uniquely considered as relevant. Another exception was the question regarding the replication of the original effect. Tables from the final report of each Many Labs project were used to answer that question.

<!-- @ref(sec:measures) didn't work for section cross-reference in the paragraph above-->

After the data was initially coded according to the preregistered coding protocol, it was determined, based on discussion with the supervisor, that some of the initial ratings were possibly to stringent. As a result, some of the items in the protocol were reformulated, so that the conclusions from the resulting analyses were based on a more favourable judgement of QMPs. These changes included: accepting linked online appendices as a data source for example items; assuming that if no differences between replication and original are measurement is mentioned, that there is no difference, even if it is not explicitly clear; and generally simplified criteria for what constituted sufficiently clear reporting. The revision of the coding protocol happened after all the data were coded using the initial protocol. As a result the coded data using the initial protocol, and the coded data using the revised protocol are both available available on this thesis' OSF page: <https://osf.io/9r8yt/>. Both raw and processed formats are present.

```{r}

```

## Measures {#sec:measures}

### Measurement Error Measures

Two main sources of information were used to test hypothesis 1, the reported reliability coefficient from the replication protocols, and the reported reliability coefficient in the original articles. The reported reliability coefficient is the score reliability reported per measure in the replication protocol or original article. Both the type (Cronbach's alpha, retest, interrater, etc.) and the value itself were extracted.

The test of hypothesis 2 was based on different score reliabilities. As stated earlier, only score reliabilities of scalar measures with multiple items were included in the test of hypothesis 2. Because data was available for each lab location within a replication set, it was possible to calculate score reliabilities of a measure for each lab location. With location specific score reliabilities, it was possible to assess the variation of score reliabilities of the same measure across different contexts.

Two different types of score reliabilities were calculated: Cronbach's alpha and omega. Cronbach's alpha was used as it is one of the most popular indices of score reliability [@cho2015cronbach]. Omega was added to the analysis as it has been argued to be a more informative measure of score reliability, while also providing validity evidence for the scale [@crutzen2017scale; @deng2017testing]. Cronbach's alpha and omega were calculated using the alpha and omega functions respectively in the psych R package [@revelle2022psych]. Default arguments were used for both functions except the nfactors in the omega function, which was set to 1.

### Replicability Measure

Hypothesis 3 as well as 5 specify an association between either score reliability or QMPs respectively, and replication success. Replication success was based on the test of the main effect of a study. However, the unit of measurement for the data in this thesis was a measure of a unique variable, not an effect. Fortunately, since only primary measures were included in the data, each measured a variable of the main effect, and as a result the measure remains related to the test of the main effect. Replication was based on the significance of the meta-analytic effect of the replication set as reported in its respective Many Labs report. An effect was considered replicated if the meta-analytic p-value was lower than .05, and in the same direction as the original effect. In all cases, meta-analytic significance of an effect meant that support in the direction of the original effect was found.

The tests for hypotheses 4, 5, & 6 were all based on the number of QMPs per category that were determined by the research using the coding protocol. An overview of the QMP categories of the coding variables, based on @flake2020measurement, can be seen in Table INSERT TABLE 1 REF HERE

INSERT TABLE 1 HERE

Selection and creation share a category as the justifications and requirements in selecting a measure are similar to those for creating a new measure. Modification variables were only assessed when it was clear that the measure was modified from an original source in some way. The questions were all answered as either true, false, or not applicable. Questions related to modification had 'no modification' as an additional answer option, in case the specific aspect of the measure was explicitly not modified. Beside these quantitative variables, a total of four qualitative variables were also included to measure details surrounding measurement practices. These variables assessed: the specific version of the measure (if reported); whether and what previous materials were used for the study; what, if any, psychometric evidence beyond reliability score (e.g. factor analysis, IRT model, convergent validity) was provided; and whether it was specified or not that the measure was or was not modified.

Two of the qualitative items have an equivalent in the coding key of @flake2022construct. Eleven out of twenty of the quantitative coding variables have an equivalent in @flake2022construct. Variables related to the specific measurement scaling and purpose in the paper, were omitted from the analyses of this thesis as they were deemed neither relevant for the analyses of this study, nor necessary in matching this study to the key results of @flake2022construct. Questions about more general measurement features such as the number of items and how scores were aggregated were altered from @flake2022construct to better fit the aims of this thesis.

The tests for hypotheses 4, 5, & 6 were all based on QMP ratios, both total QMP ratios and ratios per QMP type. This ratio was based on the number of coded trues divided by the number of applicable QMPs. When a question was coded as true, this equated to a count of one QMP for that type and in total, whereas false equated to a zero. If coded as not applicable or no modification (for the modification items), this item was considered not applicable. Non applicable responses were not only counted as a zero, but additionally the denominator for calculating the ratio of QMPs was lowered by one. These ratios were preferred over counts, since non applicable items could thus be accounted for.

### QMP measures

The same data as for the test of hypothesis 2 was used to check for the unidimensionality of the factor structure, as part of the exploratory analyses. A check for unidimensionality as validity evidence only makes sense if the measure was intended to be unidimensional. Therefore, for each replication for which data was available, the protocol was checked to ensure that the scores on each measure were meant to assess a single construct.

### Validity Measure

The same data as for the test of hypothesis 2 was used to check for the unidimensionality of the factor structure, as part of the exploratory analyses. A check for unidimensionality as validity evidence only makes sense if the measure was intended to be unidimensional. Therefore, for each replication for which data was available, the protocol was checked to ensure that the scores on each measure were meant to assess a single construct.

```{r Check for Reliability and Validity Testability}
## Checking the measures to recalculate the reliability and factor analysis for.
## The question: "Are they psychometrics scales or not?", is key here.
View(coded_data_replications)

# Likely reliability: 1.10, 1.11, 1.12.3, 2.4.1, 2.4.2, 2.8.2, 2.10.1, 2.12.1, 
# 2.12.2, 2.12.3, 2.15, 2.19.2, 2.23, 3.7.1, 3.7.2, 3.8.1, 3.8.2, 5.7, 5.9.1
# Likely factor: 1.10, 1.11, 1.12.3, 2.4.1, 2.4.2, 2.8.2, 2.10.1, 2.12.1, 2.12.2,
# 2,12,3, 2.15, 2.19.2, 2.23, 3.7.1, 3.7.2, 3.8.1, 3.8.2, 5.7, 5.9.1
# Maybe reliability: 1.3, 1.12.1, 2.2, 2.3, 2.19.1, 2.20, 3.2.1, 3.5, 5.1.1, 
# 5.1.2, 5.4, 5.5.1, 5.5.2
# Maybe factor: 1.3, 1.12.1, 2.3, 2.19.1, 2.20, 3.2.1, 3.5, 5.1.1, 5.1.2, 5.4,
# 5.5.1, 5.5.2

# 1.9 is difficult, might be a scale as dv rather than voting behaviour.
```


## Analyses

### Analyses Measurement Error

Every hypothesis test in this thesis used an alpha significance cutoff of .05. No correction for multiple testing was applied, to avoid inflating type II error rate. The generally small sample size and consequently low power of these analyses meant that a smaller alpha would increase the number of false negatives. On top of that, these analyses were considered as exploratory. In this exploratory context, the rate of discovery of any potential true effects was given more value relative to decreasing any false positives compared to typical confirmatory research. That as a consequence obtaining false positives was more likely has been accepted, as false negatives were deemed more harmful to this study's aims.

The measurement error analyses testing hypotheses 1, 2, & 3 deviated from the preregistration for two main reasons: there were too few reported reliability coefficients, and too few multiple item scalar measures to calculate the reliability coefficients from. Because of the small sample size, the test for hypothesis 1 was changed, and the low power as a result of the sample size was accepted for hypothesis 2 & 3. This should be kept in mind while interpreting the results from these hypothesis tests.

Thus for testing hypothesis 1, instead of comparing the reported Cronbach's alpha coefficient between original and replication, the total number of reported reliability coefficients were compared instead. This total also included reliability coefficients other than Cronbach's alpha. This comparison was implemented via a chi-square test using the base R stats package [@R2021R].

The number of usable datasets to calculate reliability coefficients from was similarly limited in number. However, the number of datasets were still sufficient to be able to estimate the models of the planned analysis for hypothesis 2, albeit with a more exploratory goal than intended.

Using the lmer function from the lme4 package [@bates2015lme4], a multilevel null model with the Cronbach's alpha or omega coefficient of a replication as dependent was specified, where the random intercept was grouped by replication set. The formulas for this model, where $alpha_{ij}$ represents the calculated Cronbach's alpha coefficient of a single study at one lab location with $i = 1,..., n_{locations}$ and $j = 1,...,n_{replications}$ look as follows:

```{=tex}
\begin{align*}
& Level 1: alpha_{ij} = \beta_{0j} + e_{ij} \qquad\qquad\quad e_{ij} \sim N(0, \sigma_{e}^{2}) \\
& Level 2: \beta_{0j} = \gamma_{00} + u_{0j} \qquad\qquad\qquad u_{0j} \sim N(0, \sigma_{u0}^{2}) \\
& Mixed: alpha_{ij} = \gamma_{00} + u_{0j} + e_{ij}
\end{align*}
```
The purpose of this analysis was to investigate variability in calculated reliability coefficients within and between replication sets. However, to the author's knowledge no formal test exists as of the writing of this thesis for testing the significance of within group variance after controlling for between group variance. Instead the intraclass correlation (ICC) coefficient was calculated an indicator of within vs. between variability. The formula used to calculate the ICC is presented below.

$$ICC = \frac{u_{0j}}{(e_{ij} + u_{0j})}$$

A high ICC indicates low within group variance relative to between group variance, which would be in line with hypothesis 2.

In order to test hypothesis 3, the average calculated Cronbach's alpha level within a replication set is used as a predictor of replication success. Replication success was either set to 1 if it reached meta analytic significance at an alpha of .05 in the same direction as that found in the original study, or 0 otherwise. This test aligns with a logistic regression model. With $i = 1,..., n_{replications}$, $p_{replicated}$ being binomially distributed, and $alpha$ referring to the averaged calculated Cronbach's alpha coefficient, the following regression equation was used to model the relation:

$$f(replicated_{i}) = \beta_{0} + \beta_{1} alpha_{i}$$

Where:

$$f(.) = log\frac{p_{replicated}}{1 - p_{replicated}}$$

In this data, replication attempts at different lab locations can be seen as nested within a replication set. In order to see if this nesting might impact the analyses of hypothesis 3, multilevel logistic regression models were included as sensitivity checks. The dependent variable remains replication success at the replication set level, while Cronbach's alpha is no longer averaged across locations. The models used were a multilevel logistic random intercept and random slope model.

### Analyses QMPs

The reported analyses were primarily based on the revised set of QMP data. The results using the original QMP data are mentioned when the results led to different conclusions. The analysis code for both the original and revised QMP data are available in R script on the OSF page of this thesis: <https://osf.io/9r8yt/>.

Hypothesis 4, 5, & 6 were all tested using a model with the same dependent variable: total QMP ratio. In order to model a ratio dependent variable, beta regression was used. Beta regression models operate similarly to standard generalized linear models with similar interpretations of the results. What differentiates them is that they are suitable to model dependent variables with values in the interval of $(0, 1)$, meaning ratios. Furthermore, they are well suited for modelling heteroskedastic and asymmetrically distributed dependent variables [@cribari2010beta]. The beta regression model was implemented using the betareg function within the betareg package [@cribari2010beta]. Defaults were used for all function parameters besides the model formula and data. All beta regression models used the beta link function, which is as follows:

$$g(.) = (0, 1) \mapsto \mathbb{R}$$

All of these models assume that the QMPs are the same across each replication set. The reason being that only one protocol was analysed per replication set, and several of the questions regarding QMPs focus solely on the reporting of relevant measurement information. This assumption was deemed likely, since each replication within a set was based on the same protocol. Furthermore, the goal of these protocols is to ensure that the measurement practices are equal across the different replication studies of the same original effect.

To test hypothesis 4, the total QMP ratios between replication and original research was compared. The model formula for this test looked as follows:

$$g(QMP\_ratio_{i}) = \beta_{0} + \beta_{1} replication_{i}$$

Where $replication_{i}$ is a dummy coded variable, which was coded as 1 in case the reported QMP ratio belonged to a replications, and 0 if it belonged to an original study. For hypothesis 5 a similar formula was used:

$$g(QMP\_ratio_{i}) = \beta_{0} + \beta_{1} replicated_{i}$$

Where $replicated_{i}$ is a dummy coded variable, which was coded as 1 if the meta-analytic effect in the replication set matches the original effect in direction at a significance alpha level of .05, and 0 otherwise. Note here that unlike in the test for hypothesis 4, $QMP\_ratio_{i}$ refers only to the QMP ratios of the replications, meaning that original article QMPs were not included in this analysis.

The relationship between original article QMPs and replication protocol QMPs was modelled to test hypothesis 6. The outcome variable remains the same as in the model for hypothesis 5. The independent variable is changed to the QMP ratio for original articles. The resulting model formula is as follows:

$$g(QMP\_ratio\_replication_{i}) = \beta_{0} + \beta_{1} QMP\_ratio\_original_{i}$$

These models had independence of observations as another assumption, the observations here referring to a replication set. this is known to be false since the data is nested within four separate Many Labs projects. In order to test if this nested structure might influence the relations described in hypothesis 5 & 6, random intercept and random slope multilevel models were implemented as sensitivity analyses for the tests of both hypothesis 5 & 6. In these models each replication set was treated as being nested in its respective Many Labs project. Worth noting is that currently the betareg package does not allow for estimation of multilevel models. As an approximation, Gaussian random-intercept multilevel models were implemented using the lmer function from the lme4 package [@bates2015lme4]. The outcome QMP ratio variable was transformed using the logit link function (the default link function in the betareg package) to better fit the Gaussian model.

### Exploratory Analyses

Follow-up analyses were preregistered for hypothesis 6. Using a similar model to the main hypothesis test with all unique combination of the five different QMP types being tested across original and replication data. The goal of these analyses was to see if there are any type specific carry-over effects in QMPs between original and replication. As these analyses are exploratory in nature, due to the large number of tests, the focus was on visualisation rather than inference.

Finally, in order to present additional validity evidence alongside the analyses of Cronbach's alpha and omega coefficients, the unidimensionality of each measure was investigated. To test this, each multiple item scalar measure had their factor structure investigated on the entire measure data of the replication set. A single factor confirmatory factor analysis was implemented using the *fa* function from the *psych* package [@revelle2022psych] in R. The maximum likelihood factoring method was used, with defaults being used for all other function arguments. From this analysis, the RMSEA of the one factor solution was extracted. Additionally, a parallel test was conducted using the *fa.parallel* function also from the psych package. If either the RMSEA score \< .08, or the parallel analysis returned a one factor solution, unidimensionality was coded as true, otherwise it was coded as false.

# Results

## Descriptives

The mean sample size within the replications was 4844 participants (IQR = 3816). In the original articles the mean sample size was 244 (IQR = 140). In total, support was found for hypothesized effects of 34 of the 77 measures. Proportions of replication success per Many Labs project can be found in Table XXX.

INSERT REPLICATION RATIO TABLE HERE

Existing materials were used for 73 of the 77 measures in replications and for 28 in original studies. The specific version of the measure was reported for thirteen measures in replications, and nine measures in original articles. For those measures that were not based on existing materials, one in the replications and twelve in original articles clearly stated that they were newly created for the study. Only one measure in the original articles was modified from its original source during the data collection period, all other measures were modified before that, if at all.

## Reliability

Figure XXX depicts the type of measure (single-item, multiple item, or not a scale), if for the measure reliability was reported or not, and what type of reliability coefficient was reported. Reliabilities were only assessed for multiple item measures, which were approximately equal between replication and original (35 and 38, respectively). Looking at the multiple item measures, four in the replication protocols and twelve in the original articles reported a reliability coefficient. The most commonly reported reliability coefficient was Cronbach's alpha, which accounted for three out of four reported reliabilities in the replication protocols, and eleven out of twelve in the original articles.

INSERT RELIABILITY REPORTING FLOW DIAGRAM FIGURE

A chi-squared test compared whether the number of reliabilities reported among multiple item measures differed between replications and original. Reliabilities were found to be more commonly reported in original articles compared to replication protocols ($\chi^{2}(1) = 4.09, p = 0.043$). These results are contradictory to hypothesis 1.

The average Cronbach's alpha coefficient across replication sets was $0.618$ with a standard deviation of $0.554$, for omega the mean was $0.773$ with a standard deviation of $0.161$. Figure XXX shows the distributions of the Cronbach's alpha coefficients for each replication set separated by successful and unsuccessful replication.

INSERT PLOT 34

The between-group (across different replication set) variance and within-group (lab locations within a replication set) variance in the Cronbach's alpha coefficient were assessed using a multilevel model as specified in Formulas XXX, XXX, & XXX. The between-group and within-group variance for Cronbach's alpha were approximately $0.198$ and $0.044$ respectively. The resulting ICC, obtained using formula XXX, was approximately $0.819$. This suggests that variance was larger between replication sets than within. The same analysis for omega coefficients even showed a relatively larger degree of between group variance (between-group variance $= 0.034$, within-group variance $= 0.004$, ICC = $0.900$). Both results are in line with hypothesis 2, although caution in interpreting this result is advised due to the small sample size and the lack of an available inferential test.

The relationship between reliability and replication success was primarily tested via logistic regression, with replication success as the dependent variable. Further sensitivity analyses were performed using multilevel logistic models, with each measure as an overarching group wherein its reliability at each lab location is nested. The results of the logistic and multilevel logistic models are shown in Table XXX for both Cronbach's alpha and omega reliability coefficients.

INSERT REPLICABILITY AND RELIAIBLITY TESTS TABLE HERE

Cronbach's alpha and omega did not significantly relate to replication success in the main logistic regression model (Cronbach's alpha: $B = -1.621, SD = 1.527, p = .288$; omega: $B = 0.495, SD = 3.776, p = .896$), nor in any of the multilevel sensitivity analyses models (see Table XXX). This is contradictory to the expectation in hypothesis 3. However, the results from these analyses should be evaluated with caution, given the small sample number of measures used to estimate these models.

## Questionable Measurement Practices

The counts of the QMP ratio obtained for each QMP type (see Table XXX) are shown across the top layer of Figure XXX for both replication protocols and original articles. The overall number of counts for the QMP ratios of the QMP modification type were lower compared to the others, because only 52 out of 77 measures in replications, and only ten measures used in original studies were modified. The bottom layer of Figure XXX shows the overlayed distributions of the total QMP ratios from the replication protocols and original studies, with dots indicating each datapoint, and the line indicating the mean for either distribution.

INSERT HERE 256

Beta-regression was used to test the difference in QMP ratio between original articles and replication protocols. Results showed that original articles contained a significantly larger proportion of QMPs than replication protocols for the measures of the same effects ($B = -0.779, SD = 0.140, p < .001$). This result is in line with hypothesis 4.

A similar beta-regression tested the association between replication success and QMP ratio in replication protocols. Results showed that successful replication significantly related to a decrease in the ratio of QMPs in replication protocols $\left(B = -0.588, SD = 0.212, p < .001 \right)$. Sensitivity analyses were performed using a random-intercept multilevel regression function. This model also found a significant association between replication success and QMP ratio $\left(B = -0.963, t(70.319) = -1.491, p = .011 \right)$. Both of these results are in line with hypothesis 5. However, the relationship in the multilevel model is no longer significant for QMP ratios obtained with the initial coding protocol $\left(B = -0.517, t(63.122) = -2.635, p = .140 \right)$.

The relation between QMP ratios in the replication protocols and corresponding original articles were investigated using a beta regression model. It showed that total QMP ratio in the original article can significantly predict total QMP ratio in the subsequent replication ($B = 1.477, SD = 0.579, p < .001$), which provides some evidence in favor of hypothesis 6. However, this relation was not significant for the QMP ratios obtained with the initial protocol ($B = -0.387, SD = 0.511, p = 0.449$). Sensitivity analyses using a random intercept logistic multilevel model also found similar discrepancy in results between revised and initial protocol (revised protocol: $B = 0.249, t(74.473) = 1.498, p < 0.001$; initial protocol $B = -0.057, t(74.620) = -0.332, p = 0.741$).

INSERT FIGURE 26 HERE

## Exploratory Results

Planned exploratory analyses were preformed to expand on the test for hypothesis 6. The relationships between original article QMP ratios and replication protocol QMP ratios were investigated for each QMP type separately. Table XXX shows the results from these tests with the revised protocol QMP coding, while Table XXX shows the results for the QMP ratios obtained via the initial protocol. There were few significant relations and even fewer that are consistent between both coding protocols. The relations that are consistent are those between original article definition QMPs with replication selection and total QMPs, original article selection QMPs with replication total QMPs, and finally original article quantification QMPs with replication selection QMPs. All of these relations were positive across both coding protocols, illustrating that a greater QMP ratio of the specified type in an original article is associated with an increase in QMP ratio for the specific QMP type, or total QMP ratio in the replication protocol.

INSERT QMP REPLICATION ASSOCIATION TESTS TABLE

INSERT QMP ORIGINAL REPLICATION ASSOCIATION TESTS TABLE

Figure XXX visually illustrates the relations of QMP ratios from different QMP types between replication protocols and original articles further.

The unidimensionality of the measures used in the Many Labs projects was assessed through factor analyses, and evaluated based on the criteria discussed in section XXX. Suitable data for factor analyses was available for nineteen measures. These measures were the same as those shown in Figure XXX, with the addition of one more measure from [@cacioppo1983effects]. A composite score based on the measurement responses was used for sixteen of the nineteen measures to form a single variable in the analyses, and were thus regarded as intended to be unidimensional. The number of dimensions for the remaining three were unclear. Sufficient fit of the unidimensional model was found for six out of the nineteen evaluated measures. However, as shown in Table XXX the unidimensional model did not hold for any measure data based on the parallel analysis test alone. Only the RMSEA $< .08$ criteria suggested evidence for single factor structures.

INSERT FACTOR ANALYSES RESULTS TABLE HERE

# Discussion

In the current study, measurement error and QMPs were assessed in both replications and original research. These measurement related concepts were then linked to replicability in order to investigate them as potential causes inhibiting successful replications. The results of the tests for the hypotheses showed mixed results. However, when including exploratory and descriptive analyses, a clearer picture of the state of measurement in replication and original research emerges.

Before interpreting of the results, it should be reiterated that the sample size for several analyses was smaller than what is generally recommended for such analyses. This holds especially for the tests of hypotheses 2, & 3, since the number of measures for which reliability could be calculated for these analyses was only eighteen. This was less than anticipated, which brings us to the first topic of the discussion.

## Quality of Measurement

This thesis assessed the quality of measurement and measurement reporting in original and replication research by investigating the quality of measurement in the Many Labs projects. Findings were in line with similar investigations in the literature by @shaw2020measurement & @flake2022construct. These two studies found that sufficient validity evidence and reliability reporting for measures was lacking, even more so for replications than for original studies. The lack of support found for hypothesis 1 shows that this also held for this thesis' sample. In fact, support was found for an effect in the direction opposite to what was hypothesized in the preregistration, as the original articles reported reliability coefficients more often than replication studies.

However, even the original articles only reported reliability for approximately one third of the used multiple item measures. Reliability coefficients were reported in such small numbers that the original analysis plan for hypothesis 1 was altered, from evaluating the reliability coefficient value to the number of times reliability was reported in an article or protocol. As a result, the interpretation of rejecting the hypothesis based on this new test changed compared to the preregistered test. Still, the test provides evidence towards the same point, which is that reliability evidence for the used measure is lacking more in replication protocols than in original articles, although most multiple item measures in original articles were also reported without reliability evidence. This shows that lack of reliability evidence is a widespread issue, something that has been shown in earlier literature on this topic [@vacha1999practices; @green2011recent].

Furthermore, across measures used in both original and replication studies, less than half of the measures were multiple item scales. Research commonly made use of single item measures (see Figure XXX for more details), an observation that was also made by @shaw2020measurement. The use of these single item measures comes with risks, since by virtue of being singular they assume that whatever they are measuring is unidimensional, while not being able to test this assumption [@shaw2020measurement]. This is potentially quite worrying given that the exploratory analyses of the multiple item measures, for which unidimensionality could be tested, showed only little support for unidimensionality of the measures used in replications. While the measures that were tested for unidimensionality were mostly used as single indicators in the analyses for the replication protocol, one could argue this does not necessarily mean these measures all assessed constructs with a unidimensional structure. For example, IQ is often used as a singular indicator, even though it consists of several factors [@weiss2013wisc]. However, it should be noted that at least with respect to Many Labs 2, all of the measures that were considered unidimensional for the analyses in this thesis were also considered unidimensional in the investigation by @shaw2020measurement} who also observed a lack of unidimensionality evidence in their investigation of Many Labs 2. To conclude, there is an indication that validity evidence for measures used in replication research is lacking overall.

Several results did show more promising findings for the way measurement related issues are treated in replications. Firstly, the support found for hypothesis 2 showed some evidence that most of the variability in reliability coefficients is between measures, rather than within measures across different locations. If variation in the reliability of a measurement across locations is little, it could indicate that the measurement operated similarly across these locations, which would make the scores from the different lab locations directly comparable and aggregatable. However, again the small sample number of measures for which reliability could be assessed does mean that this estimate remains uncertain. Secondly, most replication protocols clearly stated that their measure was either newly created or based on existing materials. Still, in most instances neither original nor replication provided information on the exact version of their measure. This may largely be due to the fact that many of the used measures were single-item, for which specifying the exact versions may not be seen as crucial because they more or less describe themselves. Finally, QMPs were overall less common in replication protocols than they were in original articles, supporting hypothesis 4. This runs counter to the findings of @shaw2020measurement & @flake2022construct, who found that measures came with less validity evidence in replications than in original studies, as the result of QMPs.

The structured replication protocols that were used in the Many Labs projects, I suspect improved the reporting practices and measurement consistency in replication protocols compared to original articles. Most replication protocols even contained a section under each replicated study, which listed any deviations from the original and as a result many potential QMPs were averted. [@protzko2020high] already demonstrated that transparent reporting of methodology before conducting research can improve the robustness of psychological science, and I believe the same holds true here.

## Impact on replicability

Besides assessing the overall quality of measurement reporting, this thesis also investigated if the reliability of a measure, and QMPs can impact replicability. First of all, contrary to expectations (see Hypothesis 3), less reliable measures were not associated with a decrease in replicability. This result is surprising, since @stanley2014expectations demonstrated the potential impact that measurement error can have on the veracity of determining replication success based on a significance test without attenuating for measurement error. The significance test cut-off criteria that was used in this thesis to assess replication success similarly did not attenuate for measurement error, and thus results in line with the simulation studies of @stanley2014expectations were expected.

While no evidence for hypothesis 3 was found, caution in interpreting the test results is needed. The number of measures for which reliability was calculated was quite low (eighteen for Cronbach's alpha, fifteen for omega), it could be that the test was simply underpowered to detect the effect. On the other hand, it might be that the detrimental influence of measurement error on interpretations of replication results is small in large scale replication projects. Across the multiple replications, measurement error's influence on the effect could have been cancelled out at the meta-analytic level. This would require the measurement error to be random and small, and as a result the average value of the effect and the error around would remain similar, thus not altering the result of the meta-analytic significance test.

Results regarding the relationship between QMPs and replicability were more in line with expectations. In particular, support was found for hypothesis 5, showing that an increase in total QMP ratio was associated with a decrease in replicability. Existing literature has already warned about the potential detrimental effects of QMPs on replications [@shaw2020measurement; @flake2022construct]. This result provides further evidence supporting these claims. However, the association was only significant when data was used from the revised coding protocol, data from the initial protocol, which was more stringent in its assessment of QMP, did not indicate a significant association with replicability.

Similarly, the test for hypothesis 6 showed inconsistent results between the revised and initial coding protocols. Only for the revised protocol data was an increase in total QMPs in the original study associated with an increase in total QMPs in the protocol of the replication for that study. The result obtained with the data from the revised protocol are in line with expectations. If information on measurement is lacking or the measure is poorly motivated in a study, then any subsequent replication attempt will lack information needed match its measurement to the original. Even if it does match the original, when the measure itself is a poor indicator of the assessed variable or construct, both the original and replication data obtained from this measure may not substantially relate to the investigated psychological phenomenon the original study set out to investigate [@flake2022construct]. However, the fact that the conclusions were different based on the protocol makes it difficult to determine the exact nature of the influence of original study QMPs on replication protocol QMPs.

Further analyses based on the QMP ratios of each specific type indicated that across both the initial and revised coding protocols together, only selection, definition, and quantification QMPs in the original article related significantly to selection type QMPs, total QMPs, or both in the replication (see Tables XXX & XXX).

It could be that original articles have to provide a clear definition as well as details on the scale, in order for a replication to be able to justify selecting the measure. A replication may also be in general more dependent on the original article for information on the definition of the constructs and the choice of measurement, compared to a description of the Operationalisation, quantification, and any modifications. These former are features of a measurement, which while preferably similar in a replication to the original, can and should be reported in the replication protocol even if the original article does not contain the details on these features itself. However, these are just speculations as the results showed no clear relations between different QMP types across original and replication (see Figure XXX), and testing of causal links was not possible with this data.

For both hypothesis 5 & 6, the results were in line with expectations, but only for the revised coding protocol. Neither hypothesis was supported by the data on QMPs obtained with the initial coding protocol. The initial protocol was revised because the ratings based on that protocol were consider to stringent, and as a result, several items were reformulated so that criteria for what constituted sufficiently clear reporting were lowered. This meant that in the revised protocol, coded QMPs would need to clearly inhibit evaluation of the measurement, whereas for the initial protocol some aspects of the measurement reporting which were coded as questionable, may have been questionable only to a small degree. As a consequence, QMPs coded with the revised protocol represent measurement practices that represent on average more questionable practices than QMPs coded with the initial protocol. With this in mind, the difference between total QMP ratio of initial and revised protocol can be seen as a form of effect size difference. It could be that only replication QMPs that represented robust violations of good measurement practices significantly lower the probability of successful replication. In that case, a stronger relation between replicability and QMPs would be found with the revised protocol, since these QMPs represented more clearly questionable practices in measurement reporting than QMPs coded with the initial protocol, which would explain the disparate results that were found.

## Limitations & Future Research

The design of this thesis came with certain limitations. First of all, there was no experimental manipulation included. As a result, the ability to investigate the exact causal links between QMPs and replicability was limited. This thesis was intended to be exploratory in nature, and thus did not focus on causal analyses. However, future research establishing causal links will prove to be very beneficial in improving the understanding of the relationships between measurement issues and replicability.

Another limitation of this study is that the results differed based on whether data from the revised or the initial coding protocol was used. As a result, any relationship between QMPs and replicability remains uncertain. The revised coding protocol was adapted from the initial coding protocol before any analyses were performed, it is however still possible that a change in protocol may have biased results. Future research investigating this topic should use a coding protocol which is verified independently by several researchers on a sample of articles and protocols first, before being used to code the QMP data that will be analysed. As a first step, the coding protocol from this thesis and the coding key from @flake2022construct can be used as a basis.

It is important to also consider whether or not replication protocols and research articles can be fairly compared in terms of QMPs. Articles are generally longer than a study's description within a replication protocol, and as a result it seems reasonable to expect research articles to generally contain more measurement details than replication protocols. Regardless, this thesis assumed that a replication protocol is comparable to an article in providing an overview of the measure and how it was used, as it is the only comparable source for such information that would otherwise be difficult to find anywhere else. This assumption seems practical if not tenable when the replication protocol is the only available sources of measurement details. However, in case more detailed information on a measurement is available in other sources, such as an online appendix, it could be important data to include when assessing QMPs. This research did not make use of any online appendices in assessing QMPs, future research may wish to include them and other sources as part of the data.

As a last note on QMPs, it is worth highlighting the way the QMP ratios were calculated. The total QMP ratio was based on the counts for each of the items from all the different QMP types divided by the total number of applicable questions. However, for each QMP type the number of items was different. This means that QMP types with a larger number of items (such as operationalisation) had a greater impact on the total QMP ratio than QMP types with a smaller number of items (such as definition). If the impact for each QMP type on each other and replicability is considered equally important, the total QMP ratio may not be considered a valid indice. On the other hand, it could be that each item should be considered equally important rather than each QMP type. For this study, the protocol was made and the analyses were conducted under the assumption that each item had equal weight. However, this is an assumption this article cannot provide evidence for.

The small sample of measures for which reliability coefficients and unidimensional factor structure could be assessed limited the degree to which substantial interpretations can be made from the reliability and validity analyses. While future research will also unlikely be able to increase the number of multiple item measures used by replications, future research can broaden their analyses to include a larger number of replications, or take into account a type of measure which this thesis mostly ignored in the analyses of reliability and validity: single-item measures.

Reliability assessment for single item measures differs substantially from multiple item measures [@wanous1996estimating], and for this reason was not included as part of the analyses for this thesis. Furthermore, single items cannot be tested on unidimensionality, since a single item is already assumed to measure only a single dimension by definition. However, future research might consider methods specifically designed to assess reliability and validity of single item measures. This could prove to be a worthwhile contribution, given the proportionally large number of single item measures that were encountered in the data for this thesis (see Figure XXX).

Finally, this paper investigated the consistency of measures, by assessing the variance in reliability across different labs wherein the measure was conducted, and comparing it to the variance in reliability between the different measures. Besides comparing the degree of reliability across labs, it is also possible to assess whether a measure assesses the same underlying construct across lab locations. This can be done through measurement invariance testing. Measurement invariance is a concept in latent factor modelling that illustrates the extent to which the relationship between items and factors is dependent on some other variable, usually a group [@mellenbergh1989item]. @shaw2020measurement investigated measurement invariance for the responses to measures at each lab location within the replication sets of Many Labs 2. Investigations similar to this have also been suggested for future replication projects [@flake2022construct]. For these reasons, future research should consider measurement invariance tests, if assessing measurement consistency in replications across different labs.

## Implications for Research and Replications in Psychological Science

Taking all the findings together, the following assessments will be relevant to consider for future replication research. With respect to the quality of measurement reporting, the small number of reported reliabilities, small number of reported psychometric validity evidence, and lack of unidimensionality evidence for data aggregated to a single indicator corroborate findings by @shaw2020measurement & @flake2022construct. Evidence supporting the measures used in replication should be reported more frequently, so that they can be evaluated and the validity of replication research can be improved.

There are additional validity issues in measurement as the result of QMPs. While it is debatable whether or not $17.9\%$ average QMP ratio in replications is too much, the association between original paper and replication protocol QMPs demonstrates how a lack of information for measurement in the original can spill over to the reporting in replications. This could lead to issues for replication attempts if the measure in an original article has poor reliability and validity. In that case, the tests in the original article are based on data from measures that do not accurately capture the construct they were intended to measure. The results of any direct replication attempt will face the same issues, and as a consequence the replication is unlikely to provide a relevant test of a psychological theory, as the original study itself was not a robust test of that psychological theory to begin with [@isager2020deciding, @isager2021deciding, @nuijten2022assessin]. Thus, future replicators should first evaluate the quality of the measurement (reporting) in an article before deciding to replicate it.

In case the quality appears low, replication attempts can still proceed with some adjustments. One option is to change the focus of the replication to be conceptual rather than direct. In that case the measure and the operationalisation can be freely altered from the original, which means that a more sufficient test of the theory can be developed. If deemed necessary, a direct replication can always be performed later based on the conceptual replication. In that case, the direct replication will be able to provide a sufficient test of a psychological theory, on the condition that the conceptual replication's test improvements led to it becoming a sufficient test.

The results also suggest that the use of structured protocols in the reporting of measurement may be an effective way of reducing QMPs, and increasing the reporting of reliability and validity evidence. However, even in the Many Labs projects, which made use of structured protocols to report their measures, low reliability and validity evidence reporting was found. These protocols could therefore still increase the attention they give to reporting reliability and validity evidence. In particular, Figure XXX shows descriptive evidence that replication protocols are lacking in definition and selection evidence. Furthermore, since replications must base their measures on those of the original articles they are replicating, it is important that articles chosen for replications provide clear evidence on what is being measured, how it is scored, why it was chosen or why it was constructed the way it is, and how it is used in practice. It is these last two aspects that original articles appear to particularly be underreporting (See Figure XXX). When original articles do not report their measurement details, replication attempts cannot guarantee a direct replication of the original. By improving the quality of measurement reporting in original articles, the validity of both original research results and future replication attempts would increase. It would be particularly important to focus on remedying strong violations of good measurement reporting, as these were shown to have a significant detrimental impact on replicability. This suggests that even small but critical improvements in the reporting of measurement information could lead to a substantial increase in the replicability of psychological science.

# Conclusion

Overall, the observed quality of measurement and frequency of measurement reporting in psychological research and replications was low, which is in line with other research on the topic. Improvements in the reporting on measurement are needed for both original and replication research. The measure source, content, and how it was used should be described in detail, and evidence illustrating the reliability and validity of the measure should be provided. Not only would the clarity of research improve, higher quality of measurement reporting in replications was shown to be related to an increase in replicability of the findings related to that measure. Tentative evidence was also found for spill-over effects of the quality of measurement reporting from original study to replications. These results combined suggest that improvements in the reporting of measurements are necessary in both replications and original articles. Furthermore, future replicators should carefully consider the quality of measurement in a study before attempting a direct replication, even considering a conceptual replication instead if the measurement in the original was not of sufficient quality. However, the evidence for the relationship between measurement quality and replicability was not robustly identified, and future research will be needed to confirm the exact nature of this relationship.

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
