---
title             : "Measurement Reliabily, Validity, and Reporting in Replicaitons and Original Research"
shorttitle        : "Measurement Reliability, Validity, and Reporting"

author: 
  - name          : Cas Goos
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Professor Cobbenhagenlaan 125, 5037 DB, Tilburg, The Netherlands"
    email         : "c.goos@tilburguniversity.edu"

  - name          : Marjan Bakker
    affiliation   : "1"

  - name          : Jelte M. Wicherts
    affiliation   : "1"

  - name          : Michèle B. Nuijten
    affiliation   : "1"


authornote: |
  The authors made the following contributions. CG: Conceptualization, Data curation, Formal Analysis, Investigation, Methodology, Project Administration, Software, Visualization, Writing - Original Draft Preparation, Writing - Review & Editing; MB: Conceptualization, Supervision, Writing - Review & Editing; JW: Conceptualization, Supervision, Writing - Review & Editing; MN: Conceptualization, Project Administration, Supervision, Validation, Writing - Review & Editing.


abstract: |
  For a replication to be informative, measurement should be reliable and valid in both original and replication studies. 

keywords          : "reliability, construct validity, measurement, reporting, questionable measurement practices, replications, informative replications, psychology"
wordcount         : "1"

bibliography      : ["r-references.bib", "references.bib"]

floatsintext      : yes
linenumbers       : no
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

header-includes:
  - | 
    \makeatletter
    \renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-1em}%
      {\normalfont\normalsize\bfseries\typesectitle}}
    
    \renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-\z@\relax}%
      {\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
    \makeatother
  - \renewcommand\author[1]{}
  - \renewcommand\affiliation[1]{}
  - \authorsnames[1, 1, 1, 1]{Cas Goos, Marjan Bakker, Jelte M. Wicherts, Michèle B. Nuijten}
  - \authorsaffiliations{{Department of Methodology and Statistics, Tilburg School of Social and Behavioral Sciences, Tilburg University, Tilburg, NL.}}

csl               : "`r system.file('rmd', 'apa7.csl', package = 'papaja')`"
documentclass     : "apa7"

classoption       : man
output            : papaja::apa6_docx
knit              : worcs::cite_all
---

```{r setup, include = FALSE}
# loading R libraries
library(papaja)
library(worcs)
library(lavaan)
library(psych)
library(metafor)
library(forcats)
library(ggplot2)
library(grid)
library(patchwork)

# Code below loads all data. The raw data was loaded within the 'prepare_data.R script.
load_data()

# the code below removes the raw data to preserve disk space.
# If you want to rerun the data cleaning and preparations steps, DO NOT run
# this code.
# If you want to only rerun the analyses, YOU CAN run the code below.
rm(coded_data_initial_raw, coded_data_revised_raw, coded_data_vignette_raw,
   data_2.10.1, data_2.12, data_2.15, data_2.19.1, data_2.2, data_2.20, 
   data_2.23, data_2.3, data_2.4.1, data_2.4.2, data_2.8.2, data_3.5, data_5.1,
   data_5.4, data_5.5, data_5.7, data_5.9.1, data_ml1, data_ml3)

# creates a reference list for all used R packages and the installed R version 
# (does not include Rstudio)
r_refs("r-references.bib")
```

<!-- altering latex defaults to get better figure and table placement -->

\renewcommand{\arraystretch}{0.7}

<!-- reducing the line spacing within tables -->

\renewcommand{\topfraction}{.8}

<!-- max fraction of page for floats at top -->

\renewcommand{\bottomfraction}{.8}

<!-- max fraction of page for floats at bottom -->

\renewcommand{\textfraction}{.15}

<!-- min fraction of page for text -->

\renewcommand{\floatpagefraction}{.8}

<!-- min fraction of page that should have floats .66 -->

\setcounter{topnumber}{3} <!-- max number of floats at top of page -->

\setcounter{bottomnumber}{3} <!-- max number of floats at bottom of page -->

\setcounter{totalnumber}{4} <!-- max number of floats on a page -->

<!-- remember to use [htp] or [htpb] for placement -->

```{r analysis-preferences}
# Seed for random number generation
set.seed(26052025)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

```{r helper_functions}
# Function for storing data openly in the intermediate data folder
saving_to_intermediate_data <- function(data){
  open_data(data = data, filename = paste0(paste0(
    "Data/IntermediateData/", deparse(substitute(data))), ".csv"),
    codebook = NULL, value_labels = NULL) 
}

# Function for storing data openly in the analysis data folder
saving_to_analysis_data <- function(data){
  open_data(data = data, filename = paste0(paste0(
    "Data/AnalysisData/", deparse(substitute(data))), ".csv"),
    codebook = NULL, value_labels = NULL) 
}
```

<!-- Introduction -->

The challenge with measurement in quantitative psychological research is that unlike height or weight, psychological constructs cannot be directly measured, as they cannot be directly observed. Instead, we use scores from a measurement as an indicator for a psychological construct. Yet, it cannot simply be assumed that any measure will form a good indicator of the constructs of interest. The measure has to be validated by establishing the reliability and construct validity of the measure. "Reliability refers to the consistency of scale scores when a construct is assessed multiple times or in multiple ways" [@crutzenScaleQualityAlpha2017b; pp. 244]. Meanwhile, construct validity refers to whether the variance in the measured item scores between participants can be attributed to the variance in the unobserved construct(s) of interest between participants [@cronbachConstructValidityPsychological1955; @vazireCredibilityReplicabilityImproving2022].

However, research indicates that measures often lack reliability and construct validity evidence !!!CITE!!!, either because the evidence is not reported, or because the measures are not validated. @flakeMeasurementSchmeasurementQuestionable2020 dubbed these practices - that raise doubts about a measurement’s validity - Questionable Measurement Practices (QMPs), being a term analogous to Questionable Research Practices [QRPs; Simmons et al. (2011); John et al. (2012); Wicherts et al. (2016)]. Existing research has already shown practices that reflect QMPs to be relatively common in both replication and original studies !!!CITE!!!. The purpose of this study is to provide a combined overview of the information on the reporting practices of measurement in replication and original with reliability and construct validity indices obtained from the shared item response data. By combining information on reporting with our own calculated indices, we can reflect on which validity issues lie in reporting and which are an issue with the measurement itself.

## Measurement Reporting

One issue that has been observed with respect to measurement validation is that the reported measurement information is typically insufficient for readers to understand, evaluate, and reuse the measurement that was used [!!!CITE]. @flakeConstructValidityValidity2022 documented the measurement reporting practices among 100 replications and their respective original articles from the Reproducibility Project Psychology [@opensciencecollaborationEstimatingReproducibilityPsychological2015]. They coded the number of measures, the content of the measures, and information describing and justifying the measurement. They observed limited reporting of evidence demonstrating the reliability and validity of the measurement. Additionally, only eight of the 40 translated scales contained validity evidence for the translated version. A similar lack of reliability and validity reporting has been observed in other studies as well [@hoganEmpiricalStudyReporting2004; @flakeConstructValidationSocial2017]. If readers cannot glance the construct validity and reliability of a measure from the reported information, then readers lack the basic information needed to determine whether the measured variables are actually related to the constructs of interest. In turn, if we cannot determine whether the constructs of interest are captured in the measured variables or not, then the relation of the results of the statistical analysis using these variables and any psychological phenomena of interest is also strenuous. In short, if the reported information does not show that the measurement was valid, then any substantive conclusions from the data are unfortunately left unsubstantiated.

Further findings by @flakeConstructValidityValidity2022 and others [@flakeConstructValidationSocial2017; @maireadshawMeasurementPracticesLargescale2020] show that measurement reporting was not only limited in terms of the reliability and validity evidence needed to evaluate the measure. The lack of information was also observed for basic content descriptions such as the number of items, the response format, and the scoring of the scale were irregular. This creates challenges for future researchers that want to reuse to measurement. Specifically for replication studies attempting to reconstruct the measurement from these incomplete descriptions, they may end up with a measurement that assesses the constructs in a substantially different way from the original study, or even assess different constructs altogether. If different constructs are assessed, the replication cannot be seen as a test of the same phenomenon as the original. If the same constructs are assessed, but in a substantially different way, the estimated effects in the replication cannot easily be put side-by-side with the effects in the original study. In either case, substantial comparisons between original and replication are hindered. The inability to reuse a measure does not only affect replications however, it also limits our ability to validate the measure thoroughly. Even if the measure is shown to be construct valid in one context, the factor relations may not be stable for this measure in other contexts and implementations, a concept known as measurement invariance [@hornPracticalTheoreticalGuide1992; @cheungDirectComparisonApproach2012]. While it is important to assess measurement (in)variance before using measures in a new context, if that measure is not reusable, the invariance cannot even be assessed.

Our first aim is to extend these existing findings on measurement reporting practices with a descriptive account of these practices for the Many Labs replications and the related original studies. We evaluate to what extent the reporting and use of item-based measurement scales in our sample allows us to evaluate and reuse the measurement. We do not ourselves evaluate whether the reported measurement information provide sufficient justification for its use, merely that information is reported such that that part of the measurement can be evaluated. We extend earlier investigations through new questions about the presence of justification for measurement selection or creation, item transformations such as reverse coding being described, the administration format being described, a change in the number of items from the source, whether changes in items response options, or number of items, or administration were validated or justified. All of our measurement reporting items were inspired by and categorized based on the QMP table in @flakeMeasurementSchmeasurementQuestionable2020.

We chose the Many Labs replications and associated original studies as our sample, due to them being of particular interest. The Many Labs replications were a series of large-scale collaborations in which multiple labs across the world directly replicated classic and contemporary psychological studies [@kleinInvestigatingVariationReplicability2014a; @ebersoleManyLabs32016; @kleinManyLabs22018b; @ebersoleManyLabs52020d; @kleinManyLabs42022]. The original studies chosen for replication in the Many Labs were not only picked based on ease and feasibility, but importantly for us they were also picked to display a diverse range of effects that were also of key interest to the field. This makes these studies an important sample to evaluate, because the replicated effects represent a broad range of relevant findings. Furthermore, because the Many Labs projects used preregistered and documented structured protocols, we expect the measurement reporting for the replications to represent a high standard within the field. Any issues in measurement reporting here might suggest that other replications could face similar or greater challenges.

## Reliability

Psychometrics offers various ways in which to assess construct validity and reliability [@nunnallyOverviewPsychologicalMeasurement1978; @mellenberghConceptualIntroductionPsychometrics2011]. It is often the results from these psychometric analyses that form one of the pre-requisite pieces of evidence for assessing whether or not the measure is valid or not.

When psychometric indicators are reported - which as mentioned earlier is not common - it is often limited to Cronbach's Alpha, a commonly used indicator for reliability. Cronbach's Alpha comes with strong assumptions that are themselves rarely tested [@crutzenScaleQualityAlpha2017b]. Furthermore, research by @husseyAberrantAbundanceCronbachs2023 has shown evidence that the reporting of reliability may not only be little, but also biased. They observed a disproportional amount of Cronbach's Alpha values at the common acceptably reliable threshold value of .70, and relatively low reporting below .70. Thus, the lack of reported measurement evidence may on occasion be hiding psychometric skeletons in the closet.

As a result, if we want to evaluate the reliability of measurement in our sample, we cannot rely on the reported information alone to give us a representative picture. Thus, we evaluate the measurement reliability within the Many Labs original and replication studies by estimating it from the item response data. The data on the item responses are openly available per lab for our sample. As a result, we can evaluate not only the reliability per measure, but also the variation in reliability across labs. The variation is relevant because Cronbach's alpha is an indicator of reliability within a particular sample and not of the reliability of the measure in general, as it is proportional to the total variance in the target variable in the sample. Despite this many researchers report and interpret Cronbach's Alpha as a universal quality of the measure [@cortinaWhatCoefficientAlpha1993; @schmittUsesAbusesCoefficient1996]. @maireadshawMeasurementPracticesLargescale2020 has so far observed that for Many Labs 2 the overall sample Cronbach's alpha level across scales was below .5, a degree of reliability far below what many would consider acceptable for most research purposes. It is our second aim to empirically evaluate both the degree as well as the variation of reliability of the measures in our sample that includes the other Many Labs projects.

## Unidimensionality

Reliability is a pre-requisite to construct validity, because a measure for which a participant’s response is not consistent, cannot capture a stable construct [Kaplan and Saccuzzo (2013, pp. 154–155)]. Another, important pre-requisite for many measures is unidimensionality. Unidimensionality means that the items on a measure assess, show evidence of measuring a single construct. Thus, if a set of items are intended to measure the same one construct, then unidimensionality is key to the construct validity of that set of items. Additionally, most reliability indicators - including Cronbach's Alpha - cannot be assessed without first making sure that the measure is unidimensional. Thus for our third aim we again used the item response data. This time hitting two birds with one stone. We check the measures' construct validity, and we check if one of the key assumptions of Cronbach's Alpha is met.

!!!substantively belangrijk, want de scores worden als unidimensioneel geinterpreteerd. Alpha assumptie testen, als kleiner deel bespreken; Reliability kan in sommige gevallen slecht zijn als de schaal gewoon multi-dimensioneel is. Eerst een beetje techniek, daarna meer kwalitatief oordeel dat het gewoon belangrijk is voor interpretatie!!!

@maireadshawMeasurementPracticesLargescale2020 checked the factor structure on the openly available item data from Many Labs 2 [@kleinManyLabs22018b]. They observed that no scales in their sample did the results from the factor analysis meet all of their evaluation criteria for unidimensionality. @maireadshawMeasurementPracticesLargescale2020 had access to measures that were used across multiple contexts to evaluate their validity across more than one instance. However, in practice, measures are frequently made on the fly and reused maybe once or never [@weidmanJingleJangleEmotion2017; @elsonPsychologicalMeasuresArent2023; @anvariFragmentedFieldConstruct2025; @anvariDefragmentingPsychology2025]. This is dangerous, because these measures are often not validated before use, and if they cannot be reused, they are not going to be validated afterwards either. Furthermore, validating a scale requires substantial sample sizes in most cases [@maccallumSampleSizeFactor1999, @gorsuchFactorAnalysis2013], sample sizes that many original studies cannot obtain, in which case these measures are reliant on future research to obtain the required sample size to validate the measure.

Thus, for our third research aim - similarly to our aim with reliability - we check the unidimensionality of measures from the Many Labs projects per lab with meta-analytic assessment of the variation in unidimensionality across labs. The combined goal of our three research aims is to create a description of the measurement use in our data.

# Disclosures

## Preregistration

We preregistered data collection, coding protocol, and planned analyses: <https://osf.io/jgxyu>. We deviated from our coding protocol as is explained further in the text. Our pre-registered analyses have been move to the [Supplementary Analyses A](../../SupplementaryMaterials/SupplementaryAnalysesScripts/Supplementary_exploratory_version_pre-reg_analyses.Rmd) in favor of focusing on the descriptive results here.

## Data, Materials, and Online Resources

This manuscript was created in RStudio [*v`r rstudioapi::versionInfo()$version`*; @R-Rstudio] with R Version `r paste0(R.Version()$major, ".", R.Version()$minor)` [@R-base], and generated using the Workflow for Open Reproducible Code in Science [*v`r getNamespaceVersion("worcs")[[1]]`*; @R-worcs] to ensure reproducibility and transparency. All code and data used to generate this manuscript and its results are available at: <https://github.com/CasGoos/measurement_and_replication> and <https://osf.io/9r8yt/> (DOI: 10.17605/OSF.IO/9R8YT).

## Reporting

We report how we determined all data exclusions, all manipulations, and all measures in the study. Our sample size was predetermined by the number of studies in the Many Labs projects.

## Ethical Approval

This research was approved by the Tilburg University School of Social and Behavioral Sciences Ethical Review Board (nr. TSB_TP_REMA06).

# Method

## Data Source

The data consists of three main sources: replication datasets, replication protocols, and original study articles. We retrieved the data of Many Labs 1, 2, 3, & 5 [@kleinInvestigatingVariationReplicability2014a; @ebersoleManyLabs32016; @kleinManyLabs22018b; @ebersoleManyLabs52020d] from their respective OSF pages. Many Labs 4 [@kleinManyLabs42022] was excluded, as there was no publicly available replication protocol. Additionally, the replication of [@crosbyWhereWeLook2008a] in Many Labs 5 made use of videos and eye-tracking measures, which did not match this study’s focus on item-based measures.

## Unit of Analysis

Our unit of analysis is a measure of a single variable within a replication protocol or original article that was used in the main analysis that was replicated. For example, if conscientiousness and agreeableness were both measured using a big-five personality questionnaire, each of which was a variable in a replicated effect, then both the conscientiousness and agreeableness part of that questionnaire would be coded as their own unit of analysis. We also allowed for multiple variables to be measured per study. We used the replication protocols to identify the measure of each variable. We did not include acquiescence bias checks, manipulation checks, pilot test measures, and measures added for exploratory analyses. Our final sample size was `r nrow(coded_data_replications)` measures of unique variables for both original and replication studies\^[Initially the original articles contained 3 more measures of unique variables than the replication protocols. This difference was due to the way that the moral foundations questionnaire was framed in the original article compared to in the replication protocol. In the original article it was framed as measuring five different moral foundations, while in the replication protocol the measure assessed the two overarching categories that were used to test the main effect in both the original and replication research. The measurement information reported was comparable across all five categories, and thus it was deemed that the measurement could be reduced to reflect two overarching categories facilitate easier comparison between measurement in original and replication.]

## Data Collection

The data on the preregistered replication protocols, and replication datasets from Many Labs 1, 2, 3, & 5 were all retrieved from their respective OSF pages: <https://osf.io/wx7ck/>, <https://osf.io/8cd4r/>, <https://osf.io/ct89g/>, & <https://osf.io/7a6rd/>. Both the replication protocols and replication datasets were scanned through to ensure that they were the correct files to code measurement reporting information from. However, no coding or analysis of either of them had taken place before the analyses were preregistered. Further details on the search strategy can be found in the [coding protocol information file](../../SupplementaryMaterials/CodingProtocols/coding_protocol_information.Rmd) in the supplementary materials.

### Replication Datasets

The replication datasets refer to the publicly available datasets containing the data obtained from all labs of each Many Labs replication. For the analyses, we extracted the scores on the items of each previously identified measure that also met our inclusion criteria specified in the paragraph below. When scores could not be clearly identified, any available codebooks, analysis scripts, or study materials were used to identify the relevant scores.

To be included in our reliability recalculations and factor analyses, the measure had to be a scale of multiple items. If cleaned data were available, we chose these over raw data, to ensure that variables were coded as intended (e.g., no reverse-coded items). We omitted pilot data from the analyses. These criteria combined with difficulties in retrieving suitable data from some datasets resulted in a set of item score data from `r length(unique(calculated_reliability_lab_data$g))` replication sets spread across on average approximately `r round(mean(table(calculated_reliability_lab_data$g)), 0)` lab locations for our analyses.

```{r CleaningReplicationDatasetsData, include = FALSE, eval = FALSE}
##### This code can be used to rerun the data preparation to convert the raw
##### (input) data of the Many Labs replications into intermediate data.
##### However, because the intermediate data is also available in this project,
##### the manuscript can also be reproduced without running this code block.

### Below we extract the relevant data from the Many Labs' datasets. The first
### number in each dataset refers to the Many Labs project it is related to.
### The second number to which study the data is from in order of appearance in
### the Many Labs pre-registered protocols, or in the case of Many Labs 5, 
### within the OSF folder structure. If there is a third number, the study had 
### multiple relevant measures this refers to the order of appearance of the 
### measure where the data is from within the study's description in the Many 
### Labs protocol or OSF folder structure. A fourth number indicates which part
### of the data of this measure was taken, in case the measure assessed multiple
### constructs, as these were treated separately for some analyses.

## ML 1
# 1.3
data_1.3_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[22:29])
colnames(data_1.3_clean)[1] <- "g"
# 1.10
data_1.10_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[108:115])
colnames(data_1.10_clean)[1] <- "g"
# 1.11
data_1.11_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[73:76])
colnames(data_1.11_clean)[1] <- "g"
# 1.12.1
# not found
# 1.12.3
data_1.12.3.1_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[54:59])
colnames(data_1.12.3.1_clean)[1] <- "g"
data_1.12.3.2_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[60:65])
colnames(data_1.12.3.2_clean)[1] <- "g"

## ML 2
# 2.2
data_2.2_clean <- cbind(as.factor(data_2.2[[5]]), data_2.2[6:11])
colnames(data_2.2_clean)[1] <- "g"
# 2.3
# data does not appear suitable
# 2.4.1
data_2.4.1_clean <- cbind(as.factor(data_2.4.1[[5]]), data_2.4.1[6:11])
colnames(data_2.4.1_clean)[1] <- "g"
# 2.4.2
data_2.4.2_clean <- cbind(as.factor(data_2.4.2[[5]]), data_2.4.2[6:14])
colnames(data_2.4.2_clean)[1] <- "g"
# 2.8.2
data_2.8.2_clean <- cbind(as.factor(data_2.8.2[[6]]), data_2.8.2[9:13])
colnames(data_2.8.2_clean)[1] <- "g"
# 2.10.1
data_2.10.1_clean <- cbind(as.factor(data_2.10.1[[5]]), data_2.10.1[6:11])
colnames(data_2.10.1_clean)[1] <- "g"
# 2.12.1
data_2.12.1_clean <- cbind(as.factor(data_2.12[[5]]), data_2.12[c(6,7,8,9,10,31,32,33,34,35)]) 
data_2.12.1_clean[3465:6905,2:6] <- data_2.12.1_clean[3465:6905,7:11]
data_2.12.1_clean <- data_2.12.1_clean[1:6]
colnames(data_2.12.1_clean)[1] <- "g"
# 2.12.2
data_2.12.2_clean <- cbind(as.factor(data_2.12[[5]]), data_2.12[c(11,14,15,18,19,22,24,27,28,29,36,39,40,43,44,47,49,52,53,54)]) 
data_2.12.2_clean[3465:6905,2:11] <- data_2.12.2_clean[3465:6905,12:21]
data_2.12.2_clean <- data_2.12.2_clean[1:11]
colnames(data_2.12.2_clean)[1] <- "g"
# 2.12.3
data_2.12.3_clean <- cbind(as.factor(data_2.12[[5]]), data_2.12[c(12,13,16,17,20,21,23,25,26,30,37,38,41,42,45,46,48,50,51,55)]) 
data_2.12.3_clean[3465:6905,2:11] <- data_2.12.3_clean[3465:6905,12:21]
data_2.12.3_clean <- data_2.12.3_clean[1:11]
colnames(data_2.12.3_clean)[1] <- "g"
# 2.15
data_2.15_clean <- cbind(as.factor(data_2.15[[5]]), data_2.15[8:12])
colnames(data_2.15_clean)[1] <- "g"
# 2.19.1
# difficult to extract
# 2.19.2
# difficult to extract
# 2.20
data_2.20_clean <- cbind(as.factor(data_2.20[[5]]), data_2.20[6:45]) 
data_2.20_clean[3729:7396,2:21] <- data_2.20_clean[3729:7396,22:41]
data_2.20_clean <- data_2.20_clean[1:21] 
# coding so all 1's means somebody used rule-based grouping strategy
data_2.20_clean[,c(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)] <- ifelse(data_2.20_clean[,c(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)] == 1, 1, 0)
data_2.20_clean[,c(3, 5, 7, 9, 11, 13, 15, 17, 19, 21)] <- ifelse(data_2.20_clean[,c(3, 5, 7, 9, 11, 13, 15, 17, 19, 21)] == 2, 1, 0)
colnames(data_2.20_clean)[1] <- "g"
# 2.23
data_2.23_clean <- cbind(as.factor(data_2.23[[5]]), data_2.23[c(7,8,12,13,15)])
colnames(data_2.23_clean)[1] <- "g"


## ML 3
# 3.2.1
data_3.2.1 <- cbind(as.factor(data_ml3[[1]]), data_ml3[77:86] - 1)
data_3.2.1.1_clean <- na.omit(data_3.2.1[1:6])
colnames(data_3.2.1.1_clean)[1] <- "g"
data_3.2.1.2_clean <- na.omit(data_3.2.1[c(1, 7:11)])
colnames(data_3.2.1.2_clean)[1] <- "g"
# 3.5
# data appears unusable
# 3.7.1
data_3.7.1_clean <- na.omit(cbind(as.factor(data_ml3[[1]]), data_ml3[38:42]))
colnames(data_3.7.1_clean)[1] <- "g"
# 3.7.2
data_3.7.2_clean <- na.omit(cbind(as.factor(data_ml3[[1]]), data_ml3[89:94]))
colnames(data_3.7.2_clean)[1] <- "g"
# 3.8.1
# a single measure was reported
# 3.8.2
data_3.8.2_clean <- na.omit(cbind(as.factor(data_ml3[[1]]), data_ml3[29:30])) 
colnames(data_3.8.2_clean)[1] <- "g"


## ML 5
# 5.1.1
data_5.1.1_clean <- cbind(as.factor(data_5.1[[2]]), data_5.1[13:27])
colnames(data_5.1.1_clean)[1] <- "g"
# 5.1.2
data_5.1.2_clean <- cbind(as.factor(data_5.1[[2]]), data_5.1[28:33])
colnames(data_5.1.2_clean)[1] <- "g"
# 5.4
data_5.4_clean <- cbind(as.factor(data_5.4[[1]]), data_5.4[18:41])
colnames(data_5.4_clean)[1] <- "g"
# 5.5.1 & 5.5.2
# from this dataset it appears that this data will be difficult to use.
# 5.5.2
# also difficult to use
# 5.7 
data_5.7_clean <- cbind(as.factor(data_5.7[[3]]), data_5.7[c(25, 34, 35, 36, 37, 38, 39, 40, 41, 42)])
colnames(data_5.7_clean)[1] <- "g"
# 5.9.1
data_5.9.1_clean <- na.omit(cbind(as.factor(data_5.9.1[[4]]), data_5.9.1[c(79, 83, 87, 91, 95, 98, 101)]))
colnames(data_5.9.1_clean)[1] <- "g"


### Saving to Intermediate Data Folder
saving_to_intermediate_data(data_1.3_clean)
saving_to_intermediate_data(data_1.10_clean)
saving_to_intermediate_data(data_1.11_clean)
saving_to_intermediate_data(data_1.12.3.1_clean)
saving_to_intermediate_data(data_1.12.3.2_clean)
saving_to_intermediate_data(data_2.10.1_clean)
saving_to_intermediate_data(data_2.12.1_clean)
saving_to_intermediate_data(data_2.12.2_clean)
saving_to_intermediate_data(data_2.12.3_clean)
saving_to_intermediate_data(data_2.15_clean)
saving_to_intermediate_data(data_2.20_clean)
saving_to_intermediate_data(data_2.23_clean)
saving_to_intermediate_data(data_3.2.1.1_clean)
saving_to_intermediate_data(data_3.2.1.2_clean)
saving_to_intermediate_data(data_3.7.1_clean)
saving_to_intermediate_data(data_3.7.2_clean)
saving_to_intermediate_data(data_3.8.2_clean)
saving_to_intermediate_data(data_5.1.1_clean)
saving_to_intermediate_data(data_5.1.2_clean)
saving_to_intermediate_data(data_5.4_clean)
saving_to_intermediate_data(data_5.7_clean)
saving_to_intermediate_data(data_5.9.1_clean)

```

### Replication Protocols

The replication protocols refer to the publicly available protocols describing the background, methodology, and analysis of the replication of an original study. These were retrieved from the OSF pages of the Many Labs projects (the search strategy and OSF file locations can be found in the [data retrieval information](../../SupplementaryMaterials/data_retrieval_information.Rmd) supplementary document; URL <https://github.com/CasGoos/measurement_and_replication/blob/master/SupplementaryMaterials/data_retrieval_information.Rmd>).

### Original Articles

We identified and retrieved all original study articles using the citations for these articles in each replication protocol.

```{r CleaningCodedData, include = FALSE, eval = FALSE}
##### This code can be used to rerun the data preparation to convert the raw 
##### (input) data of the coded data into intermediate data. However, because 
##### the intermediate data is also available in this project, the manuscript  
##### can also be reproduced without running this code block.

# Selecting the relevant rows and columns for the data
coded_data_initial_sel <- coded_data_initial_raw[3:160, 18:57]
coded_data_revised_sel <- coded_data_revised_raw[3:160, 18:38]
coded_data_vignette_sel <- coded_data_vignette_raw[3:160, 2]

# Combining the datasets
coded_data_full <- cbind(coded_data_initial_sel, 
                         cbind(coded_data_revised_sel, coded_data_vignette_sel))

# filtering out unnecessary double columns
coded_data_full <- cbind(coded_data_full[, 1:40], coded_data_full[, 45:62])


### data preparation
# renaming columns
colnames(coded_data_full) <- c("many_labs_version", "rep_org", "title", "measure_name", 
      "variable_name", "multi", "variable_order", "N", "N_items", 
      "hypothesis_support", "reliability_type", "reliability_type_text", 
      "reliability_coeff", "def_1", "op_version", "op_1", "op_2", "op_3", "op_4", 
      "op_5", "sel_existing", "sel_existing_text", "sel_1", "sel_2", "sel_3", 
      "sel_4", "sel_psychometric_evidence", "sel_psychometric_evidence_text", 
      "quant_1", "quant_2", "quant_3", "quant_4", "mod_check", "mod_1", "mod_2", 
      "mod_3", "mod_4", "mod_5", "mod_6", "mod_time", "op_1_REV", "op_2_REV",
      "op_5_REV", "sel_1_REV", "sel_3_REV", "sel_psychometric_evidence_REV", 
      "sel_psychometric_evidence_text_REV", "quant_1_REV", "quant_2_REV", 
      "quant_3_REV", "mod_check_REV", "mod_1_REV", "mod_2_REV", "mod_3_REV", 
      "mod_4_REV", "mod_5_REV", "mod_6_REV", "inseperable_material")
  
# renaming rows
rownames(coded_data_full) <- 1:nrow(coded_data_full)

# fixing some coding mistakes
coded_data_full$variable_name[79] <- "quote attribution effect"
coded_data_full$N[3] <- "5284"
coded_data_full$N[158] <- "1202"
coded_data_full$reliability_type[50] <- "Not Reported"
coded_data_full$reliability_type[127] <- "Not Reported"
coded_data_full$op_1[145] <- "False"
coded_data_full$op_3[121] <- "True"
coded_data_full$op_5[157] <- "True"
coded_data_full$sel_1[59] <- "True"
coded_data_full$quant_2[113] <- "True"
coded_data_full$mod_time[1] <- "Before"
coded_data_full$psychometric_evidence_text_REV[coded_data_full$sel_psychometric_evidence_text_REV == "convergent validitiy"] <- "convergent validity"


# removing missing entry 77
coded_data_full <- data.frame(coded_data_full)[-77,]

# Many labs 2.25 and 2.26, as well as 3.4 and 3.5 (for replications) were coded 
# in reverse order thus need to be swapped in right order. Additionally, some
# of the entries were included later than following their order, due to some
# minor coding oversights.
Coded_Data_Full_Restructured <- coded_data_full[c(1:18, 148, 19:23, 147, 24:28, 
                                                  153, 29:40, 154, 42, 41, 155, 
                                                  43:49, 156, 51, 50, 52:76, 
                                                  157, 77:88, 149:152, 89:146),]


# rename the rownames to match the new order
rownames(Coded_Data_Full_Restructured) <- 1:nrow(Coded_Data_Full_Restructured)

# changing the variable types for each column to better represent their 
# intended variable type
class(Coded_Data_Full_Restructured$many_labs_version) <- "numeric"
Coded_Data_Full_Restructured$many_labs_version <- as.factor(Coded_Data_Full_Restructured$many_labs_version)
Coded_Data_Full_Restructured$rep_org <- droplevels(as.factor(Coded_Data_Full_Restructured$rep_org))
Coded_Data_Full_Restructured$multi <- droplevels(as.factor(Coded_Data_Full_Restructured$multi))
Coded_Data_Full_Restructured$variable_order <- droplevels(as.factor(Coded_Data_Full_Restructured$variable_order))
class(Coded_Data_Full_Restructured$N) <- "numeric"
Coded_Data_Full_Restructured$N_items <- droplevels(as.factor(Coded_Data_Full_Restructured$N_items))
Coded_Data_Full_Restructured$hypothesis_support <- droplevels(as.factor(Coded_Data_Full_Restructured$hypothesis_support))
levels(Coded_Data_Full_Restructured$hypothesis_support) <- c("No", "Unclear", "Yes")
Coded_Data_Full_Restructured$reliability_type <- droplevels(as.factor(Coded_Data_Full_Restructured$reliability_type))
class(Coded_Data_Full_Restructured$reliability_coeff) <- "numeric"
Coded_Data_Full_Restructured$def_1 <- as.logical(Coded_Data_Full_Restructured$def_1)
Coded_Data_Full_Restructured$op_1 <- as.logical(Coded_Data_Full_Restructured$op_1)
Coded_Data_Full_Restructured$op_2 <- as.logical(Coded_Data_Full_Restructured$op_2)
Coded_Data_Full_Restructured$op_3 <- as.logical(Coded_Data_Full_Restructured$op_3)
Coded_Data_Full_Restructured$op_4 <- as.logical(Coded_Data_Full_Restructured$op_4)
Coded_Data_Full_Restructured$op_5 <- as.logical(Coded_Data_Full_Restructured$op_5)
Coded_Data_Full_Restructured$sel_existing <- droplevels(as.factor(Coded_Data_Full_Restructured$sel_existing))
Coded_Data_Full_Restructured$sel_1 <- as.logical(Coded_Data_Full_Restructured$sel_1)
Coded_Data_Full_Restructured$sel_2 <- as.logical(Coded_Data_Full_Restructured$sel_2)
Coded_Data_Full_Restructured$sel_3 <- as.logical(Coded_Data_Full_Restructured$sel_3)
Coded_Data_Full_Restructured$sel_4 <- as.logical(Coded_Data_Full_Restructured$sel_4)
Coded_Data_Full_Restructured$sel_psychometric_evidence <- droplevels(as.factor(Coded_Data_Full_Restructured$sel_psychometric_evidence))
Coded_Data_Full_Restructured$quant_1 <- as.logical(Coded_Data_Full_Restructured$quant_1)
Coded_Data_Full_Restructured$quant_2 <- as.logical(Coded_Data_Full_Restructured$quant_2)
Coded_Data_Full_Restructured$quant_3 <- as.logical(Coded_Data_Full_Restructured$quant_3)
Coded_Data_Full_Restructured$quant_4 <- as.logical(Coded_Data_Full_Restructured$quant_4)
Coded_Data_Full_Restructured$mod_check <- droplevels(as.factor(Coded_Data_Full_Restructured$mod_check))
Coded_Data_Full_Restructured$mod_1 <- as.logical(Coded_Data_Full_Restructured$mod_1)
Coded_Data_Full_Restructured$mod_2 <- as.logical(Coded_Data_Full_Restructured$mod_2)
Coded_Data_Full_Restructured$mod_3 <- as.logical(Coded_Data_Full_Restructured$mod_3)
Coded_Data_Full_Restructured$mod_4 <- as.logical(Coded_Data_Full_Restructured$mod_4)
Coded_Data_Full_Restructured$mod_5 <- as.logical(Coded_Data_Full_Restructured$mod_5)
Coded_Data_Full_Restructured$mod_6 <- as.logical(Coded_Data_Full_Restructured$mod_6)
Coded_Data_Full_Restructured$mod_time <- droplevels(as.factor(Coded_Data_Full_Restructured$mod_time))
Coded_Data_Full_Restructured$op_1_REV <- as.logical(Coded_Data_Full_Restructured$op_1_REV)
Coded_Data_Full_Restructured$op_2_REV <- as.logical(Coded_Data_Full_Restructured$op_2_REV)
Coded_Data_Full_Restructured$op_5_REV <- as.logical(Coded_Data_Full_Restructured$op_5_REV)
Coded_Data_Full_Restructured$sel_1_REV <- as.logical(Coded_Data_Full_Restructured$sel_1_REV)
Coded_Data_Full_Restructured$sel_3_REV <- as.logical(Coded_Data_Full_Restructured$sel_3_REV)
Coded_Data_Full_Restructured$sel_psychometric_evidence_REV <- droplevels(as.factor(Coded_Data_Full_Restructured$sel_psychometric_evidence_REV))
Coded_Data_Full_Restructured$quant_1_REV <- as.logical(Coded_Data_Full_Restructured$quant_1_REV)
Coded_Data_Full_Restructured$quant_2_REV <- as.logical(Coded_Data_Full_Restructured$quant_2_REV)
Coded_Data_Full_Restructured$quant_3_REV <- as.logical(Coded_Data_Full_Restructured$quant_3_REV)
Coded_Data_Full_Restructured$mod_check_REV <- droplevels(as.factor(Coded_Data_Full_Restructured$mod_check_REV))
Coded_Data_Full_Restructured$mod_1_REV <- as.logical(Coded_Data_Full_Restructured$mod_1_REV)
Coded_Data_Full_Restructured$mod_2_REV <- as.logical(Coded_Data_Full_Restructured$mod_2_REV)
Coded_Data_Full_Restructured$mod_3_REV <- as.logical(Coded_Data_Full_Restructured$mod_3_REV)
Coded_Data_Full_Restructured$mod_4_REV <- as.logical(Coded_Data_Full_Restructured$mod_4_REV)
Coded_Data_Full_Restructured$mod_5_REV <- as.logical(Coded_Data_Full_Restructured$mod_5_REV)
Coded_Data_Full_Restructured$mod_6_REV <- as.logical(Coded_Data_Full_Restructured$mod_6_REV)
Coded_Data_Full_Restructured$inseperable_material <- droplevels(as.factor(Coded_Data_Full_Restructured$inseperable_material)

                                                                
                                                                

# the moral foundations questionnaire in original 2.4 is reported using all 5 of 
# its factors, whereas in replication 2.4 only the two overarching groups of 
# binding and individualizing foundations are described. For that reason a 
# shortened original dataset will be used for any direct comparisons between 
# original and replication coding.

Coded_Data_Full_Shortened <- Coded_Data_Full_Restructured[c(1:94, 96, 99:157),] 
Coded_Data_Full_Shortened[c(95,96),5] <- c("individualizing moral foundations", "binding moral foundations")
Coded_Data_Full_Shortened[95,13] <- NA
Coded_Data_Full_Shortened[96,13] <- NA


# separating the replication and original data from each other.
coded_data_replications <- Coded_Data_Full_Shortened[1:77,]
coded_data_original <- Coded_Data_Full_Shortened[78:154,]

# exporting cleaned data
saving_to_intermediate_data(coded_data_replications)
saving_to_intermediate_data(coded_data_original)

```

## Measures

### Coding Protocol

We extracted reported measurement information from the original articles and replication protocols using our preregistered coding protocol. We extracted the reported reliability coefficient and type of index (Cronbach’s Alpha, retest, interrater, etc.) when present. Similarly, we coded if any psychometric construct validity evidence, such as a factor analysis, was presented for the measure. We additionally included twenty items that were based on the QMP table presented in @flakeMeasurementSchmeasurementQuestionable2020. For each of these items we checked if a piece information we would want to see reported with regards to the measurement was described. If the information was clearly reported we coded the item as true; false if not or not clear enough; or not applicable if not relevant for that measure (e.g., reporting results from a factor analysis for single-item measures). Example items can be seen in Table \@ref(tab:QMPCodingInfoTable).

```{r QMPCodingInfoTable, warning = FALSE}
QMP_info_dataframe <- data.frame(Category = c("Definition", "", "", "Operationalisation", "", "", "", "Selection/Creation", "", "", "Quantification", "Modification", "", "", "", "", ""),
           'N Questions' = c("1", "", "", "5", "", "", "", "4", "", "", "4", "6", "", "", "", "", ""),
           'Example Question' = c("A psychological/sociological definition",
            "is given to the name of the measured",
            "variable within the paper.", 
            "The administration format (pen-and-",
            "paper/computer) and environment (in",
            "public/in a lab) are described (Note:",
            "both should be present for a true rating).", 
            "The source of the scale is provided",
            "(in case the scale was newly developed",
            "this should be clearly stated).", 
            "The number of items are described.", 
            "Any format changes are mentioned",
            "(paper-and-pencil <–> computer), if no",
            "changes were made to the format, and",
            "this was mentioned then code as No",
            "modification. If it is not clear, then code",
            "as False."))

# making the column names look less robot speak-y.
colnames(QMP_info_dataframe) <- c("Category", "N Questions", "Example Question")


# transfer the data to an APA table for printing
apa_table(
  QMP_info_dataframe, align = c("l", "r", "l")
  , caption = "Information of QMP coding variables per category."
  , note = "N Questions refers only to the questions used for calculating QMP ratios. Selection and creation share a category as the justifications and requirements in selecting a measure are similar to those for creating a new measure."
  , escape = FALSE, placement = "htp", booktabs = TRUE)

```

After the initial coding, we made minor revisions to fourteen of the twenty QMP items from the preregistered coding protocol. These items were changed after familiarizing ourselves with the way measurement was reported, and it became clear that our criteria were too stringent in the exact way and degree of detail in which to present things. For example, in the initial protocol, an example item had to be present within the article or protocol itself, or else this was counted as a QMP. In the revised protocol, references to online appendices with example items were also considered sufficient for this item. The analyses, tables, and figures presented in this article are all based on the revised coding protocol. The equivalent QMP descriptives obtained with the initial protocol can be found in [Supplementary Analyses B](../../SupplementaryMaterials/SupplementaryAnalysesScripts/Supplementary_initial_QMP_ratio_table.rmd).

We initially intended to construct a QMP index from the QMP items and perform regression analyses between this index and replication outcomes. However, this index could not be validated properly and its results would be misleading to present here in the main article. A more detailed explanation of what was omitted and why, as well as the results from these preregistered analyses can be found in [Supplementary Analyses A](../../SupplementaryMaterials/SupplementaryAnalysesScripts/Supplementary_exploratory_version_pre-reg_analyses.Rmd).

```{r Calculating_QMP_Data, include = FALSE, eval = FALSE}
# function for changing QMP data to their ratio equivalent
Create_QMP_descriptive_case <- function(practice_name, original_coded_var, replication_coded_var){
  # add practice name to first row
  Practice <- practice_name
  
  # Sum all QMP/GMP (Good Measurement Practice), keeping NA items out of
  # the equation
  original_n_QMPs <- sum(original_coded_var == "FALSE", na.rm = TRUE)
  replication_n_QMPs <- sum(replication_coded_var == "FALSE", na.rm = TRUE)
  
  original_n_GMPs <- sum(original_coded_var == "TRUE", na.rm = TRUE)
  replication_n_GMPs <- sum(replication_coded_var == "TRUE", na.rm = TRUE)
  
  # calculating the N of applicable items for originals
  N_applicable_original <- original_n_QMPs + original_n_GMPs
  
  # checking that we are not dividing by 0, and then calculating the ratio of 
  # QMPs for applicable items for originals
  if(N_applicable_original != 0){
    QMP_percentage_original <- round(original_n_QMPs / (N_applicable_original), 2)
  } else{
    QMP_percentage_original <- 0
  }                               
  
  # calculating the N of applicable items for replications
  N_applicable_replication <- replication_n_QMPs + replication_n_GMPs
  
  # checking that we are not dividing by 0, and then calculating the ratio of 
  # QMPs for applicable items for replications
  if(N_applicable_replication != 0){
    QMP_percentage_replication <- round(replication_n_QMPs / N_applicable_replication, 2)
  } else{
    QMP_percentage_replication <- 0
  }     
  
  # calculating the Phi coefficient to estimate the relation between variables
  Phi <- phi(matrix(c(original_n_QMPs, original_n_GMPs, replication_n_QMPs, 
                      replication_n_GMPs), nrow = 2, byrow = TRUE))
  
  return(c(Practice, QMP_percentage_original, N_applicable_original, QMP_percentage_replication, N_applicable_replication, Phi))
}



# loading the relevant items
original_QMP_data <- coded_data_original[c("def_1", "sel_2", "op_4", "reliability_type", "sel_existing", "op_version", "op_1_REV", "sel_1_REV", "sel_3_REV", "sel_4", "sel_psychometric_evidence", "op_2_REV", "op_3", "quant_1_REV", "quant_2_REV", "quant_3_REV", "quant_4", "op_5_REV", "mod_1_REV", "mod_2_REV", "mod_3_REV", "mod_4_REV", "mod_5_REV", "mod_6_REV")]

replication_QMP_data <- coded_data_replications[c("def_1", "sel_2", "op_4", "reliability_type", "sel_existing", "op_version", "op_1_REV", "sel_1_REV", "sel_3_REV", "sel_4", "sel_psychometric_evidence", "op_2_REV", "op_3", "quant_1_REV", "quant_2_REV", "quant_3_REV", "quant_4", "op_5_REV", "mod_1_REV", "mod_2_REV", "mod_3_REV", "mod_4_REV", "mod_5_REV", "mod_6_REV")]



# create an empty dataset to add a recoded TRUE and FALSE response for QMPs to 
QMP_data <- data.frame(delete_this = rep(NA, 77))


QMP_data$reliability_reported_org <- ifelse(coded_data_original$N_items == "multiple item measure", ifelse(original_QMP_data$reliability_type != "Not Reported" & original_QMP_data$reliability_type != "" & !is.na(original_QMP_data$reliability_type), TRUE, ifelse(original_QMP_data$reliability_type == "Not Reported", FALSE, NA)), NA)

QMP_data$reliability_reported_rep <- ifelse(coded_data_replications$N_items == "multiple item measure", ifelse(replication_QMP_data$reliability_type != "Not Reported" & replication_QMP_data$reliability_type != "" & !is.na(replication_QMP_data$reliability_type), TRUE, ifelse(replication_QMP_data$reliability_type == "Not Reported", FALSE, NA)), NA)


# add if clearly specified if measure existed or not
QMP_data$select_or_create_clarity_org <- original_QMP_data$sel_existing == "Not Clearly Stated"

QMP_data$select_or_create_clarity_rep <- replication_QMP_data$sel_existing == "Not Clearly Stated"


# add if version was clearly specified
QMP_data$version_clarity_org <- original_QMP_data$op_1_REV == FALSE | original_QMP_data$op_version == "" & original_QMP_data$sel_existing == "True, namely:"

QMP_data$version_clarity_rep <- replication_QMP_data$op_1_REV == FALSE | replication_QMP_data$op_version == "" & replication_QMP_data$sel_existing == "True, namely:"


# add if factor structure was analysed
QMP_data$factor_analysis_org <- ifelse(original_QMP_data$sel_psychometric_evidence == "None", FALSE, ifelse(original_QMP_data$sel_psychometric_evidence != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)", TRUE, NA))

QMP_data$factor_analysis_rep <- ifelse(replication_QMP_data$sel_psychometric_evidence == "None", FALSE, ifelse(replication_QMP_data$sel_psychometric_evidence != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)", TRUE, NA))

# direct copies
QMP_data$justified_definition_org <- original_QMP_data$def_1
QMP_data$justified_definition_rep <- replication_QMP_data$def_1
QMP_data$justified_selection_org <- original_QMP_data$sel_2
QMP_data$justified_selection_rep <- replication_QMP_data$sel_2
QMP_data$justified_operationalisation_org <- original_QMP_data$op_4
QMP_data$justified_operationalisation_rep <- replication_QMP_data$op_4
QMP_data$source_org <- original_QMP_data$sel_1_REV
QMP_data$source_rep <- replication_QMP_data$sel_1_REV
QMP_data$in_psychometric_org <- original_QMP_data$sel_3_REV
QMP_data$in_psychometric_rep <- replication_QMP_data$sel_3_REV
QMP_data$out_psychometric_org <- original_QMP_data$sel_4
QMP_data$out_psychometric_rep <- replication_QMP_data$sel_4
QMP_data$admin_format_org <- original_QMP_data$op_2_REV
QMP_data$admin_format_rep <- replication_QMP_data$op_2_REV
QMP_data$admin_procedure_org <- original_QMP_data$op_3
QMP_data$admin_procedure_rep <- replication_QMP_data$op_3
QMP_data$N_items_org <- original_QMP_data$quant_1_REV
QMP_data$N_items_rep <- replication_QMP_data$quant_1_REV
QMP_data$N_response_org <- original_QMP_data$quant_2_REV
QMP_data$N_response_rep <- replication_QMP_data$quant_2_REV
QMP_data$recoding_org <- original_QMP_data$quant_3_REV
QMP_data$recoding_rep <- replication_QMP_data$quant_3_REV
QMP_data$aggregation_org <- original_QMP_data$quant_4
QMP_data$aggregation_rep <- replication_QMP_data$quant_4
QMP_data$example_items_org <- original_QMP_data$op_5_REV
QMP_data$example_items_rep <- replication_QMP_data$op_5_REV
QMP_data$mod_admin_format_org <- original_QMP_data$mod_1_REV
QMP_data$mod_admin_format_rep <- replication_QMP_data$mod_1_REV
QMP_data$mod_admin_format_support_org <- original_QMP_data$mod_2_REV
QMP_data$mod_admin_format_support_rep <- replication_QMP_data$mod_2_REV
QMP_data$mod_language_org <- original_QMP_data$mod_3_REV
QMP_data$mod_language_rep <- replication_QMP_data$mod_3_REV
QMP_data$mod_language_support_org <- original_QMP_data$mod_4_REV
QMP_data$mod_language_support_rep <- replication_QMP_data$mod_4_REV
QMP_data$mod_N_items_or_response_org <- original_QMP_data$mod_5_REV
QMP_data$mod_N_items_or_response_rep <- replication_QMP_data$mod_5_REV
QMP_data$mod_N_items_or_response_support_org <- original_QMP_data$mod_6_REV
QMP_data$mod_N_items_or_response_support_rep <- replication_QMP_data$mod_6_REV


QMP_data <- QMP_data[,!(names(QMP_data) %in% "delete_this")]



### QMP ratio data
# create the empty QMP ratio dataset
QMP_ratio_data <- data.frame(Practice = rep("", ncol(QMP_data)/2),
                   QMP_percentage_original = rep(0, ncol(QMP_data)/2),
                   N_applicable_original = rep(0, ncol(QMP_data)/2),
                   QMP_percentage_replication = rep(0, ncol(QMP_data)/2),
                   N_applicable_replication = rep(0, ncol(QMP_data)/2),
                   Phi = rep(0, ncol(QMP_data)/2))


# we loop through the length of the number of Measurement Practices (MPs) 
# ignoring doubles due to having both original and replication
for(i in 1:(ncol(QMP_data)/2)){
  # we create the name of the variable by taking the name and removing the 
  # _org suffix. Then we for each column except the NA initial column
  # we take the original MP and the related replication MP, and calculate
  # their QMP
  QMP_ratio_data[i,] <- Create_QMP_descriptive_case(substr(names(QMP_data)[i*2], 1, nchar(names(QMP_data)[i*2]) - 4), QMP_data[[(i*2)-1]], QMP_data[[i*2]])
  }

# store QMP ratio data in the analysis data folder
saving_to_analysis_data(QMP_ratio_data)
```

### Calculating Reliaiblity

We calculated reliability for each measure from the replication datasets with suitable data for each lab the measure was used in. We calculated both Cronbach’s alpha, as well as its standard error using formulas 2 & 3 from @duhachekAlphasStandardError2004a. We used these values to conduct a meta-analysis of the measure's reliability, also commonly referred to as a Reliability Generalization (RG) Meta-Analysis [@botellaManagingHeterogeneityVariance2012; @lopez-ibanezReliabilityGeneralizationMetaanalysis2024; @vacha-haaseReliabilityGeneralizationExploring1998]. We performed the RG Meta-Analysis using the rma function from the metafor R package [*v`r getNamespaceVersion("metafor")[[1]]`*; @R-metafor] and default settings. We then used the results from the meta-analysis to evaluate the heterogeneity via the tau statistic and the Cochran's Q-test [@cochranCombinationEstimatesDifferent1954]. These indicators have received criticism as indicators of heterogeneity, especially when within study sample sizes are small and power to detect heterogeneity is low [@hoaglinMisunderstandingsCochransTest2016; @pereiraCriticalInterpretationCochrans2010]. Therefore, we also present the prediction interval and implore that the heterogeneity results should be viewed critically [@borensteinAvoidingCommonMistakes2024]. We implemented no correction for bias, because the Many Labs replications were not at risk of publication bias.

Our analyses will focus on Cronbach’s Alpha, because it is common in research, which also allows us to make comparisons between calculated and reported reliabilities. Furthermore, we were able to calculate the standard error of alpha to be used in the RG meta-analysis. However, Cronbach's Alpha comes with strong assumptions on the underlying factor structure, including that the measure is unidimensional, which means that there should a single factor underlying the measurement scores. Therefore, we also estimated McDonald’s Omega for each lab from the same set of replications as for Cronbach's Alpha, since it has been argued to be a more informative measure of reliability than Cronbach's Alpha with less strict assumptions [@crutzenScaleQualityAlpha2017b; @dengTestingDifferenceReliability2017]. The results based on Omega can be found in [Supplementary Analyses D](../../SupplementaryMaterials/SupplementaryAnalysesScripts/Supplementary_omega_analyses.Rmd).

```{r calculated_reliability_across_labs, include = FALSE, eval = FALSE}
# creating an empty data frame to insert all the responses into
calculated_reliability_lab_data <- data.frame(alpha = 0, omega.tot = 0, 
                                          omega.hier = 0, ASE = 0, g = 0)

# Combining the data together 1.3, & 5.4 were omitted, because data was not
# recorded in usable numeric format
extracted_score_data <- list(data_1.10_clean, data_1.11_clean, 
    data_1.12.3.1_clean, data_1.12.3.2_clean, data_2.12.1_clean, 
    data_2.12.2_clean, data_2.12.3_clean, data_2.15_clean, data_2.20_clean, 
    data_2.23_clean, data_3.2.1.1_clean, data_3.2.1.2_clean, data_3.7.1_clean,
    data_3.7.2_clean, data_3.8.2_clean, data_5.1.1_clean, data_5.1.2_clean, 
    data_5.7_clean, data_5.9.1_clean)


# obtaining the omega and alpha values for a measure in one lab.
get_omega_and_alpha_values <- function(Data){
  # first we calculate alpha and ase separately in case the omega function goes haywire
  alpha <- psych::alpha(Data)$total[["std.alpha"]]
  ase <- psych::alpha(Data)$total[["ase"]]
  
  # try to calculate the omega
  result <- c(NA, NA)
  tryCatch({
      result <- omega(Data)
    }, error = function(e) {
      result <- c(NA, NA)
    }, warning = function(w) {
      result <- c(NA, NA)
    })
  
  # combine the information in one vector
  omega_and_alpha_vec <- as.numeric(c(alpha, result[c(4, 1)], ase))
  
  return(omega_and_alpha_vec)
}


# calculate the alpha, omega.tot, omega.hier, & ASE for all relevant datasets
# for each lab.
for (i in 1:length(extracted_score_data)){
  calculated_reliability_instance <- tapply(extracted_score_data[[i]][-1], 
                                            extracted_score_data[[i]]$g, 
                                            get_omega_and_alpha_values)
  
  calculated_reliability_instance <- data.frame(matrix(unlist(
    calculated_reliability_instance), ncol = 4, byrow = TRUE))
  
  # make sure the var names match the complete dataframe
  colnames(calculated_reliability_instance) <- c("alpha", "omega.tot", 
                                                 "omega.hier", "ASE")
  # adding the measure as a group (g) indicator
  calculated_reliability_instance$g <- i
  
  # adding this measure's data to the total
  calculated_reliability_lab_data <- rbind(calculated_reliability_lab_data, 
                                       calculated_reliability_instance)
}


# removing the empty first row
calculated_reliability_lab_data <- calculated_reliability_lab_data[-1,]

# making sure group (g) is a factor
calculated_reliability_lab_data$g <- as.factor(calculated_reliability_lab_data$g)

# indexing the meta-analysis results with a specific index relating to a 
# row (measure) in coded_data_replications
calculated_reliability_lab_data$coded_data_index <- c(rep(10, 36), rep(11, 36), 
  rep(14, 36), rep(14, 36), rep(29, 74), rep(30, 74), rep(31, 74), rep(34, 61), 
  rep(39, 60), rep(42, 58), rep(51, 21), rep(51, 21), rep(59, 20), rep(60, 21), 
  rep(61, 21), rep(65, 4), rep(66, 4), rep(74, 8), rep(76, 5))

# changing the group variable to reflect measure descriptions from the text.
# checked using:
# coded_data_replications[unique(calculated_reliability_data$reporting_index), 3], and
# coded_data_replications[unique(calculated_reliability_data$reporting_index), 5]
levels(calculated_reliability_lab_data$g) <- c("Caruso et al. (2012)", 
    "Husnu & Crisp (2010)", "Nosek et al. (2002), Math", "Nosek et al. (2002), Art", 
    "Anderson et al. (2012), SWL", "Anderson et al. (2012), PA", 
    "Anderson et al. (2012), NA", "Giessner & Schubert, (2007)", 
    "Norenzayan et al. (2002)", "Zhong & Lijenquist (2006)", 
    "Monin & Miller (2001), most", "Monin & Miller (2001), some", 
    "Cacioppo et al. (1983), arg",  "Cacioppo et al. (1983), nfc", 
    "De Fruyt et al. (2000)", "Albarracín et al. (2008), exp 5 verb", 
    "Albarracín et al. (2008), exp 5 math", "Shnabel & Nadler (2008)",
    "Vohs & Schooler (2008)")
  
calculated_reliability_lab_data$g <- factor(calculated_reliability_lab_data$g, 
    labels = c("Caruso et al. (2012)", 
    "Husnu & Crisp (2010)", "Nosek et al. (2002), Math", 
    "Nosek et al. (2002), Art", "Anderson et al. (2012), SWL", 
    "Anderson et al. (2012), PA", "Anderson et al. (2012), NA", 
    "Giessner & Schubert, (2007)", "Norenzayan et al. (2002)", 
    "Zhong & Lijenquist (2006)", "Monin & Miller (2001), most", 
    "Monin & Miller (2001), some", "Cacioppo et al. (1983), arg",  
    "Cacioppo et al. (1983), nfc", "De Fruyt et al. (2000)", 
    "Albarracín et al. (2008), exp 5 verb", 
    "Albarracín et al. (2008), exp 5 math", "Shnabel & Nadler (2008)", 
    "Vohs & Schooler (2008)"))

# adding whether or not an effect replicated based on what was coded from the 
# replication report.
calculated_reliability_lab_data$replication_success <- c(coded_data_replications[
    calculated_reliability_lab_data$coded_data_index, "hypothesis_support"])



# store per lab calculated reliability data in the analysis data folder
saving_to_analysis_data(calculated_reliability_lab_data)
```

```{r averaged_reliability, include = FALSE, eval = FALSE}
# function to assess the heterogeneity in the calculated Cronbach's Alpha values
# and get Cronbach's Alpha prediction intervals
assess_heterogeneity<- function(data_on_alpha){
  # run a random effects meta-analysis
  rma_model <- rma(yi = data_on_alpha$alpha, sei = data_on_alpha$ASE, 
                   method = "REML", control = list(stepadj = 0.5, maxiter = 1000))
  
  # extract the relevant heterogeneity information
  temp_tau <- sqrt(rma_model$tau2)
  temp_QEp <- rma_model$QEp
  
  # get prediction intervals for alpha
  rma_prediction <- predict(rma_model)
  temp_pi.lb <- rma_prediction$pi.lb
  temp_pi.ub <- rma_prediction$pi.ub
  
  return(c(temp_tau, temp_QEp, temp_pi.lb, temp_pi.ub))
}

# function to convert the lab specifc reliability to averaged
convert_reliability_data_to_avg <- function(reliability_data){
  # conducting the reliability-generalization meta-analysis for each measure
  heterogeneity_results <- assess_heterogeneity(reliability_data[c(1, 4)])
  
  # getting the avearage reliability scores for each measure
  avg_reliabilities <- colMeans(reliability_data[1:4], na.rm = TRUE)
  
  # indicates the measure
  g <- reliability_data$g[[1]]
  
  # we need one of the indices per measure as they are all the same across labs
  coded_data_index <- reliability_data$coded_data_index[[1]]
  
  # we need one of the indices per measure as they are all the same across labs
  replication_success <- reliability_data$replication_success[[1]]
  
  # extracting the reported reliability coefficient 
  coefficient_reported <- coded_data_original$reliability_coeff[coded_data_index]
  
  # calculating the difference between reported and calculated average 
  # reliability coefficient
  coeficient_difference <- coefficient_reported - avg_reliabilities[[1]]
  
  # testing whether or not (for those studies that had a reported alpha) if
  # it was out of the 95% bounds around the mean calculated alpha
  population_95_bounds <- quantile(reliability_data$alpha, probs = c(0.025, 0.975))
  
  significance_reported_coefficient <- coefficient_reported < population_95_bounds[1] | 
                                          coefficient_reported > population_95_bounds[2]
  
  # return all the data as a single row in the dataframe
  return(data.frame(alpha = avg_reliabilities[[1]], omega.tot = avg_reliabilities[[2]], 
                    omega.hier = avg_reliabilities[[3]], ASE = avg_reliabilities[[4]], 
                    tau = heterogeneity_results[[1]], QEp = heterogeneity_results[[2]],
                    pi.lb = heterogeneity_results[[3]], pi.ub = heterogeneity_results[[4]], 
                    g = g, coded_data_index = coded_data_index, 
                    replication_success, reported_coefficient = coefficient_reported, 
                    coefficient_difference = coeficient_difference, 
                    significance_reported_coefficient = significance_reported_coefficient))
}

# calculate the average reliability data + heterogeneity test + reported and calculated
# reliability coefficient comparison
avg_reliability_list <- tapply(calculated_reliability_lab_data, calculated_reliability_lab_data$g, convert_reliability_data_to_avg)

# convert data output to a dataframe
measure_reliability_data <- do.call(rbind.data.frame, avg_reliability_list)


# store per measure  reliability data in the analysis data folder
saving_to_analysis_data(measure_reliability_data)
```

### Unidimensionality

All measurement items for which we checked unidimensionality, were used to form a singular index of one latent variable. Therefore, unidimensionality presents a valuable indication of the construct validity of a measure in our dataset. To test for unidimensionality, we fit a single-factor model on the item response data of each individual lab with the relevant measurement data. This was done to simulate the inference a lab would make if they were to check for unidimensionality. Our inference of unidimensionality is based on a set of five model fit indices. If the indices point towards a good unidimensional fit that is also consistent across labs - as we also evaluate the variability across labs - then we can say that with this preliminary evidence at least, we did not observe any issues with the construct validity of that measure. If the indices point towards a lack of unidimensionality in a substantial number of labs, that would be a warning sign for the lack of construct validity for that measure. Our goal is not to fully check the validity of these measures, this requires additional steps that are beyond the scope of this study.

For the first four of our five fit indices we fit a Confirmatory Factor Analysis (CFA) model with one factor to the data, and we then evaluate the RMSEA value, CFI value, SRMR value, and the result from the exact fit test. For the RMSEA and SRMR value we checked if it was below .08. For the CFI value we checked if the value was above .90. Finally, for the exact fit test we evaluated if the fit test was statistically significant at an alpha of .05. These values are based on common thresholds used to determine adequate fit. We consider the use of these thresholds sufficient for our descriptive aim, even though we understand apprehension against rules of thumb when used to evaluate measurement in individual studies.

For the final index we ran a parallel analysis. A parallel analysis runs multiple Exploratory Factor Analyses where the number of factors in the model is increased by 1 until the number of factors is one less than the number of items. It then compares the eigenvalues (an indication of how much variance is explained by that factor) of each factor to the eigenvalues for that factor if the data matrix was effectively random. If only the first factor has an eigenvalue that is significantly higher than the eigenvalue when the data matrix is random, the test is passed. We chose a combination of indices since each one has their own limitations, and combining them gives us a more robust picture of the unidimensionality of the measures.

```{r unidimensionality_tests, include = FALSE, eval = FALSE}
# function that runs all our checks for Cronbach's alpha. Also functions as some basic validity checks
validity_and_alpha_assumption_tests <- function(data){
  ### Tests for Unidimensionality
  # Test 1a: obtaining single factor model cfa RMSEA, CFI, SRMR, and exact fit test results
  RMSEA_values <- c(NA, NA)
  CFI <- NA
  SRMR <- NA
  fit_results <- c(NA, NA, NA)
  tryCatch({
    # creating the base function for the model (it is unidimensional)
    model_free <- "Factor =~ " 

    for (item_name in names(data)[-1]){
      # adding all of the items from the scale to the model formula
      model_free <- paste0(model_free, item_name, " + ")
    }
      
    # trimming the excess " + "
    model_free <- substring(model_free, 1, nchar(model_free) - 3)

    # fitting the free coefficient estimated single dimensional model
    fit.modelfree <- cfa(model_free, 
                              data = data, 
                              std.lv = TRUE, 
                              estimator = "MLM")

    # extracting the fit measures
    fit_measures <- fitMeasures(fit.modelfree, c("chisq", "df", "pvalue", "rmsea", "rmsea.ci.upper", "cfi", "srmr"))
    
    # extracting RMSEA and its standard error
    rmsea <- fit_measures["rmsea"] 
    rmsea.ci.upper <- fit_measures["rmsea.ci.upper"]
    rmsea.se <- (rmsea.ci.upper - rmsea) / 1.645
    RMSEA_values <- c(rmsea, rmsea.se)
    
    # extracting CFI
    CFI <- fit_measures["cfi"] 
    
    # extracting SRMR
    SRMR <- fit_measures["srmr"] 
    
    # extracting exact fit test measures
    fit_chisq <- fit_measures["chisq"]
    fit_df <- fit_measures["df"]
    fit_pvalue <- fit_measures["pvalue"]
    fit_results <- c(fit_chisq, fit_df, fit_pvalue)
    
    
  }, error = function(e) {
    RMSEA_values <- c(NA, NA)
    CFI <- NA
    SRMR <- NA
    fit_results <- c(NA, NA, NA)
  })

  
  
  # Test 1b: conducting parallel test to check if one factor solution is best.
  parallel_N_factors <- NA
  tryCatch({
      parallel_N_factors <- suppressWarnings(fa.parallel(data[-1], fa = "fa", plot = FALSE))$nfact

    }, error = function(e) {
      parallel_N_factors <- NA
  })
  
  # adding a single factor check count
  if(is.null(RMSEA_values[[1]]) | is.null(parallel_N_factors)){
    unidimensional_check_count <- NA
  } else{
    unidimensional_check_count <- sum(RMSEA_values[[1]] < .08, CFI > .90, SRMR < .08, fit_results[[3]] < .05, parallel_N_factors == 1)
  }
  
  
  ### Test 2: Test for Tau equivalence
  Tau_results <- c(NA, NA, NA)
  tryCatch({
    # creating the base function for the fixed model (it is unidimensional)
    model_tau_restrict <- "Factor =~ " 
  
    for (item_name in names(data)[-1]){
      # adding all of the items from the scale to the model formula
      # estimated with the same coefficient a to emulate a Tau equivalent model
      model_tau_restrict <- paste0(model_tau_restrict, "a*", item_name, " + ")
    }
    
    # trimming the excess " + "
    model_tau_restrict <- substring(model_tau_restrict, 1, nchar(model_tau_restrict) - 3)
    
    # fitting the tau restricted model
    fit.modeltaurestrict <- cfa(model_tau_restrict, 
                                     data = data, 
                                     std.lv = TRUE, 
                                     estimator = "MLM")
    
    #summary(fit.modelfree, fit.measures = TRUE)
    #summary(fit.modeltaurestrict, fit.measures = TRUE)
    
    # conducting the fit test between the free coefficient model and the tau
    # restricted model
    fit.test <- anova(fit.modelfree, fit.modeltaurestrict)
    
    # extract the relevant results
    Tau_results <- fit.test[2, c("Chisq diff", "Df diff", "Pr(>Chisq)")]
    }, error = function(e) {
      Tau_results <- c(NA, NA, NA)
  })
  
  ### Test 3: Assessment of uncorrelated errors
  error_cor_results <- rep(NA, 4)
  
  
  tryCatch({
    # copying the tau restricted model as the baseline.
    model_freed_errors <- model_tau_restrict
    fit.modelfreed <- fit.modeltaurestrict
    
    # create empty vector to put data into
    spec.all.vec <- rep(NA, 5)
    
    # looping through five instances of freeing the highest mod index error 
    # covariance parameter estimation. We loop 5 times rather than adding the top 5
    # highest mod index, because when one parameter is freed it will affect the extent 
    # to which other error covariances impact the fit since (part of) their additional 
    # explained variance may have already been captured in the parameter just freed.
    for (i in 1:5) {
      modindex_highest <- suppressWarnings(modindices(fit.modelfreed, sort = TRUE))[1,]
      
      # store the fully standardized estimated coefficient change for the highest
      # mod index freed parameter
      spec.all.vec[i] <- modindex_highest[[7]]
      
      # add the freeing of the parameter into the model code
      model_freed_errors <- paste(model_freed_errors, "\n", modindex_highest[[1]], modindex_highest[[2]], modindex_highest[[3]])
      
      # rerun the model
      fit.modelfreed  <- cfa(model_freed_errors, 
                             data = data, 
                             std.lv = TRUE, 
                             estimator = "MLM")
    }
  
    # perform a model comparison between the model with the top 5 error covariances
    # freely estimated and the model with Tau equivalence.
    fit.test2 <- anova(fit.modeltaurestrict, fit.modelfreed)
    
    error_cor_test_results <- fit.test2[2, c("Chisq diff", "Df diff", "Pr(>Chisq)")]
    
    # calculate the mean of the fully standardized estimated coefficient change for
    # all the highest mod index freed parameters
    spec.all.mean <- mean(spec.all.vec)
    
    # store all error related results
    error_cor_results <- unlist(c(spec.all.mean, error_cor_test_results))  
    
    }, error = function(e) {
      error_cor_results <- NA
  })
  
  ### storing all of the data
  FA_data <- c(RMSEA_values, CFI, SRMR, fit_results, parallel_N_factors, 
               unidimensional_check_count, Tau_results, error_cor_results)
  
  # returning the FA data
  return(FA_data)
}


extracted_data_list <- list(data_1.10_clean, data_1.11_clean, data_1.12.3.1_clean, data_1.12.3.2_clean, data_2.12.1_clean, data_2.12.2_clean, data_2.12.3_clean, data_2.15_clean, data_2.20_clean, data_2.23_clean, data_3.2.1.1_clean, data_3.2.1.2_clean, data_3.7.1_clean, data_3.7.2_clean, data_3.8.2_clean, data_5.1.1_clean, data_5.1.2_clean, data_5.7_clean, data_5.9.1_clean)




# list to store the data in
FA_list <- list(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
                NA, NA, NA, NA, NA, NA, NA, NA, NA)

# running the validity/alpha assumption checks for all of the extracted datasets
for (i in 1:length(extracted_data_list)){
  # applying the function across each individual lab
  FA_data <- tapply(extracted_data_list[[i]], extracted_data_list[[i]]$g,
                    validity_and_alpha_assumption_tests)

  # transforming the returned list to a dataframe
  FA_data_frame <- do.call(rbind.data.frame, FA_data)
  
  # Setting the column names
  names(FA_data_frame) <- c("RMSEA", "RMSEA_se", "CFI", "SRMR", "fit_chisq", "fit_df", 
                            "fit_pvalue", "N_factors", "unidimensional_check_count", "Tau_chisq_diff", 
                            "Tau_Df_diff", "Tau_p_diff", "mean_spec", "err_chisq_diff", 
                            "err_Df_diff", "err_p_diff")
  
  # append results to the overarching FA results list
  FA_list[[i]] <- FA_data_frame
}




# store all dataframes with the list of the assumption test data in 
# the analysis data folder
fa_Caruso_2012 <- FA_list[[1]]
saving_to_analysis_data(fa_Caruso_2012)
fa_Husnu_2010 <- FA_list[[2]]
saving_to_analysis_data(fa_Husnu_2010)
fa_Nosek_2002_Math <- FA_list[[3]]
saving_to_analysis_data(fa_Nosek_2002_Math)
fa_Nosek_2002_Art <- FA_list[[4]]
saving_to_analysis_data(fa_Nosek_2002_Art)
fa_Anderson_2012_SWL <- FA_list[[5]]
saving_to_analysis_data(fa_Anderson_2012_SWL)
fa_Anderson_2012_PA <- FA_list[[6]]
saving_to_analysis_data(fa_Anderson_2012_PA)
fa_Anderson_2012_NA <- FA_list[[7]]
saving_to_analysis_data(fa_Anderson_2012_NA)
fa_Giessner_2007 <- FA_list[[8]]
saving_to_analysis_data(fa_Giessner_2007)
fa_Norenzayan_2002 <- FA_list[[9]]
saving_to_analysis_data(fa_Norenzayan_2002)
fa_Zhong_2006 <- FA_list[[10]]
saving_to_analysis_data(fa_Zhong_2006)
fa_Monin_2001_most <- FA_list[[11]]
saving_to_analysis_data(fa_Monin_2001_most)
fa_Monin_2001_some <- FA_list[[12]]
saving_to_analysis_data(fa_Monin_2001_some)
fa_Cacioppo_1983_arg <- FA_list[[13]]
saving_to_analysis_data(fa_Cacioppo_1983_arg)
fa_Cacioppo_1983_nfc <- FA_list[[14]]
saving_to_analysis_data(fa_Cacioppo_1983_nfc)
fa_De_Fruyt_2000 <- FA_list[[15]]
saving_to_analysis_data(fa_De_Fruyt_2000)
fa_Albarracin_2008_verb <- FA_list[[16]]
saving_to_analysis_data(fa_Albarracin_2008_verb)
fa_Albarracin_2008_math <- FA_list[[17]]
saving_to_analysis_data(fa_Albarracin_2008_math)
fa_Shnabel_2008 <- FA_list[[18]]
saving_to_analysis_data(fa_Shnabel_2008)
fa_Vohs_2008 <- FA_list[[19]]
saving_to_analysis_data(fa_Vohs_2008)


rm(extracted_data_list)
```

# Results

## Measurement Reporting

### Reliability

Figure \@ref(fig:ReliabilityReportingFlowDiagram) depicts the flow of measures in relation to reliability reporting. First, it shows that almost half of the measures in both replication (N = `r sum(coded_data_replications$N_items == "1 item measure")`) and original research (N = `r sum(coded_data_original$N_items == "1 item measure")`) were single-item measures. Second, only `r apa_num(sum(coded_data_original$reliability_type != "Not Reported" & coded_data_original$reliability_type != ""), numerals = FALSE)` measures out of `r sum(coded_data_original$N_items == "multiple item measure")` (`r round((sum(coded_data_original$reliability_type != "Not Reported" & coded_data_original$reliability_type != "") / sum(coded_data_original$N_items == "multiple item measure")) * 100, 1)` %) multi-item scales in original and `r apa_num(sum(coded_data_replications$reliability_type != "Not Reported" & coded_data_replications$reliability_type != ""), numerals = FALSE)` out of `r sum(coded_data_replications$N_items == "multiple item measure")` (`r round((sum(coded_data_replications$reliability_type != "Not Reported" & coded_data_replications$reliability_type != "") / sum(coded_data_replications$N_items == "multiple item measure")) * 100, 1)` %) multi-item scales in replications were reported alongside a reliability indicator. @flakeConstructValidityValidity2022 observed a much larger degree of reliability coefficient reporting in both original (60.8%) and replications (37.1%). However, similarly to @flakeConstructValidityValidity2022, we observed that reliability reporting was more common in original studies as compared to replication studies, and that Cronbach’s Alpha was the most commonly reported reliability indicator in our sample as well.

```{r ReliabilityReportingFlowDiagram, fig.cap = "Reliability reporting flow diagram. Figure shows the number of measures as reported in both the replication protocols and original article, which meet the criterion in the box within the diagram and those criteria before it.", out.height = "60%"}
knitr::include_graphics(path = "../../SupplementaryMaterials/reliability_reporting_flow_diagram.png")
```

### Validity

For validity evidence the pattern was similar. Only, `r apa_num(sum(coded_data_original$sel_psychometric_evidence_REV != "None" & coded_data_original$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)" | coded_data_original$sel_3_REV == TRUE, na.rm = TRUE), numerals = FALSE)` (`r round((sum(coded_data_original$sel_psychometric_evidence_REV != "None" & coded_data_original$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)" | coded_data_original$sel_3_REV == TRUE, na.rm = TRUE) / nrow(coded_data_original)) * 100, 1)` %) original studies and `r apa_num(sum(coded_data_replications$sel_psychometric_evidence_REV != "None" & coded_data_replications$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)" | coded_data_replications$sel_3_REV == TRUE, na.rm = TRUE), numerals = FALSE)` (`r round((sum(coded_data_replications$sel_psychometric_evidence_REV != "None" & coded_data_replications$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)" | coded_data_replications$sel_3_REV == TRUE, na.rm = TRUE) / nrow(coded_data_replications)) * 100, 1)`) replications reported psychometric validity evidence on their measurement. These proportions align with the results from @flakeConstructValidityValidity2022 for both original (9.3%) and replication studies (6.2%), and again, similar to @flakeConstructValidityValidity2022 we observe that validity evidence is also more commonly reported in original compared to replication studies. Exploratory factor analyses (original: `r apa_num(sum(coded_data_original$sel_psychometric_evidence_REV == "Exploratory Factor Analysis"), numerals = FALSE)`, replications: `r apa_num(sum(coded_data_replications$sel_psychometric_evidence_REV == "Exploratory Factor Analysis"))`) and convergent validity evidence (original: `r apa_num(sum(coded_data_original$sel_psychometric_evidence_text_REV == "convergent validity"), numerals = FALSE)`, replications: `r apa_num(sum(coded_data_replications$sel_psychometric_evidence_text_REV == "convergent validity"), numerals = FALSE)`) were the most commonly reported validity indicators. Psychometric validity evidence from previous studies was reported in `r apa_num(sum(coded_data_original$sel_4 == TRUE, na.rm = TRUE), numerals = FALSE)` original studies and `r apa_num(sum(coded_data_replications$sel_4 == TRUE, na.rm = TRUE), numerals = FALSE)` replications. For single-item measures, `r apa_num(sum(coded_data_replications$sel_psychometric_evidence_text_REV[coded_data_replications$N_items == "1 item measure"] != ""), numerals = FALSE)` were reported with validity evidence – specifically, all were convergent validity – in the replication protocols. In the original studies, `r apa_num(sum(coded_data_original$sel_psychometric_evidence_text_REV[coded_data_original$N_items == "1 item measure"] != ""), numerals = FALSE)` single-item measures came with reported validity evidence.

```{r validity_evidence_reporting, include = FALSE, eval = FALSE}
table(coded_data_original$sel_psychometric_evidence_REV)
table(coded_data_replications$sel_psychometric_evidence_REV)

# how much psychometric validity evidence was reported
sum(coded_data_original$sel_psychometric_evidence_REV != "None" & coded_data_original$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)" | coded_data_original$sel_3_REV == TRUE, na.rm = TRUE)
sum(coded_data_replications$sel_psychometric_evidence_REV != "None" & coded_data_replications$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)" | coded_data_replications$sel_3_REV == TRUE, na.rm = TRUE)

table(coded_data_original$sel_psychometric_evidence_text_REV)
table(coded_data_replications$sel_psychometric_evidence_text_REV)


# internal
table(coded_data_original$sel_3_REV)
table(coded_data_replications$sel_3_REV)
# external
table(coded_data_original$sel_4)
table(coded_data_replications$sel_4)

sum(coded_data_original$sel_4 == TRUE, na.rm = TRUE)
sum(coded_data_replications$sel_4 == TRUE, na.rm = TRUE)


# how many existing measures were used
sum(coded_data_original$sel_existing == "True, namely:")
sum(coded_data_replications$sel_existing == "True, namely:")

# for existing measures, how many had the specific version reported
sum(coded_data_original$op_1_REV[coded_data_original$sel_existing == "True, namely:"] == TRUE, na.rm = TRUE)
sum(coded_data_replications$op_1_REV[coded_data_replications$sel_existing == "True, namely:"] == TRUE , na.rm = TRUE)


# for measures how much exploratory factor analysis validity evidence was reported
sum(coded_data_original$sel_psychometric_evidence_REV == "Exploratory Factor Analysis")
sum(coded_data_replications$sel_psychometric_evidence_REV == "Exploratory Factor Analysis")


# for measures how much convergent validity evidence was reported
sum(coded_data_original$sel_psychometric_evidence_text_REV == "convergent validity")
sum(coded_data_replications$sel_psychometric_evidence_text_REV == "convergent validity")


# for single item measures was any validity evidence reported
apa_num(sum(coded_data_replications$sel_psychometric_evidence_text_REV[coded_data_replications$N_items == "1 item measure"] != ""), numerals = FALSE)

apa_num(sum(coded_data_original$sel_psychometric_evidence_text_REV[coded_data_original$N_items == "1 item measure"] != ""), numerals = FALSE)


# if an existing measure was used, how did that affect the reporting of reliability
# and validity evidence
table(coded_data_original$sel_existing, coded_data_original$reliability_type) # reliability does appear to indicate more reporting for existing reliabilities

table(coded_data_original$sel_existing, coded_data_original$sel_psychometric_evidence_REV) # not much to go on for psychometric validity evidence

# overall not substantive enough data too highlight in the article (would require a test to evaluate)
```

### Questionable Measurement Practices

We also coded the reporting of measurement information besides reliability and validity reporting. Table \@ref(tab:QMPTable) below shows the ratio of measurements that aligned with that row's practice of reporting a specific piece of measurement information and for how many of the `r nrow(coded_data_original)` measures the reporting of that measurement information was applicable for both original and replications. The ratio is based on the number of measures that reported that piece of measurement information for the measures for which that item was applicable. A higher ratio thus indicates better measurement reporting.

```{r QMPTable}
# create informative Measurement Practice Labels 
MPractice <- c("Reliability is reported", "It is clear if the measure existed or is newly created",    "Exact version of the measure is specified", "Validity evidence from a factor analysis is presented",   "Measured variable is defined", "Choice of measure selection or creation is justified",    "Implemented operationalisation is justified", "Source of the measure is provided",    "Psychometric evidence from the study is given", "Psychometric evidence from an earlier study is given",    "Administration format and environment are described", "Administration procedure is described",   "N items are described", "N response options are described", "Recoding of responses is described", "Creation of the index is described", "Text or supplement has example items", "Administration format changes are mentioned",    "Administration format changes are justified/validated", "Translations are mentioned",    "Translated measures are justified/validated", "Changes in N items or response options are mentioned",    "Changes in N items or response options are justified/validated")   


# construct the QMP table to be printed 
QMP_table <- data.frame("Practice" = MPractice, 
                        "Original" = paste0(1 - qmp_ratio_data$QMP_percentage_original, " (", qmp_ratio_data$N_applicable_original, ")"), 
                        "Replication" = paste0(1 - qmp_ratio_data$QMP_percentage_replication, " (", qmp_ratio_data$N_applicable_replication, ")"))   

QMP_table_formatted <- rbind(rbind(rbind(rbind(rbind(c("Definition", "", ""), QMP_table[5,]),
                                               c("Operationalisation", "", ""), QMP_table[c(3, 11, 12, 7, 17),]),
                                         c("Selection/Creation", "", ""), QMP_table[c(2, 6, 8, 9, 10, 1, 4),]),
                                   c("Quantification", "", ""), QMP_table[c(13, 14, 15, 16),]),
                             c("Modification", "", ""), QMP_table[18:23,])



# print the table in apa formatting 
apa_table(QMP_table_formatted, align = c("l", "r", "r"), 
          caption = "Ratio of QMPs and number of applicable measures, for original and replication", 
          note = "The first number in both Original and Replication is the ratio of measures for which that reporting practice contained sufficient information. The number between brackets indicates the number of measures for which the practice was applicable.", escape = FALSE, placement = "htp", booktabs = TRUE)
```

For our purposes, we can focus on the reporting of information most relevant for reconstructing the measure in future research, as well as the modification reporting practices to learn about the relation between original and replication studies. It was clear that existing measures were used for `r round(sum(coded_data_original$sel_existing == "True, namely:") / nrow(coded_data_replications) * 100, 1)`% original and `r round(sum(coded_data_replications$sel_existing == "True, namely:") / nrow(coded_data_original) * 100, 1)`% replication measures. It was not always clear which measures were used (`r round(sum(coded_data_original$sel_existing == "Not Clearly Stated") / nrow(coded_data_original) * 100, 1)`% original; `r round(sum(coded_data_replications$sel_existing == "Not Clearly Stated") / nrow(coded_data_replications) * 100, 1)`% replication), meaning they could not clearly be identified to be the same between original and replication . The number of items (`r round((1 - qmp_ratio_data$QMP_percentage_original[13]) * 100, 1)`% original (n = `r qmp_ratio_data$N_applicable_original[13]`); `r round((1 - qmp_ratio_data$QMP_percentage_replication[13]) * 100, 1)`% replications (n = `r qmp_ratio_data$N_applicable_replication[13]`)) and response options (`r round((1 - qmp_ratio_data$QMP_percentage_original[14]) * 100, 1)`% original (n = `r qmp_ratio_data$N_applicable_original[14]`); `r round((1 - qmp_ratio_data$QMP_percentage_replication[14]) * 100, 1)`% replications (n = `r qmp_ratio_data$N_applicable_replication[14]`)) were usually clear, and modifications to these being unclearly reported was atypical, but did occur (`r round((1 - qmp_ratio_data$QMP_percentage_original[22]) * 100, 1)`% original (n = `r qmp_ratio_data$N_applicable_original[22]`); `r round((1 - qmp_ratio_data$QMP_percentage_replication[22]) * 100, 1)`% replications (n = `r qmp_ratio_data$N_applicable_replication[22]`)). The percentage of numbers of items reported was similar to @flakeConstructValidityValidity2022 (original: 92%; replication 85.6%). The proportion to which the number of items were changed from original to replication was also similar (`r round(sum(!is.na(coded_data_replications$mod_5_REV)) / nrow(coded_data_replications) * 100, 1)`) to @flakeConstructValidityValidity2022 (16%). In general, modifications could only rarely be identified for original studies. In replication studies we observe that not all modifications were validated or justified (language: `r round((1 - qmp_ratio_data$QMP_percentage_replication[21]) * 100, 1)`% (n = `r qmp_ratio_data$N_applicable_replication[21]`); administration format: `r round((1 - qmp_ratio_data$QMP_percentage_replication[19]) * 100, 1)`% (n = `r qmp_ratio_data$N_applicable_replication[19]`)), especially for changes in items and response options (`round((1 - qmp_ratio_data$QMP_percentage_replication[23]) * 100, 1)`(n = `r qmp_ratio_data$N_applicable_replication[23]`)). In general, we observed that measurement was modified in some way from original to replication for `r round((sum(coded_data_replications$mod_check == "True") / nrow(coded_data_replications)) * 100, 1)`% of measures, and it was unclear whether modification occurred for `r round((sum(coded_data_replications$mod_check == "None Reported") / nrow(coded_data_replications)) * 100, 1)`%.

???LEFTOVER RESULTS from @flakeConstructValidityValidity2022. - had a citation flake:original ::: 29% - response format reported flake:original :: 94% - scale scoring flake:original :: 49% - citing existing validity evidence flake:original :: 29% - citing existing validity evidence flake:replication :: 38% - translation validated flake:replication :: 20% ???

## Measurement Response Data Analysis

### Calculated Reliability Coefficients

The average calculated Cronbach’s alpha coefficient across individual replications was `r apa_num(mean(calculated_reliability_lab_data$alpha))` with a standard deviation of `r apa_num(sd(calculated_reliability_lab_data$alpha))`. Figure \@ref(fig:PlotAlphaDistributions) displays the distributions of the calculated Cronbach’s Alpha scores from each lab for each measure, separated by successful and unsuccessful replication, based on the meta-analytic p-value (alpha \< .05) retrieved from the Many Labs reports.

```{r alpha_distributions_data_prep}
### Preparation code for the alpha distribution figure
# creating an editable copy of the calculated_reliability_lab_data
calculated_alpha_plot_data <- calculated_reliability_lab_data

# calculating the average alpha per group
calculated_alpha_plot_data$avg.alpha <- ave(calculated_reliability_lab_data$alpha, 
                                            calculated_reliability_lab_data$g)

# making replication success a Boolean, so we can order the dataset by it.
calculated_alpha_plot_data$replication_success <- ifelse(
  calculated_alpha_plot_data$replication_success == "Yes", TRUE, FALSE)

# reordering the plot from succesfully replicated to unsuccesfully replicated, and then from least to most reliable.
calculated_alpha_plot_data <- calculated_alpha_plot_data[order(-calculated_alpha_plot_data$replication_success, -calculated_alpha_plot_data$avg.alpha),]

# Removing calculated alpha's that fell below 0 to make everythign fit nicely in the plot.
calculated_alpha_plot_data <- calculated_alpha_plot_data[calculated_alpha_plot_data$alpha > 0,]

# reordering the grouping variable factor levels
calculated_alpha_plot_data$g <- fct_inorder(as.factor(calculated_alpha_plot_data$g), ordered = NA)


# adding an index to which row in the plot data the row in measure_reliability_data belongs to
measure_reliability_data$graph_index <- NA

for (i in 1:nrow(measure_reliability_data)){
  measure_reliability_data$graph_index[which(unique(calculated_alpha_plot_data$g)[i] == measure_reliability_data$g)] <- i
}

```

```{r PlotAlphaDistributions, warning = FALSE,  fig.cap = "Distributions of calculated Cronbach’s alpha coefficients calculated for the responses on a measure at each lab location, across the nineteen measures for which raw data was available from which Cronbach’s alpha coefficients could be calculated. Cronbach's alpha values that fell below 0 were excluded. The green lines indicate the meta-analytic prediction interval lower and upper bound. The blue triangles indicate the reported alpha coefficient for that measure from the original article, when reported. The Tau column besides the figure shows the tau heterogeneity estimate based on a meta-analysis of the calculated reliabilities for each measure. Meta-analyses for which the Q-test for heterogeneity was significant at alpha < .05 are in black, while non-significant results are in grey. The Diff column shows the difference between reported reliability and the average reliability calculated from the Many Labs data for the applicable measures. The reported reliabilities that fell outside the 95% quantile of calculated reliability scores are shown in black, otherwise in grey."}

# plot for distribution of alpha
ggplot(calculated_alpha_plot_data, aes(x = alpha, y = g)) +
  geom_boxplot(outlier.shape = NA) +
  geom_hline(yintercept = 6.5, color = "red", size = 1) +
  geom_point(alpha = 0.1) +
  
  # adding in the tau values
  geom_text(data = measure_reliability_data, label = format(measure_reliability_data$tau, digits = 1), x = 1.15, y = measure_reliability_data$graph_index, color = ifelse(measure_reliability_data$QEp < .05, "black", "grey"), size = 2.8) +
  
  # adding in the difference in alpha coefficients
  geom_text(data = measure_reliability_data[c(1:8, 18),], label = format(measure_reliability_data$coefficient_difference[c(1:8, 18)], digits = 2), x = 1.28, y = measure_reliability_data[!is.na(measure_reliability_data$coefficient_difference),]$graph_index, color = ifelse(measure_reliability_data$significance_reported_coefficient[c(1:8, 18)], "black", "grey"), size = 2.8) +
  
  # setting the theme
  theme_minimal() +
  theme(legend.position = "none", plot.margin = unit(c(1, 6.5, 1, 1), "lines")) +
  
  # adding the necessary indicative texts
  annotation_custom(grob = textGrob(label = "Not Replicated", hjust = 0, gp = gpar(fontsize = 10)), ymin = 7.25, ymax = 7.25, xmin = 0.01, xmax = 0.01) +
  annotation_custom(grob = textGrob(label = "Replicated", hjust = 0, gp = gpar(fontsize = 10)), ymin = 6, ymax = 6, xmin = 0.01, xmax = 0.01) +
  annotation_custom(grob = textGrob(label = "Tau", hjust = 0, gp = gpar(fontsize = 12)), ymin = 20.2, ymax = 20.2, xmin = 1.1, xmax = 1.1) +
  annotation_custom(grob = textGrob(label = "Diff", hjust = 0, gp = gpar(fontsize = 12)), ymin = 20.2, ymax = 20.2, xmin = 1.24, xmax = 1.24) +
  
  coord_cartesian(xlim = c(0, 1), clip = "off") +
  
  
  # adding the blue triangles for reported reliability and green prediction intervals
  geom_point(data = measure_reliability_data, mapping = aes(x = reported_coefficient, y = graph_index), color = "blue", shape = 17, size = 3) +
  # we remove 14 & 15 lower bound because they are below 0
  geom_point(data = measure_reliability_data[-c(14, 15),], mapping = aes(x = pi.lb, y = graph_index), color = "green", shape = 124, size = 2.5) + 
  geom_point(data = measure_reliability_data, mapping = aes(x = pi.ub, y = graph_index), color = "green", shape = 124, size = 2.5) + 
  
  ylab("") +
  xlab("Cronbach's alpha") 

```

We found statistically significant indication of heterogeneity in Cronbach's alpha for `r apa_num(sum(measure_reliability_data$QEp < .05), numerals = FALSE)` of the `r apa_num(nrow(measure_reliability_data), numerals = FALSE)` measures. However, as noted earlier a more accurate indication of the heterogeneity can be observed in the prediction intervals. The prediction intervals generally show larger indications of heterogeneity for some of the lower reliability measures and less for higher reliability measures when compared to the Tau test results. An increase in the number of labs also appears to be a factor for narrowing the prediction intervals. Additionally, we observe that for most measures the reported reliabilities were generally lower than the reliabilities calculated for the replication labs. Another interesting observation is that reported reliabilities in the original study were more common for those measures that had higher average calculated reliabilities (\>.80) in the replications compared to on average less reliable measures.

### Unidimensionality assessment

```{r PlotData1Factor, warning = FALSE}
# load in the FA_list data
FA_list <- list(fa_caruso_2012, fa_husnu_2010, fa_nosek_2002_math, fa_nosek_2002_art,
                fa_anderson_2012_swl, fa_anderson_2012_pa, fa_anderson_2012_na,
                fa_giessner_2007, fa_norenzayan_2002, fa_zhong_2006, fa_monin_2001_most,
                fa_monin_2001_some, fa_cacioppo_1983_arg, fa_cacioppo_1983_nfc, 
                fa_de_fruyt_2000, fa_albarracin_2008_verb, fa_albarracin_2008_math,
                fa_shnabel_2008, fa_vohs_2008)



# create the base dataframe
unidimensionality_graph_data <- data.frame(RMSEA = NA,
                                           RMSEA_se = NA,
                                           CFI = NA,
                                           SRMR = NA,
                                           fit_chisq = NA,
                                           fit_df = NA,
                                           fit_pvalue = NA,
                                           N_factors = NA,
                                           unidimensional_check_count = NA,
                                           g = NA)

measured_variables <- c("caruso_2012", "husnu_2010", "nosek_2002_math", "nosek_2002_art", "anderson_2012_swl", "anderson_2012_pa", "anderson_2012_na", "giessner_2007", "norenzayan_2002", "zhong_2006", "monin_2001_most", "monin_2001_some", "cacioppo_1983_arg", "cacioppo_1983_nfc", "de_fruyt_2000", "albarracin_2008_verb", "albarracin_2008_math", "shnabel_2008", "vohs_2008")


# adding the info from the FA_list to our data.
for (i in 1:19){

  g = measured_variables[i]
  
  unidimensionality_graph_data <- rbind(unidimensionality_graph_data, cbind(FA_list[[i]][1:9], g))

}

# removing placeholder first row
unidimensionality_graph_data <- unidimensionality_graph_data[-1, ]

# ordering the levels of g to match the order in the reliability graph
unidimensionality_graph_data$g <- factor(unidimensionality_graph_data$g, levels = c("nosek_2002_art", "nosek_2002_math", "shnabel_2008", "husnu_2010", "norenzayan_2002", "vohs_2008", "cacioppo_1983_arg", "anderson_2012_pa", "anderson_2012_na", "giessner_2007", "anderson_2012_swl", "caruso_2012", "zhong_2006", "monin_2001_some", "cacioppo_1983_nfc", "monin_2001_most", "albarracin_2008_verb", "albarracin_2008_math", "de_fruyt_2000"), labels = c("Nosek et al. (2002), Art", "Nosek et al. (2002), Math", "Shnabel & Nadler (2008)", "Husnu & Crisp (2010)", "Norenzayan et al. (2002)", "Vohs & Schooler (2008)", "Cacioppo et al. (1983), arg", "Anderson et al. (2012), PA", "Anderson et al. (2012), NA", "Giessner & Schubert, (2007)", "Anderson et al. (2012), SWL", "Caruso et al. (2012)", "Zhong & Lijenquist (2006)", "Monin & Miller (2001), some", "Cacioppo et al. (1983), nfc", "Monin & Miller (2001), most",  "Albarracín et al. (2008), exp 5 verb", "Albarracín et al. (2008), exp 5 math", "De Fruyt et al. (2000)"))

# adding a capped of version of the N_factors variable as a category variable
N_factors_capped <- rep(NA, nrow(unidimensionality_graph_data))

N_factors_capped[unidimensionality_graph_data$N_factors == 1] <- "1"
N_factors_capped[unidimensionality_graph_data$N_factors == 2] <- "2"
N_factors_capped[unidimensionality_graph_data$N_factors >= 3] <- "3+"

unidimensionality_graph_data$N_factors_capped <- as.factor(N_factors_capped)


### meta-analyzing RMSEA
assess_undidimensionality_heterogeneity <- function(data_on_RMSEA){
  temp_beta <- NA
  temp_tau <- NA
  temp_QEp <- NA
  rma_prediction <- NA
  temp_pi.lb <- NA
  temp_pi.ub <- NA
  
  tryCatch({
    # run a random effects meta-analysis
    rma_model <- rma(yi = data_on_RMSEA$RMSEA, sei = data_on_RMSEA$RMSEA_se, 
                     method = "REML", control = list(stepadj = 0.5, maxiter = 1000))
    # estimated coefficient
    temp_beta <- rma_model$beta
    
    # extract the relevant heterogeneity information
    temp_tau <- sqrt(rma_model$tau2)
    temp_QEp <- rma_model$QEp
    
    # get prediction intervals for alpha
    rma_prediction <- predict(rma_model)
    temp_pi.lb <- rma_prediction$pi.lb
    temp_pi.ub <- rma_prediction$pi.ub

    
  }, error = function(e) {
    
  })
  
  
  return(c(temp_beta, temp_tau, temp_QEp, temp_pi.lb, temp_pi.ub, data_on_RMSEA$g[1]))
}


unidimensionality_meta_list <- tapply(unidimensionality_graph_data, unidimensionality_graph_data$g, assess_undidimensionality_heterogeneity)

# convert data output to a dataframe
unidimensionality_meta_data <- do.call(rbind.data.frame, unidimensionality_meta_list)

names(unidimensionality_meta_data) <- c("beta", "tau", "QEp", "pi.lb", "pi.ub", "g")

# add the count of significant p_values for the exact test to the meta_dataframe

count_significant_results <- function(p){
  significance <- sum(p > .05, na.rm = TRUE)
  number_of_converged <- sum(!is.na(p))
  return(c(significance, number_of_converged))
}

p_meta_list <- tapply(unidimensionality_graph_data$fit_pvalue, unidimensionality_graph_data$g, count_significant_results)

p_meta_data <- do.call(rbind.data.frame, p_meta_list)

names(p_meta_data) <- c("p_count", "converged_count")

unidimensionality_meta_data <- cbind(unidimensionality_meta_data, p_meta_data)


# added the reporting or lack of reporting of psychometric validty evidence in the original to the meta data
unidimensionality_meta_data$reported_psych_val <- coded_data_original$sel_psychometric_evidence_REV[measure_reliability_data$coded_data_index] != "None" & coded_data_original$sel_psychometric_evidence_REV[measure_reliability_data$coded_data_index] != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)" | coded_data_original$sel_3_REV[measure_reliability_data$coded_data_index] == TRUE




# adding the number of non-coverged factor models
non_converged_list <- tapply(unidimensionality_graph_data$RMSEA, unidimensionality_graph_data$g, is.na)


non_converged_vec <- rep(0, 19)

for (i in 1:19){
  non_converged_vec[i] <- sum(non_converged_list[[i]])
}

# creating a string that shows the ratio of nonconverged to total
convergence_rate_vec <- paste0(unidimensionality_meta_data$converged_count, "/", unidimensionality_meta_data$converged_count + non_converged_vec)

# adding a star for De Fruyt et al. (2000)
convergence_rate_vec <- c(convergence_rate_vec[1:18], paste0(convergence_rate_vec[19], "*"))


```

Figure \@ref(fig:Plot1FactorRMSEA) below show the result of our unidimensionality test. `r apa_num((sum(unidimensionality_graph_data$RMSEA < .08, na.rm = TRUE) / nrow(unidimensionality_graph_data)) * 100)` % of labs were below our RMSEA .08 threshold, while for `r apa_num((sum(unidimensionality_meta_data$beta < .08, na.rm = TRUE)), numerals = FALSE)` measures of the total `r apa_num(nrow(unidimensionality_meta_data), numerals = FALSE)` the mean across labs was below this threshold. Furthermore, the RMSEA shows considerable variation within measures, with the standard deviation ranging from `r apa_num(min(tapply(unidimensionality_graph_data$RMSEA, unidimensionality_graph_data$g, sd, na.rm = TRUE)[!is.na(tapply(unidimensionality_graph_data$RMSEA, unidimensionality_graph_data$g, sd, na.rm = TRUE))]))` to `r apa_num(max(tapply(unidimensionality_graph_data$RMSEA, unidimensionality_graph_data$g, sd, na.rm = TRUE)[!is.na(tapply(unidimensionality_graph_data$RMSEA, unidimensionality_graph_data$g, sd, na.rm = TRUE))]))` (mean sd = `r apa_num(mean(tapply(unidimensionality_graph_data$RMSEA, unidimensionality_graph_data$g, sd, na.rm = TRUE)[!is.na(tapply(unidimensionality_graph_data$RMSEA, unidimensionality_graph_data$g, sd, na.rm = TRUE))]))`).

The parallel analysis results similarly showed inconsistency both across and within measures. While `r apa_num((sum(unidimensionality_graph_data$N_factors_capped == "1", na.rm = TRUE) / nrow(unidimensionality_graph_data)) * 100)`% of labs returned a single factor solution, `r apa_num((sum(unidimensionality_graph_data$N_factors_capped == "2", na.rm = TRUE) / nrow(unidimensionality_graph_data)) * 100)`% returned a two-factor, and `r apa_num((sum(unidimensionality_graph_data$N_factors_capped == "2", na.rm = TRUE) / nrow(unidimensionality_graph_data)) * 100)`% returned three or more factors. The remainder did not converge. The lack of convergence was also an issue for the CFA used for the RMSEA check. The CFA failed to converge for `r apa_num((sum(is.na(unidimensionality_graph_data$RMSEA)) / nrow(unidimensionality_graph_data)) * 100)`% of labs. Convergence in most cases was the result of either the severe misfit of our factor models or a lack of sample size to stably estimate these models. A consistent pattern was also lacking for the results from the exact fit tests. For `r sum((unidimensionality_meta_data$p_count[1:18] / unidimensionality_meta_data$converged_count[1:18]) >= .75)` out of `r sum(unidimensionality_meta_data$converged_count != 0)` measures with converged factor analyses, three quarters or more labs showed statistical significant fit, for `r sum((unidimensionality_meta_data$p_count[1:18] / unidimensionality_meta_data$converged_count[1:18]) <= .25)` a quarter or less did, and for the remaining `r sum((unidimensionality_meta_data$p_count[1:18] / unidimensionality_meta_data$converged_count[1:18]) > .25 & (unidimensionality_meta_data$p_count[1:18] / unidimensionality_meta_data$converged_count[1:18]) < .75)` the proportion lay somewhere in between.

```{r Plot1FactorRMSEA, warning = FALSE, fig.cap = "Distributions of calculated RMSEA of a single-factor model fit calculated for the responses on a measure at each lab location, across the nineteen measures for which raw data was available on which a factor model could be fitted. The red horizontal line separate the replicated from the non-repicated measures. The red vertical line indicates our .08 RMSEA cutoff value. The first number in the Conv. column besides the graph shows the number of labs per measure for which the single-factor CFA converged, with second number showing the total number of labs. The color of each dot shows the number of factors that were selected for that measure for that lab location based on the parallel analyses. Psychometric validity evidence was reported in the original Husnu & Crips (2010) and Shnabel & Nadler (2008) studies. * The Conscientiousness measure used in De Fruyt et al. (2000) consisted of two items, which are too few items to fit a factor model on. Therefore, none of these models converged."}

# plot for distribution of RMSEA and exact fit test
ggplot(unidimensionality_graph_data, aes(x = RMSEA, y = g, colour = as.factor(N_factors_capped))) +
  geom_boxplot(outlier.shape = NA, colour = "black") +
  geom_hline(yintercept = 6.5, color = "red", size = 1) +
  geom_vline(xintercept = 0.08, color = "red", size = 1) +
  geom_point(alpha = 0.3) +
  
  
  # adding in the convergence rate
  geom_text(data = unidimensionality_meta_data, label = format(convergence_rate_vec), x = 0.51, y = 1:19, size = 2.8, colour = "black") +
  
  # adding in the proportion of significant p-values
  geom_text(data = unidimensionality_meta_data, label = format(c(round(unidimensionality_meta_data$p_count[1:18] / unidimensionality_meta_data$converged_count[1:18], 2), "")), x = 0.58, y = 1:19, size = 2.8, colour = "black") +
  
  
  # setting the theme
  theme_minimal() +
  theme(legend.position = "bottom", plot.margin = unit(c(2, 5, 1, 1), "lines")) +
  
  
  # adding the necessary indicative texts
  annotation_custom(grob = textGrob(label = "Not Replicated", hjust = 0, gp = gpar(fontsize = 10)), ymin = 7.25, ymax = 7.25, xmin = 0.33, xmax = 0.33) +
  annotation_custom(grob = textGrob(label = "Replicated", hjust = 0, gp = gpar(fontsize = 10)), ymin = 6, ymax = 6, xmin = 0.37, xmax = 0.37) +
  annotation_custom(grob = textGrob(label = "Conv.", hjust = 0, gp = gpar(fontsize = 11)), ymin = 20.2, ymax = 20.2, xmin = 0.48, xmax = 0.48) +
  annotation_custom(grob = textGrob(label = "Sig.\nProp.", hjust = 0, gp = gpar(fontsize = 11)), ymin = 20.9, ymax = 20.9, xmin = 0.55, xmax = 0.56) +
  
  coord_cartesian(xlim = c(0, 0.45), clip = "off") +
  
  # we remove 7 & 15 lower bound because they are below 0
  # geom_point(data = unidimensionality_meta_data[c(1:6, 8:14, 16:17),], mapping = aes(x = pi.lb, y = c(1:6, 8:13, 15:17) ), color = "green", shape = 124, size = 2.5) + 
  # geom_point(data = unidimensionality_meta_data, mapping = aes(x = pi.ub, y = 1:19), color = "green", shape = 124, size = 2.5) + 
  
  scale_colour_manual(name = "N Factors (Parallel Analysis)", values = c("#0049b8", "#02bd8a", "#f2e30f"), na.value = "#000000") + 
  guides(colour = guide_legend(override.aes = list(alpha = 1))) +
  ylab("") +
  xlab("RMSEA (unidimensional CFA model)") 


```

Figure \@ref(fig:Plot1FactorSRMRCFI) below shows the SRMR, and CFI fit indices for the same single factor model as was used to estimate the RMSEA and exact fit test. `r apa_num((sum(unidimensionality_graph_data$SRMR < .08, na.rm = TRUE) / nrow(unidimensionality_graph_data)) * 100)`% of labs were below our SRMR .08 threshold and `r apa_num((sum(unidimensionality_graph_data$CFI < .9, na.rm = TRUE) / nrow(unidimensionality_graph_data)) * 100)`% were above our CFI .90 threshold. For the `r apa_num(nrow(unidimensionality_meta_data), numerals = FALSE)` measures, the mean SRMR was below .08 for `r apa_num((sum(tapply(unidimensionality_graph_data$SRMR, unidimensionality_graph_data$g, mean, na.rm = TRUE) < .08, na.rm = TRUE)), numerals = FALSE)` measures, while the mean CFI was above .90 for for `r apa_num((sum(tapply(unidimensionality_graph_data$CFI, unidimensionality_graph_data$g, mean, na.rm = TRUE) > .9, na.rm = TRUE)), numerals = FALSE)` measures.The mean standard deviation of the SRMR fit index is `r apa_num(mean(tapply(unidimensionality_graph_data$SRMR, unidimensionality_graph_data$g, sd, na.rm = TRUE)[!is.na(tapply(unidimensionality_graph_data$SRMR, unidimensionality_graph_data$g, sd, na.rm = TRUE))]))`. For CFI this was `r apa_num(mean(tapply(unidimensionality_graph_data$CFI, unidimensionality_graph_data$g, sd, na.rm = TRUE)[!is.na(tapply(unidimensionality_graph_data$CFI, unidimensionality_graph_data$g, sd, na.rm = TRUE))]))`. Overall, compared to the RMSEA, the SRMR and CFI fit indices show a more stable unidimensional fit. Especially the SRMR index shows considerably better fit compared to the RMSEA. When combining the results from all five unidimensionality checks together we observe that at least one of the five unidimensionality checks was passed for `r round((sum(unidimensionality_graph_data$unidimensional_check_count >= 1, na.rm = TRUE) / sum(!is.na(unidimensionality_graph_data$unidimensional_check_count))) * 100, 2)`% of the `r sum(!is.na(unidimensionality_graph_data$unidimensional_check_count))` labs with converged factor models, at least two for `r round((sum(unidimensionality_graph_data$unidimensional_check_count >= 2, na.rm = TRUE) / sum(!is.na(unidimensionality_graph_data$unidimensional_check_count))) * 100, 2)`%, at least three for `r round((sum(unidimensionality_graph_data$unidimensional_check_count >= 3, na.rm = TRUE) / sum(!is.na(unidimensionality_graph_data$unidimensional_check_count))) * 100, 2)`%, at least three for `r round((sum(unidimensionality_graph_data$unidimensional_check_count >= 4, na.rm = TRUE) / sum(!is.na(unidimensionality_graph_data$unidimensional_check_count))) * 100, 2)`%, and all five for `r round((sum(unidimensionality_graph_data$unidimensional_check_count == 5, na.rm = TRUE) / sum(!is.na(unidimensionality_graph_data$unidimensional_check_count))) * 100, 2)`%. The mean number of unidimensionality check passed was `r round(mean(unidimensionality_graph_data$unidimensional_check_count, nar.rm = TRUE), 2)`.

```{r Plot1FactorSRMRCFI, warning = FALSE, fig.cap = "Distributions of calculated SRMR and calculated CFI of a single-factor model fit calculated for the responses on a measure at each lab location, across the nineteen measures for which raw data was available on which a factor model could be fitted. The left figure shows the SRMR result, where the red vertical line indicates our .08 RMSR cutoff value. The right figure shows the CFI results, where the red vertical line indicates our .90 CFI cutoff value. In both figures the red horizontal line separate the replicated from the non-repicated measures."}

# plot for distribution of SRMR
SRMR_plot <- ggplot(unidimensionality_graph_data, aes(x = SRMR, y = g, colour = as.factor(N_factors_capped))) +
  geom_boxplot(outlier.shape = NA, colour = "black") +
  geom_point(alpha = 0.3) +
  geom_hline(yintercept = 6.5, color = "red", size = 1) +
  geom_vline(xintercept = 0.08, color = "red", size = 1) +

  # setting the theme
  theme_minimal() +
  theme(legend.position = "bottom", plot.margin = unit(c(1, 5, 1, 1), "lines")) +
  scale_x_continuous(breaks = round(c(0, 0.1, 0.2),1)) +
  
  # adding the necessary indicative texts
  annotation_custom(grob = textGrob(label = "Not Replicated", hjust = 0, gp = gpar(fontsize = 8)), ymin = 7.25, ymax = 7.25, xmin = 0.18, xmax = 0.18) +
  annotation_custom(grob = textGrob(label = "Replicated", hjust = 0, gp = gpar(fontsize = 8)), ymin = 6, ymax = 6, xmin = 0.18, xmax = 0.18) +

  coord_cartesian(xlim = c(0, 0.2), clip = "off") +

  scale_colour_manual(name = "N Factors (Parallel Analysis)", values = c("#0049b8", "#02bd8a", "#f2e30f"), na.value = "#000000") + 
  guides(colour = guide_legend(override.aes = list(alpha = 1))) +
  ylab("") +
  xlab("SRMR (unidimensional CFA model)") 


# plot for distribution of CFI
CFI_plot <- ggplot(unidimensionality_graph_data, aes(x = CFI, y = g, colour = as.factor(N_factors_capped))) +
  geom_boxplot(outlier.shape = NA, colour = "black") +
  geom_hline(yintercept = 6.5, color = "red", size = 1) +
  geom_vline(xintercept = 0.9, color = "red", size = 1) +
  geom_point(alpha = 0.3) +

  # setting the theme
  theme_minimal() +
  theme(legend.position = "none", plot.margin = unit(c(1, 5, 1, 1), "lines"), axis.text.y = element_blank()) +
  scale_x_continuous(breaks = round(c(0, 0.2, 0.4, 0.6, 0.8, 1),1)) +
  
  coord_cartesian(xlim = c(0, 1), clip = "off") +
  
  scale_colour_manual(name = "N Factors (Parallel Analysis)", values = c("#0049b8", "#02bd8a", "#f2e30f"), na.value = "#000000") + 
  guides(colour = guide_legend(override.aes = list(alpha = 1))) +
  ylab("") +
  xlab("CFI (unidimensional CFA model)") 


(SRMR_plot + CFI_plot)

```

???Compared to Shaw ,!!! - mean CFI of scales is: `r  mean(0.919, 0.966, 0.877, 0.946, 0.616, 0.92, 0.889, 0.873)` 0.919 - mean SRMR of scales is: `r  mean(0.066, 0.028, 0.047, 0.045, 0.267, 0.04, 0.059, 0.036)` 0.066 - mean of RMSEA of scales is: `r  mean(0.056, 0.082, 0.093, 0.059, 0.262, 0.191, 0.174, 0.085)` 0.056. However these are means of scales and not per lab???

# Discussion

To assess psychological phenomena we need measurement that is valid and reliable for our constructs. However, we noted that little reliability and validity evidence was reported, and that all the information needed to understand and recreate the measurement was rarely all available. Furthermore our reanalysis of the reliability and validity of the measures showed inconsistent reliability and validity support.

!!!Korte samenvatting van wat we gedaan hebben; daarna voor alle drie aims de resultaten bescrhijven; gebruik QMPs als framework, maar niet veel expliciet bescrhijven (future research, er zijn nog meer QMPs); blijf de volgorde aanhouden!!!

In line with existing research we found that for the majority of the measures in original and replication studies no reliability coefficient was reported, and psychometric validity evidence was even less common [@beckmanHowReliableAre2004; @barryValidityReliabilityReporting2014; @flakeConstructValidationSocial2017; @maireadshawMeasurementPracticesLargescale2020; @flakeConstructValidityValidity2022]. Some part of this is due to around half of the measures in original and replication studies being single item measures. While it is possible to check the reliability and validity of single item measures, these methods often require multiple measures or studies to be able to cross-validate the measure with [@leppinkWeNeedMore2017; @sarstedtSelectingSingleItems2016]. Another reason can be found in our observation that replications generally reported less reliability and psychometric validity evidence. This could be because replications implicitly defer the responsibility of reporting validity and reliability evidence to the original study. However, as we noted before, and as our analyses of the reliability and unidimensionality reiterate, the reliability and validity of a measure varies across contexts. It is therefore still important for the reliability and validity to be described for the replications studies too.

We also noted that about two-thirds of the measurements were modified between replication and original study. While minor differences between replication and original study are inevitable, substantial changes between original and replication study raises questions about labeling those replications as direct. For example, the conscientiousness measure used for the replication of @defruytCloningersPsychobiologicalModel2000 was reduced from the original NEO-PI-R 30-item measure to a two-item measure in the replication. It is highly unlikely that a measure with such a significant change would have a comparable validity and reliability to the original, and we did unsurprisingly observe convergence issues with validity as well as low reliability for this measure. The regularity at which measures were modified also provides a response to the argument that replications do not need to report as many measurement details, since the reporting of these . If measurement is modified, then the new use and validity of the measurement has to be documented by the replication. Additionally, if changing the measurement between original and replication study is fair game, then the choice of measure has to also be substantially motivated beyond: "the original study did it that way". Finally, none of this explains away the lack of reliability and validity reporting in the original studies. It may unfortunately be more likely that many of these measures are simply not validated before (or even after) use.

The information needed to reconstruct a measure for reuse was also not consistently clearly reported. In particular, like @flakeConstructValidityValidity2022, we observed that original research underreported information needed to reconstruct the measurement In future research. The administration format and procedure for each was not properly described for around a third of the applicable measures. The number of items and response options were unclear for both in around an eight of the applicable measures, while how the index was calculated, how the items were recoded and example items were presented for each a little under half of the applicable measures.

One notable difference compared to @flakeConstructValidityValidity2022 is that in our results replications did on average have less QMPs than original research. We believe that this is in part due to the structured way in which the Many Labs protocols were written up. Several Many Labs protocols even contained specific sections to declare any deviations from the original methodology. The other reason may be due to the more lenient coding of the QMPs in the revised protocol. We mostly checked that something relevant to a specific QMP item was reported, not whether that reporting thoroughly shared all the needed information. In particular, for our items checking whether a particular practice was justified or validated, we accepted any reason that was given, we did not check whether that reason was sufficient. For example, even if we did not code QMPs in validating modifications to measurement, this does not mean that Measurement Invariance was empirically established, even if it might have been important to do so.

Inconsistency was not only observed in the reporting of measurement. When we recalculated the reliability coefficients and factor structure of the measures used in the replication projects, the observed reliability and validity varied considerably across labs. We also noted that some measures had convergence issues, where for some labs we could not get the factor model to fit. This may have been because of sample size issues, or the small number of items we often observed in the replication measures. However, gross misfit is also possible. Although, the psychometric indicators we used were applied rather broadly across a spectrum of different measures, and should not be used to make individual inferences for the measures. As a general overview, our results do carry the warning that we cannot merely assume that the measures used in our sample were valid and can be used for substantive interpretations.

We also observed that when a Cronbach's Alpha was reported for a measure in an original study, then their average Cronbach's Alpha observed among the replications were consistently relatively high. Meanwhile when no Cronbach's Alpha was reported in the original study, the reliability was often relatively low. This aligns with existing research indicating bias in reported Cronbach's Alpha values, with an excess of reported values at the common acceptably reliable threshold value of .70, and low reporting just below .70 [@husseyAberrantAbundanceCronbachs2023]. Authors may be less inclined to report reliabilities when they fall below an acceptable threshold, but not otherwise. In combination with our inconsistent reliability and validity evidence, this bias in reporting could create a false sense of security when looking only at what is reported.

Overall, our intent is not to specifically scold the Many Labs replications nor the original studies. We commend the tremendous effort involved in the creation of the Many Labs projects, and we understand that for feasibility some corners had to be cut. Similarly, we understand that complete validation of psychological measures is not a responsibility that should or can be placed on any one study. However, we would argue that full transparent measurement reporting is a responsibility both replications and original studies do have. Based on our results and the results of existing research, we believe that with current measurement reporting, you cannot reliably expect to observe the information you need to evaluate and understand the measurement that was used in a study. Even though a substantial proportion of measurement information was frequently reported - when looking at our intentionally non-strict coding protocol - information was almost never consistently clearly reported across our sample. Furthermore, critical information on reliability and validity in particular was rare. Finally, even though validating measures is difficult, its necessity cannot be ignored. If validity evidence for a measure is never obtained, it becomes questionable to draw substantive interpretations on the basis of responses to that measure.

!!!detecting meaningful effects when checking for measurement invariance across groups requires large sample sizes [@meadePowerPrecisionConfirmatory2007; @frenchMeasurementInvarianceTechniques2016; @koziolImpactModelParameterization2018].!!!

## Limitations & Future Research

Our main indicator of construct validity obtained from the response data was our unidimensionality test from a factor analysis. However, our factor analyses were not informed by theory, and were not specific to each measure. Instead the structure of the model and the test was the same for all measures. It is quite possible that a measure fails at our checks and is still a valid and reliable measure in its specific use case. We use these checks to get an assessment of one element of construct validitty within our sample, not to make inferences for particular measures, nor to test construct validity completely. Future research may want to deep dive into this specific topic and focus on doing theory informed construct validity tests using openly available measurement reponse data.

One of the most impactful limitations to our study design was that we were unable to conduct informative inferential tests on our data. This was in large part because there were simply to few measures with the relevant information for our test. We initially planned to test the significance of the difference in reported reliability between original and replication studies, and the relation between reliability and replication outcome. But due to the small number of reported reliabilities and measures for which reliability could be calculated we were unable to perform these tests. However, this limitation is also one of our most important findings. While we were unable to perform our pre-registered tests, the fact that we couldn't, is telling with respect to the lack of consistent measurement information reporting in our sample.

!!!limited usefulness of the data!!!

As another limitation, it is important to reflect on whether or not replication protocols and research articles can be fairly compared in terms of Measurement Practices. A study’s description within a replication protocol is generally shorter than an article. The protocol may therefore lack the space needed to report on the measurement in full detail. There are three reasons why we believe protocols and articles remain mostly comparable. Firstly, articles are often also restricted in the amount of space they have available to devote to measurement [@gardinerEditorialMethodsPapers2019; @zogmaisterAssessingTransparencyMethods2024]. Second, in the revised protocol some QMP items, such as whether example items were reported or not, also allowed for reporting this information in supplementary materials to count as good practice. Third, there are little to no other files outside the protocol and supplements where the measurement details for the replication could be found, as such the protocol and supplements represent the total available information on the measurement just like the original article does. A difference that was not resolved between articles and protocols was that the protocols were written before the measurement was conducted. Therefore we can not fairly compare protocol and replication on measurement information that is derived from the data – such as reliability coefficients and psychometric validity information. Still, future research should look if possible at replications that report on measurement information derived from the replication data.

???move to the paragraph above, the section in the discussion that describes the argument that replications dont have to report what is presumed to be in the original together with the counter-argument that modificaitons are regular, and as such measurement choice is fair game and should be motivated + an extra sentence that often information is not available in the original so it can hardly be relied upon???

There were additional sources of information we could have included. Data from original studies could have been used to recalculate reliability and validity indicators from to compare with the replication results. However, it is unlikely that a substantial number would have shared their data (many were conducted before the OSF and other Open Science initiatives were launched). This is likely different for more recent research [@hardwickeEstimatingPrevalenceTransparency2022; @hamiltonPrevalencePredictorsData2023a]. Including replications of recent original research could enable more comparisons based on open data from the original studies. Future research may wish to look at replications of more recent research to compare recalculated measurement information between original and replication.

!!!niet representatieief, kodnen niet alles doen, maar voor vervolgonderzoek mogelijk!!!

## Recommendations

There are two main issues we believe that should be addressed in the literature to improve measurement practices: the common use of non-validated measurement, and the lack of transparency surrounding measurement reporting. Unfortunately, there is no easy fix to these issues. The widespread use of validated measurement requires measures for a large range of topics to be validated, a practice that currently occurs only rarely. Meanwhile improving transparency in reporting is an issue that many have tried to address in relation to other parts of , but universal success has remained elusive here ???CITATIONS???. However, the positives are first that these two issues are interrelated, because solving transparency will benefit greater validation of measures, and second that every instance of transparent reporting and validation helps reduce the current problem.

Our first reccomendation is that reporting standards for measurement deserve a more central place. We found that reporting reliability and validity information was the exception rather than the rule, and even basic information such as how many items were in the measure was reported with little consistency among our sample of measures. The @americaneducationalresearchassociationStandardsEducationalPsychological2014 guidelines exist to guide researchers in the creation and validation of scales, but are not tailored as guidelines for general measurement reporting. The APA guidelines [@AmericanPsychologicalAssociation2020] do address many of the same reporting practices we assessed, however our findings and those of other research shows that uptake of these standards is minimal. Meanwhile, statistical reporting standards and guidelines have had a stronger place in the field and among journals, including recommendations from the ASA [@wassersteinASAStatementPValues2016]. A high-profile push for measurement reporting standards is lacking. We believe that measurement reporting standards deserve to be highlighted as much as statistical result reporting. after all, even well reported statistics lack have to be understood within the context of the measurement.

Transparent measurement reporting also has the advantage that it enables future researchers to recreate the original measure. This point is critical to fueling our second recommendation: as a community we need to systematically evaluate and revise our measures. To do so, we first need measures to be reusable. Second, researchers need to reuse existing measures. Third, the obtained reliability and validity of the measure then need to be compared across multiple occasions, and finally a value judgement to determine in what contexts, if any, the measure is valid has to be made. Transparent reporting takes care of the first step. For the second step, @elsonPsychologicalMeasuresArent2023 have suggested journals to implement the Standardisation Of BEhavior Research (SOBER) guidelines to specifically address issues of flexibility and norming in measurement so that measures are reused in a comparable state. Furthermore, they propose an open repository of measurement protocols to facilitate the discovery of measures and building an evidence base. For the third and fourth step, two things are needed. First, there should be a space in the scientific literature for persistent measurement evaluation, just like how a place for replication studies in the literature has been suggested to provide persistent evaluation of effects. In fact, if replications were to play closer attention to measurement evaluation and reporting, they can function as an excellent validation test of the measure as well. Second, the data obtained from the measurement should be available for a comprehensive validaity analyses of the measure within different contexts. For this it would be best if the original data of the item responses and demographics is shared for each use of the measure. This way the measure's validity and reliability can be recomputed similarly to what was done in our analyses, but with a closer link to theory. Otherwise, the summary statistics such as the variance-covariance structure, means, and standard deviations should be shared as these are sufficient for many basic factor analysis models.

Furthermore, researchers should report and evaluate their measures using more informative indicators. Cronbach's Alpha on its own gives only limited information the quality of a scale, and comes with strong assumptions [@cortinaWhatCoefficientAlpha1993; @sijtsmaUseMisuseVery2009]. Factor analytical evidence and McDonald's Omega are more informative indicators that should be presented in favor of or in tandem with Cronbach's Alpha.

Finally, we argue that researchers seeking to replicate a study should first evaluate the measurement of that study. Not only is it crucial for an informative replication that the measurement is reliable or valid. The original study should also report the measurement details necessary to reconstruct the original measurement. Otherwise it will be challenging to know if measurement was different between original and replication. If so, we suggest that researchers instead use their resources to conduct a replication of a study with reliable, valid and well-documented measurement. When replicating another study is not an option, we advise the replicating researcher to first attempt a conceptual replication using a validated measurement. Afterwards, a direct replication can be performed based on the conceptual replication to further assess the robustness.

# Conclusion

Cumulative knowledge on psychological phenomena starts with our ability to accurately capture the relevant constructs of interest. Based on the results from our sample and the findings of existing research, the validity and reliability of psychological measurements is rarely reported and based on our data cannot be assumed to be generally correct. The lack of consistent transparency in measurement reporting further complicates evaluating and reusing the existing measures. A change will be necessary. Otherwise, the basic premise that the measurement measures the variable it is intended to measure, will remain unsubstantiated for many psychological studies. Fortunately, even small steps towards improved reporting practices and single instances in cumulative measurement validation can already help to proliferate validated measurement.

## Conflicts of Interest

The author(s) declare that there were no conflicts of interest with respect to the authorship or the publication of this article.

## ORCID iDs

Cas Goos <https://orcid.org/0009-0005-3792-4148>

Marjan Bakker <https://orcid.org/0000-0001-9024-337X>

Jelte M. Wicherts <https://orcid.org/0000-0003-2415-2933>

Michèle B. Nuijten <https://orcid.org/0000-0002-1468-8585>

## Funding

The preparation of this article was supported by the Veni grant VI.Veni.201G.003 awarded to Michèle Nuijten, and the Vici grant VI.C.221.100 awarded to Jelte M. Wicherts from the Dutch Research Council (NWO).

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
