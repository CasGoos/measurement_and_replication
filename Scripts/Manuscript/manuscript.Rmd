---
title             : "Measurement Reliability, Construct Validity, and Transparent Reporting in Original and Replication Psychological Research"
shorttitle        : "Measurement Reliability, Validity, and Reporting"

author: 
  - name          : Cas Goos
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Professor Cobbenhagenlaan 125, 5037 DB, Tilburg, The Netherlands"
    email         : "c.goos@tilburguniversity.edu"

  - name          : Marjan Bakker
    affiliation   : "1"

  - name          : Jelte M. Wicherts
    affiliation   : "1"

  - name          : Michèle B. Nuijten
    affiliation   : "1"


authornote: |
  The authors made the following contributions. CG: Conceptualization, Data curation, Formal Analysis, Investigation, Methodology, Project Administration, Software, Visualization, Writing - Original Draft Preparation, Writing - Review & Editing; MB: Conceptualization, Supervision, Writing - Review & Editing; JW: Conceptualization, Supervision, Writing - Review & Editing; MN: Conceptualization, Project Administration, Supervision, Validation, Writing - Review & Editing.


abstract: |
  While published (replication) studies using measures to assess psychological phenomena need to transparently report information on measurement procedures, validity, and reliability to allow verification and use in future (replication) research, earlier results highlighted common poor reporting of psychological measurement. Here, we investigated the measurement reporting in a sample of 77 measures within 56 Many Labs replications and related original articles (14–17) and found that the information relevant for reusing measures was reported in full in around half the replication measures, and only 5.2% in the original studies. We also observed that only around a third of multiple-item measures in original studies and 11.4% in replications reported reliability coefficients, with comparable proportions for reporting any convergent, discrimant, predictive, or factorial validity evidence. We performed checks of the reliability and unidimensionality of multiple-item measures using the openly available Many Labs item response data. We observed that the reliability and unidimensionality of the measures was rarely stable across labs. These results corroborate existing findings that measurement reporting in published research lacks transparency, and that poor measurement reporting often obscure insufficient reliability and validity. We offer suggestions on how to improve measurement reporting practices and increase the use of validated measures.


keywords          : "reliability, construct validity, measurement, reporting, transparency, replication, reproducibility"
wordcount         : "198"

bibliography      : ["r-references.bib", "references.bib"]

floatsintext      : yes
linenumbers       : yes
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

header-includes:
  - | 
    \makeatletter
    \renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-1em}%
      {\normalfont\normalsize\bfseries\typesectitle}}
    
    \renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-\z@\relax}%
      {\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
    \makeatother
  - \renewcommand\author[1]{}
  - \renewcommand\affiliation[1]{}
  - \authorsnames[1, 1, 1, 1]{Cas Goos, Marjan Bakker, Jelte M. Wicherts, Michèle B. Nuijten}
  - \authorsaffiliations{{Department of Methodology and Statistics, Tilburg School of Social and Behavioral Sciences, Tilburg University, Tilburg, NL.}}

csl               : "vancouver.csl"
documentclass     : "apa7"

classoption       : man
output            : papaja::apa6_pdf
knit              : worcs::cite_all
---

```{r setup, include = FALSE}
# loading R libraries
library(papaja)
library(worcs)
library(lavaan)
library(psych)
library(metafor)
library(forcats)
library(ggplot2)
library(grid)
library(patchwork)

# Code below loads all data. The raw data was loaded within the 'prepare_data.R script.
load_data()

# the code below removes the raw data to preserve disk space.
# If you want to rerun the data cleaning and preparations steps, DO NOT run
# this code.
# If you want to only rerun the analyses, YOU CAN run the code below.
rm(coded_data_initial_raw, coded_data_revised_raw, coded_data_vignette_raw,
   data_2.10.1, data_2.12, data_2.15, data_2.19.1, data_2.2, data_2.20, 
   data_2.23, data_2.3, data_2.4.1, data_2.4.2, data_2.8.2, data_3.5, data_5.1,
   data_5.4, data_5.5, data_5.7, data_5.9.1, data_ml1, data_ml3)

# creates a reference list for all used R packages and the installed R version 
# (does not include Rstudio)
r_refs("r-references.bib")
```

<!-- altering latex defaults to get better figure and table placement -->

\renewcommand{\arraystretch}{0.7}

<!-- reducing the line spacing within tables -->

\renewcommand{\topfraction}{.8}

<!-- max fraction of page for floats at top -->

\renewcommand{\bottomfraction}{.8}

<!-- max fraction of page for floats at bottom -->

\renewcommand{\textfraction}{.15}

<!-- min fraction of page for text -->

\renewcommand{\floatpagefraction}{.8}

<!-- min fraction of page that should have floats .66 -->

\setcounter{topnumber}{3} <!-- max number of floats at top of page -->

\setcounter{bottomnumber}{3} <!-- max number of floats at bottom of page -->

\setcounter{totalnumber}{4} <!-- max number of floats on a page -->

<!-- remember to use [htp] or [htpb] for placement -->

```{r analysis-preferences}
# Seed for random number generation
set.seed(26052025)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

```{r helper_functions}
# Function for storing data openly in the intermediate data folder
saving_to_intermediate_data <- function(data){
  open_data(data = data, filename = paste0(paste0(
    "Data/IntermediateData/", deparse(substitute(data))), ".csv"),
    codebook = NULL, value_labels = NULL) 
}

# Function for storing data openly in the analysis data folder
saving_to_analysis_data <- function(data){
  open_data(data = data, filename = paste0(paste0(
    "Data/AnalysisData/", deparse(substitute(data))), ".csv"),
    codebook = NULL, value_labels = NULL) 
}
```

<!-- Introduction -->

Because psychological constructs cannot be directly measured, quantitative psychological researchers use scores from psychological measures to study psychological phenomena. Psychological measurement is no easy feat and so measures will not always form good indicators of the constructs they purport to measure. Measures need to be validated and associated with sufficient reliability to deal with systematic and unsystematic measurement errors to attribute observed variation in scores to the variance in the unobserved construct(s) of interest. Psychometrics offers a wide range of tools to assess reliability and to study validity, ranging from classic Cronbach’s Alpha to asses internal consistencies to item response theory and structural equation models allowing for fitting models that explicate the notion that supposed latent variables relate to observed items scores in the expected manner In practice, however, many psychological measures exhibit subpar (or unclear) reliabilities and are typically validated and psychometrically tested only in limited circumstances, samples, and measurement procedures, while it is likely that they do not behave psychometrically well in all relevant samples, locations, and time points [@maassenDireDisregardMeasurement2025]. 

Moreover, many measures used in psychological research lack reliability and construct validity evidence altogether, either because the evidence is not reported in studies using them, or because the measures are not validated to begin with [@beckmanHowReliableAre2004; @barryValidityReliabilityReporting2014; @flakeConstructValidationSocial2017; @maireadshawMeasurementPracticesLargescale2020; @flakeConstructValidityValidity2022]. Such practices are rightly @flakeMeasurementSchmeasurementQuestionable2020 dubbed  “Questionable Measurement Practices” (QMPs), being a term analogous to Questionable Research Practices (QRPs; [@simmonsFalsePositivePsychologyUndisclosed2011a; @johnMeasuringPrevalenceQuestionable2012a; @wichertsDegreesFreedomPlanning2016a]). A recent line of research has already shown QMPs to be relatively common in both original and replication psychology studies, damaging the trustworthiness and applicability of psychological research findings [@flakeConstructValidityValidity2022; @husseyAberrantAbundanceCronbachs2023].

The purpose of this study is to assess the measurement reporting practices of item-based measures in a diverse and seminal sample of original and replication studies in psychology. In addition, we will use the available item response data to calculate reliability indices for multiple-item measures and perform a series of unidimensionality checks --- as a prerequisite check of construct validity. By assessing the measurement information as reported together with reliability and unidimensionality using the primary data, we reflect on the apparent validity of the measurement based on the reported information, compared to our validity assessment based on the item response data.

## Measurement Reporting

One issue that has been observed with respect to measurement validation is that the reported measurement information is typically insufficient for readers to understand, evaluate, and reuse the measurement that was used. Flake et al. @flakeConstructValidityValidity2022 documented the measurement reporting practices among 100 original psychology articles and their respective replications from the Reproducibility Project Psychology [@opensciencecollaborationEstimatingReproducibilityPsychological2015]. They coded the number of measures, the content of the measures, and information describing and justifying the measurement. The reporting of reliability evidence was assessed through whether Cronbach’s Alpha coefficients were reported. For validity evidence, they looked at citations to existing validity evidence from previous studies documenting the scale’s development or use, and results from a factor analysis providing structural evidence for the measure. They observed limited reporting of both reliability and validity evidence. Additionally, only 8 of the 40 translated scales referenced an existing validity study that described evidence for the translated version. A similar lack of reliability and validity reporting has been observed in other studies as well [@hoganEmpiricalStudyReporting2004; @flakeConstructValidationSocial2017]. If the reported information does not reference any of the references to reliability or validity evidence, then readers lack the basic information needed to make an assessment of whether the measured variables relate to the constructs of interest. In turn, if the reported information does not exhibit some minimal level of validity --- through reliability coefficients, an existing validation study, or factor analysis results --- then any substantive conclusions drawn from the data are left unsupported.

Further findings by Flake et al. @flakeConstructValidityValidity2022 and others [@flakeConstructValidationSocial2017; @maireadshawMeasurementPracticesLargescale2020] show that issues in measurement reporting go beyond a lack of reporting reliability and validity evidence. Basic content descriptions such as the number of items, the response format, and the scoring of the scale are not always clearly reported. This creates challenges for future researchers who want to reuse the measure to assess the same construct. Specifically, replication studies attempting to reconstruct the measurement from these incomplete descriptions may end up with a measurement that assesses the constructs in a substantially different way from the original study, or even assess different constructs altogether. If different constructs are assessed, the replication cannot be seen as a test of the same phenomenon as in the original study. If the same constructs are assessed, but in a substantially different way, the estimated effects in the replication cannot be easily compared to the effects in the original study. In either case, substantial comparisons between original and replication are hindered.

Our first aim is to extend the existing findings on measurement reporting practices with a descriptive account of these practices for the Many Labs replications and the related original studies. We will evaluate to what extent the reporting of item-based scales in our sample are transparent enough to facilitate the evaluation and reuse of these measurements. The Many Labs replications are a series of large-scale collaborations, in which multiple labs across the world directly replicated classic and contemporary psychological studies [@kleinInvestigatingVariationReplicability2014a; @ebersoleManyLabs32016; @kleinManyLabs22018b; @ebersoleManyLabs52020d; @kleinManyLabs42022]. The original studies chosen for replication in the Many Labs were not only picked based on feasibility, but importantly for us the sample of effects to replicate was chosen to contain a diverse range of seminal effects. Furthermore, because the Many Labs projects used preregistered and documented structured protocols, we expected the measurement reporting for the replications to represent a high standard within the field. Any issues in measurement reporting here might suggest that other replications could face similar or greater challenges.

## Reliability

Two key aspects of measurement quality are reliability and construct validity. These can be studied using various psychometric tools  [@nunnallyOverviewPsychologicalMeasurement1978; @mellenberghConceptualIntroductionPsychometrics2011]. The results of such psychometric analyses often serve as essential evidence for evaluating a measure’s validity.

Reliability serves as a pre-requisite for a valid measure in most cases, because a measure for which a participant’s response is not consistent, cannot capture a stable construct [@KaplanPsychologicalTestingPrinciples2013; p. 154–5]. It is therefore concerning that the reporting of psychometric indicators of reliability is often substandard  [@beckmanHowReliableAre2004; @barryValidityReliabilityReporting2014; @flakeConstructValidationSocial2017; @flakeConstructValidityValidity2022], Moreover, reliability reporting often limits itself to Cronbach’s Alpha, which comes with strong assumptions that are rarely tested [@crutzenScaleQualityAlpha2017b; @sijtsmaUseMisuseVery2009]. Furthermore, research by Hussey et al. @husseyAberrantAbundanceCronbachs2023 has shown evidence that the reporting of reliability may not only be uncommon, but also biased. They observed a disproportional number of Cronbach’s Alpha values at the common acceptably reliability threshold value of .70, and relatively low reporting below .70. Thus, the lack of reported measurement evidence may indicate psychometric skeletons hiding in the closet.

As a result, the full evaluation of reliabilities is hindered by the possibly biased reporting in our sample of (replication) studies. Therefore, we also computed the measurement reliability within the Many Labs original and replication studies by estimating it from the raw item data. The data on the item responses are openly available per lab for our sample. Therefore, we can evaluate not only the reliability per measure, but also the variation in reliability across labs. The variation is relevant because Cronbach’s Alpha is an indicator of reliability within a particular sample and not of the reliability of the measure in general, as it is proportional to the total variance in the target variable in the sample. Still, many researchers report and interpret Cronbach’s Alpha as a universal quality of the measure [@cortinaWhatCoefficientAlpha1993; @schmittUsesAbusesCoefficient1996]. Mairead Shaw et al. @maireadshawMeasurementPracticesLargescale2020 have already observed that for Many Labs 2 the overall sample Cronbach’s Alpha level across scales was below .5, a degree of reliability far below what many would consider acceptable for most research purposes. Our second research aim is therefore to extend these findings by empirically evaluating both the degree as well as the variation of reliability of the measures in Many Labs projects 1, 2, 3, and 5.

## Construct Validity and Unidimensionality

Besides reliability, construct validity is key to establish for any measure. A multi-stage process is required to statistically and substantially establish a link between the measurement and the construct of interest. For example, the way a measure’s items relate to each other and to the underlying concept (the factor relations) can differ substantially across contexts, a concept known as measurement invariance [@hornPracticalTheoreticalGuide1992; @cheungDirectComparisonApproach2012]. Thus, we cannot assume that the measure is valid in each context it is used in or meant to be used in, it is something that has to be established for each context. Besides investigating the factor structure, the measure should also be backed up substantively and logically [@borgstedeMeaningfulMeasurementRequires2023]. However, what we typically observe in psychological research is that measures are rarely substantiated with validity evidence [@beckmanHowReliableAre2004; @barryValidityReliabilityReporting2014; @flakeConstructValidationSocial2017; @flakeConstructValidityValidity2022], and that many measures are made on the fly and reused maybe once or not at all [@weidmanJingleJangleEmotion2017; @elsonPsychologicalMeasuresArent2023; @anvariFragmentedFieldConstruct2025; @anvariDefragmentingPsychology2025]. It is not entirely surprising that most psychological measures are not fully validated, as this requires its own line of research spanning several years, to determine the relation between measure and construct(s), and how this varies across contexts [@meadePowerPrecisionConfirmatory2007; @frenchMeasurementInvarianceTechniques2016; @koziolImpactModelParameterization2018], and over time [@marshlongitudinalstabilitylatent1994], among other aspects. 

However, there are prerequisite checks of construct validity that can be conducted by researchers in most scenarios [@gorsuchFactorAnalysis2013]. One such check is to see if the number of constructs measured by the items of the measurement match the intended number of constructs. Often the scores on a measure or subscales of a measure are aggregated and used as a singular index when included as a variable in a statistical model. In that case, the measure should be unidimensional. Otherwise, the researcher is interpreting a set of scores to be one concept when in fact they are multiple distinct concepts. Additionally, most reliability indicators --- including Cronbach’s Alpha --- cannot properly estimate true variance when it is spread across distinct constructs, and will result in an inaccurate estimate of reliability [@schmittUsesAbusesCoefficient1996; @crutzenScaleQualityAlpha2017b]. Thus, unidimensionality checks offer researchers a practical wat to test if the construct validity of their measure is bound to sink or has the potential to swim.

Mairead Shaw et al. @maireadshawMeasurementPracticesLargescale2020 checked the dimensionality of the measurements from Many Labs 2 [@kleinManyLabs22018b] and while some support for unidimensionality was present, they found that none of the scales in their sample met all of their criteria for unidimensionality. Because the Many Labs replications reused existing measures across contexts, Mairead shaw et al @maireadshawMeasurementPracticesLargescale2020 could assess validity across contexts. Our third research aim is to extend these findings. We will perform our own checks of unidimensionality on measures from Many Labs 1, 2, 3, and 5 per lab, and include a meta-analytic assessment of the variation in unidimensionality across labs. The combined goal of our three research aims is to provide a descriptive account of the reliability, unidimensionality, and reporting of measurement in the included Many Labs study pairs.

# Disclosures

## Preregistration

We preregistered data collection, coding protocol, and planned analyses: <https://osf.io/jgxyu>. We deviated from our coding protocol as is explained further below. The results from our preregistered analyses are described in [Supplementary Analyses A](../../SupplementaryMaterials/SupplementaryAnalysesScripts/Supplementary_exploratory_version_pre-reg_analyses.Rmd). In the main text, we focus on the descriptive results.

## Data, Materials, and Online Resources

This manuscript was created in RStudio [*v`r rstudioapi::versionInfo()$version`*; @R-Rstudio] with R Version `r paste0(R.Version()$major, ".", R.Version()$minor)` [@R-base], and generated using the Workflow for Open Reproducible Code in Science [*v`r getNamespaceVersion("worcs")[[1]]`*; @R-worcs] to ensure reproducibility and transparency. All code and data used to generate this manuscript and its results are available at: <https://github.com/CasGoos/measurement_and_replication> and <https://osf.io/9r8yt/> (DOI: 10.17605/OSF.IO/9R8YT).

## Reporting

We report how we determined all data exclusions, all manipulations, and all measures in the study. Our sample size was predetermined by the number of studies in the Many Labs projects.

## Ethics Approval

This research was approved by the Tilburg University School of Social and Behavioral Sciences Ethics Review Board (under TSB_TP_REMA06).

# Method

## Data Source
The data consists of three main sources: replication datasets, replication protocols, and original study articles. 
We retrieved the data on the preregistered replication protocols, and replication datasets from Many Labs 1, 2, 3, & 5 [@kleinInvestigatingVariationReplicability2014a; @ebersoleManyLabs32016; @kleinManyLabs22018b; @ebersoleManyLabs52020d] from their respective OSF pages: <https://osf.io/wx7ck/>, <https://osf.io/8cd4r/>, <https://osf.io/ct89g/>, & <https://osf.io/7a6rd/>. Many Labs 4 [@kleinManyLabs42022] was excluded, as there were no publicly available replication protocols. Additionally, we excluded the replication of Crosby et al. [@crosbyWhereWeLook2008a] in Many Labs 5, as it made use of videos and eye-tracking measures, which did not match this study’s focus on item-based measures. We skimmed both the replication protocols and replication datasets to ensure that they were the correct files to code measurement reporting information from. However, no coding or analysis of either of them had taken place before the analyses were preregistered. Further details on the search strategy can be found in the [coding protocol information file](../../SupplementaryMaterials/CodingProtocols/coding_protocol_information.Rmd) in the supplementary materials.

### Replication Datasets

The replication datasets refer to the publicly available datasets containing the data obtained from all labs of each Many Labs replication. For the analyses, we extracted the scores on the items of each previously identified measure that also met our inclusion criteria specified in the paragraph below. When scores could not be clearly identified based on the information available in the dataset and the replication protocol, any available codebooks, analysis scripts, or study materials were used to identify the relevant scores.

To be included as part of the subset of measures we analyzed in our reliability recalculations and factor analyses, the measure had to be a scale of multiple items. If cleaned data were available, we chose these over raw data, to ensure that variables were coded as intended (e.g., no reverse-coded items). We omitted pilot data from the analyses. These criteria combined with our inability to definitively determine which variables in the dataset corresponded to the items on the measure of interest resulted in a set of item score data from `r length(unique(calculated_reliability_lab_data$g))` replication sets spread across on average approximately `r round(mean(table(calculated_reliability_lab_data$g)), 0)` lab locations for our analyses.

```{r CleaningReplicationDatasetsData, include = FALSE, eval = FALSE}
##### This code can be used to rerun the data preparation to convert the raw
##### (input) data of the Many Labs replications into intermediate data.
##### However, because the intermediate data is also available in this project,
##### the manuscript can also be reproduced without running this code block.

### Below we extract the relevant data from the Many Labs' datasets. The first
### number in each dataset refers to the Many Labs project it is related to.
### The second number to which study the data is from in order of appearance in
### the Many Labs pre-registered protocols, or in the case of Many Labs 5, 
### within the OSF folder structure. If there is a third number, the study had 
### multiple relevant measures this refers to the order of appearance of the 
### measure where the data is from within the study's description in the Many 
### Labs protocol or OSF folder structure. A fourth number indicates which part
### of the data of this measure was taken, in case the measure assessed multiple
### constructs, as these were treated separately for some analyses.

## ML 1
# 1.3
data_1.3_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[22:29])
colnames(data_1.3_clean)[1] <- "g"
# 1.10
data_1.10_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[108:115])
colnames(data_1.10_clean)[1] <- "g"
# 1.11
data_1.11_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[73:76])
colnames(data_1.11_clean)[1] <- "g"
# 1.12.1
# not found
# 1.12.3
data_1.12.3.1_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[54:59])
colnames(data_1.12.3.1_clean)[1] <- "g"
data_1.12.3.2_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[60:65])
colnames(data_1.12.3.2_clean)[1] <- "g"

## ML 2
# 2.2
data_2.2_clean <- cbind(as.factor(data_2.2[[5]]), data_2.2[6:11])
colnames(data_2.2_clean)[1] <- "g"
# 2.3
# data does not appear suitable
# 2.4.1
data_2.4.1_clean <- cbind(as.factor(data_2.4.1[[5]]), data_2.4.1[6:11])
colnames(data_2.4.1_clean)[1] <- "g"
# 2.4.2
data_2.4.2_clean <- cbind(as.factor(data_2.4.2[[5]]), data_2.4.2[6:14])
colnames(data_2.4.2_clean)[1] <- "g"
# 2.8.2
data_2.8.2_clean <- cbind(as.factor(data_2.8.2[[6]]), data_2.8.2[9:13])
colnames(data_2.8.2_clean)[1] <- "g"
# 2.10.1
data_2.10.1_clean <- cbind(as.factor(data_2.10.1[[5]]), data_2.10.1[6:11])
colnames(data_2.10.1_clean)[1] <- "g"
# 2.12.1
data_2.12.1_clean <- cbind(as.factor(data_2.12[[5]]), data_2.12[c(6,7,8,9,10,31,32,33,34,35)]) 
data_2.12.1_clean[3465:6905,2:6] <- data_2.12.1_clean[3465:6905,7:11]
data_2.12.1_clean <- data_2.12.1_clean[1:6]
colnames(data_2.12.1_clean)[1] <- "g"
# 2.12.2
data_2.12.2_clean <- cbind(as.factor(data_2.12[[5]]), data_2.12[c(11,14,15,18,19,22,24,27,28,29,36,39,40,43,44,47,49,52,53,54)]) 
data_2.12.2_clean[3465:6905,2:11] <- data_2.12.2_clean[3465:6905,12:21]
data_2.12.2_clean <- data_2.12.2_clean[1:11]
colnames(data_2.12.2_clean)[1] <- "g"
# 2.12.3
data_2.12.3_clean <- cbind(as.factor(data_2.12[[5]]), data_2.12[c(12,13,16,17,20,21,23,25,26,30,37,38,41,42,45,46,48,50,51,55)]) 
data_2.12.3_clean[3465:6905,2:11] <- data_2.12.3_clean[3465:6905,12:21]
data_2.12.3_clean <- data_2.12.3_clean[1:11]
colnames(data_2.12.3_clean)[1] <- "g"
# 2.15
data_2.15_clean <- cbind(as.factor(data_2.15[[5]]), data_2.15[8:12])
colnames(data_2.15_clean)[1] <- "g"
# 2.19.1
# difficult to extract
# 2.19.2
# difficult to extract
# 2.20
data_2.20_clean <- cbind(as.factor(data_2.20[[5]]), data_2.20[6:45]) 
data_2.20_clean[3729:7396,2:21] <- data_2.20_clean[3729:7396,22:41]
data_2.20_clean <- data_2.20_clean[1:21] 
# coding so all 1's means somebody used rule-based grouping strategy
data_2.20_clean[,c(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)] <- ifelse(data_2.20_clean[,c(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)] == 1, 1, 0)
data_2.20_clean[,c(3, 5, 7, 9, 11, 13, 15, 17, 19, 21)] <- ifelse(data_2.20_clean[,c(3, 5, 7, 9, 11, 13, 15, 17, 19, 21)] == 2, 1, 0)
colnames(data_2.20_clean)[1] <- "g"
# 2.23
data_2.23_clean <- cbind(as.factor(data_2.23[[5]]), data_2.23[c(7,8,12,13,15)])
colnames(data_2.23_clean)[1] <- "g"


## ML 3
# 3.2.1
data_3.2.1 <- cbind(as.factor(data_ml3[[1]]), data_ml3[77:86] - 1)
data_3.2.1.1_clean <- na.omit(data_3.2.1[1:6])
colnames(data_3.2.1.1_clean)[1] <- "g"
data_3.2.1.2_clean <- na.omit(data_3.2.1[c(1, 7:11)])
colnames(data_3.2.1.2_clean)[1] <- "g"
# 3.5
# data appears unusable
# 3.7.1
data_3.7.1_clean <- na.omit(cbind(as.factor(data_ml3[[1]]), data_ml3[38:42]))
colnames(data_3.7.1_clean)[1] <- "g"
# 3.7.2
data_3.7.2_clean <- na.omit(cbind(as.factor(data_ml3[[1]]), data_ml3[89:94]))
colnames(data_3.7.2_clean)[1] <- "g"
# 3.8.1
# a single measure was reported
# 3.8.2
data_3.8.2_clean <- na.omit(cbind(as.factor(data_ml3[[1]]), data_ml3[29:30])) 
colnames(data_3.8.2_clean)[1] <- "g"


## ML 5
# 5.1.1
data_5.1.1_clean <- cbind(as.factor(data_5.1[[2]]), data_5.1[13:27])
colnames(data_5.1.1_clean)[1] <- "g"
# 5.1.2
data_5.1.2_clean <- cbind(as.factor(data_5.1[[2]]), data_5.1[28:33])
colnames(data_5.1.2_clean)[1] <- "g"
# 5.4
data_5.4_clean <- cbind(as.factor(data_5.4[[1]]), data_5.4[18:41])
colnames(data_5.4_clean)[1] <- "g"
# 5.5.1 & 5.5.2
# from this dataset it appears that this data will be difficult to use.
# 5.5.2
# also difficult to use
# 5.7 
data_5.7_clean <- cbind(as.factor(data_5.7[[3]]), data_5.7[c(25, 34, 35, 36, 37, 38, 39, 40, 41, 42)])
colnames(data_5.7_clean)[1] <- "g"
# 5.9.1
data_5.9.1_clean <- na.omit(cbind(as.factor(data_5.9.1[[4]]), data_5.9.1[c(79, 83, 87, 91, 95, 98, 101)]))
colnames(data_5.9.1_clean)[1] <- "g"


### Saving to Intermediate Data Folder
saving_to_intermediate_data(data_1.3_clean)
saving_to_intermediate_data(data_1.10_clean)
saving_to_intermediate_data(data_1.11_clean)
saving_to_intermediate_data(data_1.12.3.1_clean)
saving_to_intermediate_data(data_1.12.3.2_clean)
saving_to_intermediate_data(data_2.10.1_clean)
saving_to_intermediate_data(data_2.12.1_clean)
saving_to_intermediate_data(data_2.12.2_clean)
saving_to_intermediate_data(data_2.12.3_clean)
saving_to_intermediate_data(data_2.15_clean)
saving_to_intermediate_data(data_2.20_clean)
saving_to_intermediate_data(data_2.23_clean)
saving_to_intermediate_data(data_3.2.1.1_clean)
saving_to_intermediate_data(data_3.2.1.2_clean)
saving_to_intermediate_data(data_3.7.1_clean)
saving_to_intermediate_data(data_3.7.2_clean)
saving_to_intermediate_data(data_3.8.2_clean)
saving_to_intermediate_data(data_5.1.1_clean)
saving_to_intermediate_data(data_5.1.2_clean)
saving_to_intermediate_data(data_5.4_clean)
saving_to_intermediate_data(data_5.7_clean)
saving_to_intermediate_data(data_5.9.1_clean)

```

### Replication Protocols & Original Articles

The replication protocols refer to the publicly available protocols describing the background, methodology, and analysis of the replication of an original study. These were retrieved from the Open Science Framework (OSF) pages of the Many Labs projects (the search strategy and OSF file locations can be found in the [data retrieval information](../../SupplementaryMaterials/data_retrieval_information.Rmd) supplementary document). We identified and retrieved all original study articles using the citations for these articles in each replication protocol.

```{r CleaningCodedData, include = FALSE, eval = FALSE}
##### This code can be used to rerun the data preparation to convert the raw 
##### (input) data of the coded data into intermediate data. However, because 
##### the intermediate data is also available in this project, the manuscript  
##### can also be reproduced without running this code block.

# Selecting the relevant rows and columns for the data
coded_data_initial_sel <- coded_data_initial_raw[3:160, 18:57]
coded_data_revised_sel <- coded_data_revised_raw[3:160, 18:38]
coded_data_vignette_sel <- coded_data_vignette_raw[3:160, 2]

# Combining the datasets
coded_data_full <- cbind(coded_data_initial_sel, 
                         cbind(coded_data_revised_sel, coded_data_vignette_sel))

# filtering out unnecessary double columns
coded_data_full <- cbind(coded_data_full[, 1:40], coded_data_full[, 45:62])


### data preparation
# renaming columns
colnames(coded_data_full) <- c("many_labs_version", "rep_org", "title", "measure_name", 
      "variable_name", "multi", "variable_order", "N", "N_items", 
      "hypothesis_support", "reliability_type", "reliability_type_text", 
      "reliability_coeff", "def_1", "op_version", "op_1", "op_2", "op_3", "op_4", 
      "op_5", "sel_existing", "sel_existing_text", "sel_1", "sel_2", "sel_3", 
      "sel_4", "sel_psychometric_evidence", "sel_psychometric_evidence_text", 
      "quant_1", "quant_2", "quant_3", "quant_4", "mod_check", "mod_1", "mod_2", 
      "mod_3", "mod_4", "mod_5", "mod_6", "mod_time", "op_1_REV", "op_2_REV",
      "op_5_REV", "sel_1_REV", "sel_3_REV", "sel_psychometric_evidence_REV", 
      "sel_psychometric_evidence_text_REV", "quant_1_REV", "quant_2_REV", 
      "quant_3_REV", "mod_check_REV", "mod_1_REV", "mod_2_REV", "mod_3_REV", 
      "mod_4_REV", "mod_5_REV", "mod_6_REV", "inseperable_material")
  
# renaming rows
rownames(coded_data_full) <- 1:nrow(coded_data_full)

# fixing some coding mistakes
coded_data_full$variable_name[79] <- "quote attribution effect"
coded_data_full$N[3] <- "5284"
coded_data_full$N[158] <- "1202"
coded_data_full$reliability_type[50] <- "Not Reported"
coded_data_full$reliability_type[127] <- "Not Reported"
coded_data_full$op_1[145] <- "False"
coded_data_full$op_3[121] <- "True"
coded_data_full$op_5[157] <- "True"
coded_data_full$sel_1[59] <- "True"
coded_data_full$quant_2[113] <- "True"
coded_data_full$mod_time[1] <- "Before"
coded_data_full$psychometric_evidence_text_REV[coded_data_full$sel_psychometric_evidence_text_REV == "convergent validitiy"] <- "convergent validity"


# removing missing entry 77
coded_data_full <- data.frame(coded_data_full)[-77,]

# Many labs 2.25 and 2.26, as well as 3.4 and 3.5 (for replications) were coded 
# in reverse order thus need to be swapped in right order. Additionally, some
# of the entries were included later than following their order, due to some
# minor coding oversights.
Coded_Data_Full_Restructured <- coded_data_full[c(1:18, 148, 19:23, 147, 24:28, 
                                                  153, 29:40, 154, 42, 41, 155, 
                                                  43:49, 156, 51, 50, 52:76, 
                                                  157, 77:88, 149:152, 89:146),]


# rename the rownames to match the new order
rownames(Coded_Data_Full_Restructured) <- 1:nrow(Coded_Data_Full_Restructured)

# changing the variable types for each column to better represent their 
# intended variable type
class(Coded_Data_Full_Restructured$many_labs_version) <- "numeric"
Coded_Data_Full_Restructured$many_labs_version <- as.factor(Coded_Data_Full_Restructured$many_labs_version)
Coded_Data_Full_Restructured$rep_org <- droplevels(as.factor(Coded_Data_Full_Restructured$rep_org))
Coded_Data_Full_Restructured$multi <- droplevels(as.factor(Coded_Data_Full_Restructured$multi))
Coded_Data_Full_Restructured$variable_order <- droplevels(as.factor(Coded_Data_Full_Restructured$variable_order))
class(Coded_Data_Full_Restructured$N) <- "numeric"
Coded_Data_Full_Restructured$N_items <- droplevels(as.factor(Coded_Data_Full_Restructured$N_items))
Coded_Data_Full_Restructured$hypothesis_support <- droplevels(as.factor(Coded_Data_Full_Restructured$hypothesis_support))
levels(Coded_Data_Full_Restructured$hypothesis_support) <- c("No", "Unclear", "Yes")
Coded_Data_Full_Restructured$reliability_type <- droplevels(as.factor(Coded_Data_Full_Restructured$reliability_type))
class(Coded_Data_Full_Restructured$reliability_coeff) <- "numeric"
Coded_Data_Full_Restructured$def_1 <- as.logical(Coded_Data_Full_Restructured$def_1)
Coded_Data_Full_Restructured$op_1 <- as.logical(Coded_Data_Full_Restructured$op_1)
Coded_Data_Full_Restructured$op_2 <- as.logical(Coded_Data_Full_Restructured$op_2)
Coded_Data_Full_Restructured$op_3 <- as.logical(Coded_Data_Full_Restructured$op_3)
Coded_Data_Full_Restructured$op_4 <- as.logical(Coded_Data_Full_Restructured$op_4)
Coded_Data_Full_Restructured$op_5 <- as.logical(Coded_Data_Full_Restructured$op_5)
Coded_Data_Full_Restructured$sel_existing <- droplevels(as.factor(Coded_Data_Full_Restructured$sel_existing))
Coded_Data_Full_Restructured$sel_1 <- as.logical(Coded_Data_Full_Restructured$sel_1)
Coded_Data_Full_Restructured$sel_2 <- as.logical(Coded_Data_Full_Restructured$sel_2)
Coded_Data_Full_Restructured$sel_3 <- as.logical(Coded_Data_Full_Restructured$sel_3)
Coded_Data_Full_Restructured$sel_4 <- as.logical(Coded_Data_Full_Restructured$sel_4)
Coded_Data_Full_Restructured$sel_psychometric_evidence <- droplevels(as.factor(Coded_Data_Full_Restructured$sel_psychometric_evidence))
Coded_Data_Full_Restructured$quant_1 <- as.logical(Coded_Data_Full_Restructured$quant_1)
Coded_Data_Full_Restructured$quant_2 <- as.logical(Coded_Data_Full_Restructured$quant_2)
Coded_Data_Full_Restructured$quant_3 <- as.logical(Coded_Data_Full_Restructured$quant_3)
Coded_Data_Full_Restructured$quant_4 <- as.logical(Coded_Data_Full_Restructured$quant_4)
Coded_Data_Full_Restructured$mod_check <- droplevels(as.factor(Coded_Data_Full_Restructured$mod_check))
Coded_Data_Full_Restructured$mod_1 <- as.logical(Coded_Data_Full_Restructured$mod_1)
Coded_Data_Full_Restructured$mod_2 <- as.logical(Coded_Data_Full_Restructured$mod_2)
Coded_Data_Full_Restructured$mod_3 <- as.logical(Coded_Data_Full_Restructured$mod_3)
Coded_Data_Full_Restructured$mod_4 <- as.logical(Coded_Data_Full_Restructured$mod_4)
Coded_Data_Full_Restructured$mod_5 <- as.logical(Coded_Data_Full_Restructured$mod_5)
Coded_Data_Full_Restructured$mod_6 <- as.logical(Coded_Data_Full_Restructured$mod_6)
Coded_Data_Full_Restructured$mod_time <- droplevels(as.factor(Coded_Data_Full_Restructured$mod_time))
Coded_Data_Full_Restructured$op_1_REV <- as.logical(Coded_Data_Full_Restructured$op_1_REV)
Coded_Data_Full_Restructured$op_2_REV <- as.logical(Coded_Data_Full_Restructured$op_2_REV)
Coded_Data_Full_Restructured$op_5_REV <- as.logical(Coded_Data_Full_Restructured$op_5_REV)
Coded_Data_Full_Restructured$sel_1_REV <- as.logical(Coded_Data_Full_Restructured$sel_1_REV)
Coded_Data_Full_Restructured$sel_3_REV <- as.logical(Coded_Data_Full_Restructured$sel_3_REV)
Coded_Data_Full_Restructured$sel_psychometric_evidence_REV <- droplevels(as.factor(Coded_Data_Full_Restructured$sel_psychometric_evidence_REV))
Coded_Data_Full_Restructured$quant_1_REV <- as.logical(Coded_Data_Full_Restructured$quant_1_REV)
Coded_Data_Full_Restructured$quant_2_REV <- as.logical(Coded_Data_Full_Restructured$quant_2_REV)
Coded_Data_Full_Restructured$quant_3_REV <- as.logical(Coded_Data_Full_Restructured$quant_3_REV)
Coded_Data_Full_Restructured$mod_check_REV <- droplevels(as.factor(Coded_Data_Full_Restructured$mod_check_REV))
Coded_Data_Full_Restructured$mod_1_REV <- as.logical(Coded_Data_Full_Restructured$mod_1_REV)
Coded_Data_Full_Restructured$mod_2_REV <- as.logical(Coded_Data_Full_Restructured$mod_2_REV)
Coded_Data_Full_Restructured$mod_3_REV <- as.logical(Coded_Data_Full_Restructured$mod_3_REV)
Coded_Data_Full_Restructured$mod_4_REV <- as.logical(Coded_Data_Full_Restructured$mod_4_REV)
Coded_Data_Full_Restructured$mod_5_REV <- as.logical(Coded_Data_Full_Restructured$mod_5_REV)
Coded_Data_Full_Restructured$mod_6_REV <- as.logical(Coded_Data_Full_Restructured$mod_6_REV)
Coded_Data_Full_Restructured$inseperable_material <- droplevels(as.factor(Coded_Data_Full_Restructured$inseperable_material)

                                                                
                                                                

# the moral foundations questionnaire in original 2.4 is reported using all 5 of 
# its factors, whereas in replication 2.4 only the two overarching groups of 
# binding and individualizing foundations are described. For that reason a 
# shortened original dataset will be used for any direct comparisons between 
# original and replication coding.

Coded_Data_Full_Shortened <- Coded_Data_Full_Restructured[c(1:94, 96, 99:157),] 
Coded_Data_Full_Shortened[c(95,96),5] <- c("individualizing moral foundations", "binding moral foundations")
Coded_Data_Full_Shortened[95,13] <- NA
Coded_Data_Full_Shortened[96,13] <- NA


# separating the replication and original data from each other.
coded_data_replications <- Coded_Data_Full_Shortened[1:77,]
coded_data_original <- Coded_Data_Full_Shortened[78:154,]

# exporting cleaned data
saving_to_intermediate_data(coded_data_replications)
saving_to_intermediate_data(coded_data_original)

```

## Unit of Analysis

Our unit of analysis is a measure of a single variable within a replication protocol or original article that was used in the main analysis that was replicated. For example, if conscientiousness and agreeableness were both measured using the Big 5 Personality Test, each of which was a variable in a replicated effect, then both the conscientiousness and agreeableness part of that questionnaire would be coded as their own unit of analysis. We also allowed for multiple variables to be measured per study. We used the replication protocols to identify the measure of each variable. We did not include acquiescence bias checks, manipulation checks, pilot test measures, and measures added for exploratory analyses. Our final sample size was `r nrow(coded_data_replications)` measures of unique variables for both original and replication studies. Initially, the original articles contained 3 more measures of unique variables than the replication protocols. This difference was due to the way that the moral foundations questionnaire was framed in the original article [@grahamLiberalsConservativesRely2009] compared to in the replication protocol for Many Labs 2 [@kleinManyLabs22018b]. In the original article it was framed as measuring five different moral foundations, while in the replication protocol the measure assessed the two overarching categories that were used to test the main effect in both the original and replication research. The measurement information reported was comparable across all five categories, and thus it was deemed that the measurement could be reduced to reflect two overarching categories to facilitate easier comparison between measurement in original and replication.

## Coding of Articles and Replications 

### Measurement Reporting

We evaluated the transparency of the measurement reporting practices within the original articles and replication protocols using our preregistered coding protocol containing 23 reporting practices. The criteria were based on Table 1 presented in Flake et al. @flakeMeasurementSchmeasurementQuestionable2020, listing what measurement information to report. We coded a criterion as “true” if the relevant measurement information was clearly reported, “false” if it was missing or unclear, and “not applicable” if it was irrelevant for that measure (e.g., reporting factor analysis results for single-item measures). These criteria for transparent reporting practices can be seen as contra-indicators of QMPs. We grouped the criteria into five categories (Definition, Operationalisation, Selection/Creation, Quantification, Modification), each representing a different element in measurement reporting. Selection and creation of a measure share a category because the criteria for selecting a measure are similar to those for creating a new measure. Examples of criteria are: "The administration format (pen-and-paper/computer) and environment (in public/in a lab) are described" (Operationalisation); "The number of items are described" (Quantification). The full coding protocol can be found in the [Revised Coding Protocol](../../SupplementaryMaterials/Measurement_Error_Reporting_Revised_Coding_Protocol.pdf) supplementary document.

Included within the list of measurement reporting information is crucially also the reported reliability coefficient and type of index (Cronbach’s Alpha, test-retest correlation, inter-rater reliability coefficient, etc.) when present. As well as the presence of any psychometric convergent, discriminant, predictive or factorial validity evidence for the measure.

After the initial coding, we made minor alterations from the preregistered coding protocol for 14 of the 23 measurement reporting practices. We revised these coded criteria because early results indicated that our original criteria were too stringent. For example, in the initial protocol, an example item of the measure had to be present within the article or protocol itself, for the measurement practice to be considered clearly reported. In the revised protocol, references to online appendices with example items were also considered sufficient for this criterion. The analyses, tables, and figures presented in this article are all based on the revised coding protocol. The equivalent measurement reporting descriptives obtained with the initial protocol can be found in [Supplementary Analyses B](../../SupplementaryMaterials/SupplementaryAnalysesScripts/Supplementary_initial_QMP_ratio_table.rmd).

We initially intended to construct an index from the transparent measurement reporting criteria and perform regression analyses on this index and associated replication outcomes. However, this index could not be validated properly, due to the lack of reported information, including information that could be used to construct a suitable outcome to assess the index' predictive validity. Results based on this index would be misleading to present here in the main article. A more detailed explanation of what was omitted and why, as well as the results from these preregistered analyses can be found in [Supplementary Analyses A](../../SupplementaryMaterials/SupplementaryAnalysesScripts/Supplementary_exploratory_version_pre-reg_analyses.Rmd).

```{r Calculating_QMP_Data, include = FALSE, eval = FALSE}
# function for changing QMP data to their ratio equivalent
Create_QMP_descriptive_case <- function(practice_name, original_coded_var, replication_coded_var){
  # add practice name to first row
  Practice <- practice_name
  
  # Sum all QMP/GMP (Good Measurement Practice), keeping NA items out of
  # the equation
  original_n_QMPs <- sum(original_coded_var == "FALSE", na.rm = TRUE)
  replication_n_QMPs <- sum(replication_coded_var == "FALSE", na.rm = TRUE)
  
  original_n_GMPs <- sum(original_coded_var == "TRUE", na.rm = TRUE)
  replication_n_GMPs <- sum(replication_coded_var == "TRUE", na.rm = TRUE)
  
  # calculating the N of applicable items for originals
  N_applicable_original <- original_n_QMPs + original_n_GMPs
  
  # checking that we are not dividing by 0, and then calculating the ratio of 
  # QMPs for applicable items for originals
  if(N_applicable_original != 0){
    QMP_percentage_original <- round(original_n_QMPs / (N_applicable_original), 2)
  } else{
    QMP_percentage_original <- 0
  }                               
  
  # calculating the N of applicable items for replications
  N_applicable_replication <- replication_n_QMPs + replication_n_GMPs
  
  # checking that we are not dividing by 0, and then calculating the ratio of 
  # QMPs for applicable items for replications
  if(N_applicable_replication != 0){
    QMP_percentage_replication <- round(replication_n_QMPs / N_applicable_replication, 2)
  } else{
    QMP_percentage_replication <- 0
  }     
  
  # calculating the Phi coefficient to estimate the relation between variables
  Phi <- phi(matrix(c(original_n_QMPs, original_n_GMPs, replication_n_QMPs, 
                      replication_n_GMPs), nrow = 2, byrow = TRUE))
  
  return(c(Practice, QMP_percentage_original, N_applicable_original, QMP_percentage_replication, N_applicable_replication, Phi))
}



# loading the relevant items
original_QMP_data <- coded_data_original[c("def_1", "sel_2", "op_4", "reliability_type", "sel_existing", "op_version", "op_1_REV", "sel_1_REV", "sel_3_REV", "sel_4", "sel_psychometric_evidence", "op_2_REV", "op_3", "quant_1_REV", "quant_2_REV", "quant_3_REV", "quant_4", "op_5_REV", "mod_1_REV", "mod_2_REV", "mod_3_REV", "mod_4_REV", "mod_5_REV", "mod_6_REV")]

replication_QMP_data <- coded_data_replications[c("def_1", "sel_2", "op_4", "reliability_type", "sel_existing", "op_version", "op_1_REV", "sel_1_REV", "sel_3_REV", "sel_4", "sel_psychometric_evidence", "op_2_REV", "op_3", "quant_1_REV", "quant_2_REV", "quant_3_REV", "quant_4", "op_5_REV", "mod_1_REV", "mod_2_REV", "mod_3_REV", "mod_4_REV", "mod_5_REV", "mod_6_REV")]



# create an empty dataset to add a recoded TRUE and FALSE response for QMPs to 
QMP_data <- data.frame(delete_this = rep(NA, 77))


QMP_data$reliability_reported_org <- ifelse(coded_data_original$N_items == "multiple item measure", ifelse(original_QMP_data$reliability_type != "Not Reported" & original_QMP_data$reliability_type != "" & !is.na(original_QMP_data$reliability_type), TRUE, ifelse(original_QMP_data$reliability_type == "Not Reported", FALSE, NA)), NA)

QMP_data$reliability_reported_rep <- ifelse(coded_data_replications$N_items == "multiple item measure", ifelse(replication_QMP_data$reliability_type != "Not Reported" & replication_QMP_data$reliability_type != "" & !is.na(replication_QMP_data$reliability_type), TRUE, ifelse(replication_QMP_data$reliability_type == "Not Reported", FALSE, NA)), NA)


# add if clearly specified if measure existed or not
QMP_data$select_or_create_clarity_org <- original_QMP_data$sel_existing == "Not Clearly Stated"

QMP_data$select_or_create_clarity_rep <- replication_QMP_data$sel_existing == "Not Clearly Stated"


# add if version was clearly specified
QMP_data$version_clarity_org <- original_QMP_data$op_1_REV == FALSE | original_QMP_data$op_version == "" & original_QMP_data$sel_existing == "True, namely:"

QMP_data$version_clarity_rep <- replication_QMP_data$op_1_REV == FALSE | replication_QMP_data$op_version == "" & replication_QMP_data$sel_existing == "True, namely:"


# add if factor structure was analysed
QMP_data$factor_analysis_org <- ifelse(original_QMP_data$sel_psychometric_evidence == "None", FALSE, ifelse(original_QMP_data$sel_psychometric_evidence != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)", TRUE, NA))

QMP_data$factor_analysis_rep <- ifelse(replication_QMP_data$sel_psychometric_evidence == "None", FALSE, ifelse(replication_QMP_data$sel_psychometric_evidence != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)", TRUE, NA))

# direct copies
QMP_data$justified_definition_org <- original_QMP_data$def_1
QMP_data$justified_definition_rep <- replication_QMP_data$def_1
QMP_data$justified_selection_org <- original_QMP_data$sel_2
QMP_data$justified_selection_rep <- replication_QMP_data$sel_2
QMP_data$justified_operationalisation_org <- original_QMP_data$op_4
QMP_data$justified_operationalisation_rep <- replication_QMP_data$op_4
QMP_data$source_org <- original_QMP_data$sel_1_REV
QMP_data$source_rep <- replication_QMP_data$sel_1_REV
QMP_data$in_psychometric_org <- original_QMP_data$sel_3_REV
QMP_data$in_psychometric_rep <- replication_QMP_data$sel_3_REV
QMP_data$out_psychometric_org <- original_QMP_data$sel_4
QMP_data$out_psychometric_rep <- replication_QMP_data$sel_4
QMP_data$admin_format_org <- original_QMP_data$op_2_REV
QMP_data$admin_format_rep <- replication_QMP_data$op_2_REV
QMP_data$admin_procedure_org <- original_QMP_data$op_3
QMP_data$admin_procedure_rep <- replication_QMP_data$op_3
QMP_data$N_items_org <- original_QMP_data$quant_1_REV
QMP_data$N_items_rep <- replication_QMP_data$quant_1_REV
QMP_data$N_response_org <- original_QMP_data$quant_2_REV
QMP_data$N_response_rep <- replication_QMP_data$quant_2_REV
QMP_data$recoding_org <- original_QMP_data$quant_3_REV
QMP_data$recoding_rep <- replication_QMP_data$quant_3_REV
QMP_data$aggregation_org <- original_QMP_data$quant_4
QMP_data$aggregation_rep <- replication_QMP_data$quant_4
QMP_data$example_items_org <- original_QMP_data$op_5_REV
QMP_data$example_items_rep <- replication_QMP_data$op_5_REV
QMP_data$mod_admin_format_org <- original_QMP_data$mod_1_REV
QMP_data$mod_admin_format_rep <- replication_QMP_data$mod_1_REV
QMP_data$mod_admin_format_support_org <- original_QMP_data$mod_2_REV
QMP_data$mod_admin_format_support_rep <- replication_QMP_data$mod_2_REV
QMP_data$mod_language_org <- original_QMP_data$mod_3_REV
QMP_data$mod_language_rep <- replication_QMP_data$mod_3_REV
QMP_data$mod_language_support_org <- original_QMP_data$mod_4_REV
QMP_data$mod_language_support_rep <- replication_QMP_data$mod_4_REV
QMP_data$mod_N_items_or_response_org <- original_QMP_data$mod_5_REV
QMP_data$mod_N_items_or_response_rep <- replication_QMP_data$mod_5_REV
QMP_data$mod_N_items_or_response_support_org <- original_QMP_data$mod_6_REV
QMP_data$mod_N_items_or_response_support_rep <- replication_QMP_data$mod_6_REV


QMP_data <- QMP_data[,!(names(QMP_data) %in% "delete_this")]



### QMP ratio data
# create the empty QMP ratio dataset
QMP_ratio_data <- data.frame(Practice = rep("", ncol(QMP_data)/2),
                   QMP_percentage_original = rep(0, ncol(QMP_data)/2),
                   N_applicable_original = rep(0, ncol(QMP_data)/2),
                   QMP_percentage_replication = rep(0, ncol(QMP_data)/2),
                   N_applicable_replication = rep(0, ncol(QMP_data)/2),
                   Phi = rep(0, ncol(QMP_data)/2))


# we loop through the length of the number of Measurement Practices (MPs) 
# ignoring doubles due to having both original and replication
for(i in 1:(ncol(QMP_data)/2)){
  # we create the name of the variable by taking the name and removing the 
  # _org suffix. Then we for each column except the NA initial column
  # we take the original MP and the related replication MP, and calculate
  # their QMP
  QMP_ratio_data[i,] <- Create_QMP_descriptive_case(substr(names(QMP_data)[i*2], 1, nchar(names(QMP_data)[i*2]) - 4), QMP_data[[(i*2)-1]], QMP_data[[i*2]])
  }

# store QMP ratio data in the analysis data folder
saving_to_analysis_data(QMP_ratio_data)
```

### Calculating Reliability

We calculated internal consistency reliabilities from the item responses on each multiple-item measure for each lab separately using available data. We calculated both Cronbach’s Alpha, as well as its standard error using formulas 2 & 3 from Duhachek and Lacobucci @duhachekAlphasStandardError2004a. We used these values to conduct a meta-analysis of the measure’s reliability, also referred to as a Reliability Generalization (RG) Meta-Analysis [@botellaManagingHeterogeneityVariance2012; @lopez-ibanezReliabilityGeneralizationMetaanalysis2024; @vacha-haaseReliabilityGeneralizationExploring1998]. We performed the RG Meta-Analysis using the *rma* function from the *metafor* R package (*v`r getNamespaceVersion("metafor")[[1]]`*; @R-metafor) and default settings. We then used the results from the meta-analysis to evaluate the heterogeneity via the tau statistic and the Cochran’s Q-test [@cochranCombinationEstimatesDifferent1954]. 
we note that these indicators have low power to detect heterogeneity when within study sample sizes are small [@hoaglinMisunderstandingsCochransTest2016; @pereiraCriticalInterpretationCochrans2010]. Therefore, we also present the 95% prediction interval (the interval within which the measure’s Cronbach’s Alpha is expected to fall if used again within the same population) and implore that the heterogeneity results should be viewed critically [@borensteinAvoidingCommonMistakes2024]. We implemented no correction for publication bias, because the Many Labs replications were guaranteed to be published regardless of their outcomes.

Our analyses will focus on Cronbach’s Alpha, because it is the most reported reliability indicator,allowing a comparison between originally reported and newly calculated reliabilities. Furthermore, the standard errors of Alpha caused in the RG meta-analysis to study variation in reliabilities across labs using the same measures. However, Cronbach’s Alpha comes with strong assumptions on the underlying factor structure, including unidimensionality of the underlying factor structure. Therefore, we also estimated McDonald’s Omega for each lab from the same set of replications as for Cronbach’s Alpha, since it has been argued to be a more informative measure of reliability than Cronbach’s Alpha with less strict assumptions [@crutzenScaleQualityAlpha2017b; @dengTestingDifferenceReliability2017]. The results based on McDonald’s Omega can be found in [Supplementary Analyses D](../../SupplementaryMaterials/SupplementaryAnalysesScripts/Supplementary_omega_analyses.Rmd).

```{r calculated_reliability_across_labs, include = FALSE, eval = FALSE}
# creating an empty data frame to insert all the responses into
calculated_reliability_lab_data <- data.frame(alpha = 0, omega.tot = 0, 
                                          omega.hier = 0, ASE = 0, g = 0)

# Combining the data together 1.3, & 5.4 were omitted, because data was not
# recorded in usable numeric format
extracted_score_data <- list(data_1.10_clean, data_1.11_clean, 
    data_1.12.3.1_clean, data_1.12.3.2_clean, data_2.12.1_clean, 
    data_2.12.2_clean, data_2.12.3_clean, data_2.15_clean, data_2.20_clean, 
    data_2.23_clean, data_3.2.1.1_clean, data_3.2.1.2_clean, data_3.7.1_clean,
    data_3.7.2_clean, data_3.8.2_clean, data_5.1.1_clean, data_5.1.2_clean, 
    data_5.7_clean, data_5.9.1_clean)


# obtaining the omega and alpha values for a measure in one lab.
get_omega_and_alpha_values <- function(Data){
  # first we calculate alpha and ase separately in case the omega function goes haywire
  alpha <- psych::alpha(Data)$total[["std.alpha"]]
  ase <- psych::alpha(Data)$total[["ase"]]
  
  # try to calculate the omega
  result <- c(NA, NA)
  tryCatch({
      result <- omega(Data)
    }, error = function(e) {
      result <- c(NA, NA)
    }, warning = function(w) {
      result <- c(NA, NA)
    })
  
  # combine the information in one vector
  omega_and_alpha_vec <- as.numeric(c(alpha, result[c(4, 1)], ase))
  
  return(omega_and_alpha_vec)
}


# calculate the alpha, omega.tot, omega.hier, & ASE for all relevant datasets
# for each lab.
for (i in 1:length(extracted_score_data)){
  calculated_reliability_instance <- tapply(extracted_score_data[[i]][-1], 
                                            extracted_score_data[[i]]$g, 
                                            get_omega_and_alpha_values)
  
  calculated_reliability_instance <- data.frame(matrix(unlist(
    calculated_reliability_instance), ncol = 4, byrow = TRUE))
  
  # make sure the var names match the complete dataframe
  colnames(calculated_reliability_instance) <- c("alpha", "omega.tot", 
                                                 "omega.hier", "ASE")
  # adding the measure as a group (g) indicator
  calculated_reliability_instance$g <- i
  
  # adding this measure's data to the total
  calculated_reliability_lab_data <- rbind(calculated_reliability_lab_data, 
                                       calculated_reliability_instance)
}


# removing the empty first row
calculated_reliability_lab_data <- calculated_reliability_lab_data[-1,]

# making sure group (g) is a factor
calculated_reliability_lab_data$g <- as.factor(calculated_reliability_lab_data$g)

# indexing the meta-analysis results with a specific index relating to a 
# row (measure) in coded_data_replications
calculated_reliability_lab_data$coded_data_index <- c(rep(10, 36), rep(11, 36), 
  rep(14, 36), rep(14, 36), rep(29, 74), rep(30, 74), rep(31, 74), rep(34, 61), 
  rep(39, 60), rep(42, 58), rep(51, 21), rep(51, 21), rep(59, 20), rep(60, 21), 
  rep(61, 21), rep(65, 4), rep(66, 4), rep(74, 8), rep(76, 5))

# changing the group variable to reflect measure descriptions from the text.
# checked using:
# coded_data_replications[unique(calculated_reliability_data$reporting_index), 3], and
# coded_data_replications[unique(calculated_reliability_data$reporting_index), 5]
levels(calculated_reliability_lab_data$g) <- c("Caruso et al. (2012)", 
    "Husnu & Crisp (2010)", "Nosek et al. (2002), Math", "Nosek et al. (2002), Art", 
    "Anderson et al. (2012), SWL", "Anderson et al. (2012), PA", 
    "Anderson et al. (2012), NA", "Giessner & Schubert, (2007)", 
    "Norenzayan et al. (2002)", "Zhong & Lijenquist (2006)", 
    "Monin & Miller (2001), most", "Monin & Miller (2001), some", 
    "Cacioppo et al. (1983), arg",  "Cacioppo et al. (1983), nfc", 
    "De Fruyt et al. (2000)", "Albarracín et al. (2008), exp 5 verb", 
    "Albarracín et al. (2008), exp 5 math", "Shnabel & Nadler (2008)",
    "Vohs & Schooler (2008)")
  
calculated_reliability_lab_data$g <- factor(calculated_reliability_lab_data$g, 
    labels = c("Caruso et al. (2012)", 
    "Husnu & Crisp (2010)", "Nosek et al. (2002), Math", 
    "Nosek et al. (2002), Art", "Anderson et al. (2012), SWL", 
    "Anderson et al. (2012), PA", "Anderson et al. (2012), NA", 
    "Giessner & Schubert, (2007)", "Norenzayan et al. (2002)", 
    "Zhong & Lijenquist (2006)", "Monin & Miller (2001), most", 
    "Monin & Miller (2001), some", "Cacioppo et al. (1983), arg",  
    "Cacioppo et al. (1983), nfc", "De Fruyt et al. (2000)", 
    "Albarracín et al. (2008), exp 5 verb", 
    "Albarracín et al. (2008), exp 5 math", "Shnabel & Nadler (2008)", 
    "Vohs & Schooler (2008)"))

# adding whether or not an effect replicated based on what was coded from the 
# replication report.
calculated_reliability_lab_data$replication_success <- c(coded_data_replications[
    calculated_reliability_lab_data$coded_data_index, "hypothesis_support"])



# store per lab calculated reliability data in the analysis data folder
saving_to_analysis_data(calculated_reliability_lab_data)
```

```{r averaged_reliability, include = FALSE, eval = FALSE}
# function to assess the heterogeneity in the calculated Cronbach's Alpha values
# and get Cronbach's Alpha prediction intervals
assess_heterogeneity<- function(data_on_alpha){
  # run a random effects meta-analysis
  rma_model <- rma(yi = data_on_alpha$alpha, sei = data_on_alpha$ASE, 
                   method = "REML", control = list(stepadj = 0.5, maxiter = 1000))
  
  # extract the relevant heterogeneity information
  temp_tau <- sqrt(rma_model$tau2)
  temp_QEp <- rma_model$QEp
  
  # get prediction intervals for alpha
  rma_prediction <- predict(rma_model)
  temp_pi.lb <- rma_prediction$pi.lb
  temp_pi.ub <- rma_prediction$pi.ub
  
  return(c(temp_tau, temp_QEp, temp_pi.lb, temp_pi.ub))
}

# function to convert the lab specifc reliability to averaged
convert_reliability_data_to_avg <- function(reliability_data){
  # conducting the reliability-generalization meta-analysis for each measure
  heterogeneity_results <- assess_heterogeneity(reliability_data[c(1, 4)])
  
  # getting the avearage reliability scores for each measure
  avg_reliabilities <- colMeans(reliability_data[1:4], na.rm = TRUE)
  
  # indicates the measure
  g <- reliability_data$g[[1]]
  
  # we need one of the indices per measure as they are all the same across labs
  coded_data_index <- reliability_data$coded_data_index[[1]]
  
  # we need one of the indices per measure as they are all the same across labs
  replication_success <- reliability_data$replication_success[[1]]
  
  # extracting the reported reliability coefficient 
  coefficient_reported <- coded_data_original$reliability_coeff[coded_data_index]
  
  # calculating the difference between reported and calculated average 
  # reliability coefficient
  coeficient_difference <- coefficient_reported - avg_reliabilities[[1]]
  
  # testing whether or not (for those studies that had a reported alpha) if
  # it was out of the 95% bounds around the mean calculated alpha
  population_95_bounds <- quantile(reliability_data$alpha, probs = c(0.025, 0.975))
  
  significance_reported_coefficient <- coefficient_reported < population_95_bounds[1] | 
                                          coefficient_reported > population_95_bounds[2]
  
  # return all the data as a single row in the dataframe
  return(data.frame(alpha = avg_reliabilities[[1]], omega.tot = avg_reliabilities[[2]], 
                    omega.hier = avg_reliabilities[[3]], ASE = avg_reliabilities[[4]], 
                    tau = heterogeneity_results[[1]], QEp = heterogeneity_results[[2]],
                    pi.lb = heterogeneity_results[[3]], pi.ub = heterogeneity_results[[4]], 
                    g = g, coded_data_index = coded_data_index, 
                    replication_success, reported_coefficient = coefficient_reported, 
                    coefficient_difference = coeficient_difference, 
                    significance_reported_coefficient = significance_reported_coefficient))
}

# calculate the average reliability data + heterogeneity test + reported and calculated
# reliability coefficient comparison
avg_reliability_list <- tapply(calculated_reliability_lab_data, calculated_reliability_lab_data$g, convert_reliability_data_to_avg)

# convert data output to a dataframe
measure_reliability_data <- do.call(rbind.data.frame, avg_reliability_list)


# store per measure  reliability data in the analysis data folder
saving_to_analysis_data(measure_reliability_data)
```

### Unidimensionality

The replicators formed a singular index of one latent variable for all measures for which we checked unidimensionality. Therefore, unidimensionality presents a valuable indication of the construct validity of a measure in our dataset. To test for unidimensionality, we fit a single-factor model on the item responses of each lab with suitable data. Our inference of unidimensionality is based on a set of four model fit indices and their commonly used thresholds indicating adequate fit: RMSEA (threshold: < .08), SRMR (< .08), CFI (> .90), and the exact fit test (statistically significant at .05). We consider the use of these thresholds sufficient for our descriptive aim, even though we understand apprehension against rules of thumb when used to evaluate measurement in individual studies. As a fifth additional fit index we ran a parallel analysis. A parallel analysis runs multiple Exploratory Factor Analyses where the number of factors in the model is increased by one until the number of factors is one less than the number of items. It then compares the eigenvalues (an indication of how much variance is explained by that factor) of each factor to the eigenvalues for that factor if the data matrix was effectively random. If only the first factor has an eigenvalue that is significantly higher than the eigenvalue when the data matrix is random, the test is passed. We chose a combination of indices to test unidimensionality, since each one has their own limitations, and combining them gives us a more robust picture of the unidimensionality of the measures.

These unidimensionality checks are not intended as a complete validation procedure of the measures in our sample. That would require an extended research process that is beyond the scope of this study. Instead, the unidimensionality is checked as a prerequisite for validity. Practically speaking a lack of unidimensional fit in one of the measures corresponds to the conclusion that a prerequisite for construct validity has been violated. On the other hand, if we observe good unidimensional fit, then based on this preliminary evidence we conclude that we do not yet observe an issue with validity. We also include the variation in the unidimensionality indices across labs in our evaluation, to determine if measures meeting the unidimensionality requirement is consistent across contexts or not.

```{r unidimensionality_tests, include = FALSE, eval = FALSE}
# function that runs all our checks for Cronbach's alpha. Also functions as some basic validity checks
validity_and_alpha_assumption_tests <- function(data){
  ### Tests for Unidimensionality
  # Test 1a: obtaining single factor model cfa RMSEA, CFI, SRMR, and exact fit test results
  RMSEA_values <- c(NA, NA)
  CFI <- NA
  SRMR <- NA
  fit_results <- c(NA, NA, NA)
  tryCatch({
    # creating the base function for the model (it is unidimensional)
    model_free <- "Factor =~ " 

    for (item_name in names(data)[-1]){
      # adding all of the items from the scale to the model formula
      model_free <- paste0(model_free, item_name, " + ")
    }
      
    # trimming the excess " + "
    model_free <- substring(model_free, 1, nchar(model_free) - 3)

    # fitting the free coefficient estimated single dimensional model
    fit.modelfree <- cfa(model_free, 
                              data = data, 
                              std.lv = TRUE, 
                              estimator = "MLM")

    # extracting the fit measures
    fit_measures <- fitMeasures(fit.modelfree, c("chisq", "df", "pvalue", "rmsea", "rmsea.ci.upper", "cfi", "srmr"))
    
    # extracting RMSEA and its standard error
    rmsea <- fit_measures["rmsea"] 
    rmsea.ci.upper <- fit_measures["rmsea.ci.upper"]
    rmsea.se <- (rmsea.ci.upper - rmsea) / 1.645
    RMSEA_values <- c(rmsea, rmsea.se)
    
    # extracting CFI
    CFI <- fit_measures["cfi"] 
    
    # extracting SRMR
    SRMR <- fit_measures["srmr"] 
    
    # extracting exact fit test measures
    fit_chisq <- fit_measures["chisq"]
    fit_df <- fit_measures["df"]
    fit_pvalue <- fit_measures["pvalue"]
    fit_results <- c(fit_chisq, fit_df, fit_pvalue)
    
    
  }, error = function(e) {
    RMSEA_values <- c(NA, NA)
    CFI <- NA
    SRMR <- NA
    fit_results <- c(NA, NA, NA)
  })

  
  
  # Test 1b: conducting parallel test to check if one factor solution is best.
  parallel_N_factors <- NA
  tryCatch({
      parallel_N_factors <- suppressWarnings(fa.parallel(data[-1], fa = "fa", plot = FALSE))$nfact

    }, error = function(e) {
      parallel_N_factors <- NA
  })
  
  # adding a single factor check count
  if(is.null(RMSEA_values[[1]]) | is.null(parallel_N_factors)){
    unidimensional_check_count <- NA
  } else{
    unidimensional_check_count <- sum(RMSEA_values[[1]] < .08, CFI > .90, SRMR < .08, fit_results[[3]] < .05, parallel_N_factors == 1)
  }
  
  
  ### Test 2: Test for Tau equivalence
  Tau_results <- c(NA, NA, NA)
  tryCatch({
    # creating the base function for the fixed model (it is unidimensional)
    model_tau_restrict <- "Factor =~ " 
  
    for (item_name in names(data)[-1]){
      # adding all of the items from the scale to the model formula
      # estimated with the same coefficient a to emulate a Tau equivalent model
      model_tau_restrict <- paste0(model_tau_restrict, "a*", item_name, " + ")
    }
    
    # trimming the excess " + "
    model_tau_restrict <- substring(model_tau_restrict, 1, nchar(model_tau_restrict) - 3)
    
    # fitting the tau restricted model
    fit.modeltaurestrict <- cfa(model_tau_restrict, 
                                     data = data, 
                                     std.lv = TRUE, 
                                     estimator = "MLM")
    
    #summary(fit.modelfree, fit.measures = TRUE)
    #summary(fit.modeltaurestrict, fit.measures = TRUE)
    
    # conducting the fit test between the free coefficient model and the tau
    # restricted model
    fit.test <- anova(fit.modelfree, fit.modeltaurestrict)
    
    # extract the relevant results
    Tau_results <- fit.test[2, c("Chisq diff", "Df diff", "Pr(>Chisq)")]
    }, error = function(e) {
      Tau_results <- c(NA, NA, NA)
  })
  
  ### Test 3: Assessment of uncorrelated errors
  error_cor_results <- rep(NA, 4)
  
  
  tryCatch({
    # copying the tau restricted model as the baseline.
    model_freed_errors <- model_tau_restrict
    fit.modelfreed <- fit.modeltaurestrict
    
    # create empty vector to put data into
    spec.all.vec <- rep(NA, 5)
    
    # looping through five instances of freeing the highest mod index error 
    # covariance parameter estimation. We loop 5 times rather than adding the top 5
    # highest mod index, because when one parameter is freed it will affect the extent 
    # to which other error covariances impact the fit since (part of) their additional 
    # explained variance may have already been captured in the parameter just freed.
    for (i in 1:5) {
      modindex_highest <- suppressWarnings(modindices(fit.modelfreed, sort = TRUE))[1,]
      
      # store the fully standardized estimated coefficient change for the highest
      # mod index freed parameter
      spec.all.vec[i] <- modindex_highest[[7]]
      
      # add the freeing of the parameter into the model code
      model_freed_errors <- paste(model_freed_errors, "\n", modindex_highest[[1]], modindex_highest[[2]], modindex_highest[[3]])
      
      # rerun the model
      fit.modelfreed  <- cfa(model_freed_errors, 
                             data = data, 
                             std.lv = TRUE, 
                             estimator = "MLM")
    }
  
    # perform a model comparison between the model with the top 5 error covariances
    # freely estimated and the model with Tau equivalence.
    fit.test2 <- anova(fit.modeltaurestrict, fit.modelfreed)
    
    error_cor_test_results <- fit.test2[2, c("Chisq diff", "Df diff", "Pr(>Chisq)")]
    
    # calculate the mean of the fully standardized estimated coefficient change for
    # all the highest mod index freed parameters
    spec.all.mean <- mean(spec.all.vec)
    
    # store all error related results
    error_cor_results <- unlist(c(spec.all.mean, error_cor_test_results))  
    
    }, error = function(e) {
      error_cor_results <- NA
  })
  
  ### storing all of the data
  FA_data <- c(RMSEA_values, CFI, SRMR, fit_results, parallel_N_factors, 
               unidimensional_check_count, Tau_results, error_cor_results)
  
  # returning the FA data
  return(FA_data)
}


extracted_data_list <- list(data_1.10_clean, data_1.11_clean, data_1.12.3.1_clean, data_1.12.3.2_clean, data_2.12.1_clean, data_2.12.2_clean, data_2.12.3_clean, data_2.15_clean, data_2.20_clean, data_2.23_clean, data_3.2.1.1_clean, data_3.2.1.2_clean, data_3.7.1_clean, data_3.7.2_clean, data_3.8.2_clean, data_5.1.1_clean, data_5.1.2_clean, data_5.7_clean, data_5.9.1_clean)




# list to store the data in
FA_list <- list(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
                NA, NA, NA, NA, NA, NA, NA, NA, NA)

# running the validity/alpha assumption checks for all of the extracted datasets
for (i in 1:length(extracted_data_list)){
  # applying the function across each individual lab
  FA_data <- tapply(extracted_data_list[[i]], extracted_data_list[[i]]$g,
                    validity_and_alpha_assumption_tests)

  # transforming the returned list to a dataframe
  FA_data_frame <- do.call(rbind.data.frame, FA_data)
  
  # Setting the column names
  names(FA_data_frame) <- c("RMSEA", "RMSEA_se", "CFI", "SRMR", "fit_chisq", "fit_df", 
                            "fit_pvalue", "N_factors", "unidimensional_check_count", "Tau_chisq_diff", 
                            "Tau_Df_diff", "Tau_p_diff", "mean_spec", "err_chisq_diff", 
                            "err_Df_diff", "err_p_diff")
  
  # append results to the overarching FA results list
  FA_list[[i]] <- FA_data_frame
}




# store all dataframes with the list of the assumption test data in 
# the analysis data folder
fa_Caruso_2012 <- FA_list[[1]]
saving_to_analysis_data(fa_Caruso_2012)
fa_Husnu_2010 <- FA_list[[2]]
saving_to_analysis_data(fa_Husnu_2010)
fa_Nosek_2002_Math <- FA_list[[3]]
saving_to_analysis_data(fa_Nosek_2002_Math)
fa_Nosek_2002_Art <- FA_list[[4]]
saving_to_analysis_data(fa_Nosek_2002_Art)
fa_Anderson_2012_SWL <- FA_list[[5]]
saving_to_analysis_data(fa_Anderson_2012_SWL)
fa_Anderson_2012_PA <- FA_list[[6]]
saving_to_analysis_data(fa_Anderson_2012_PA)
fa_Anderson_2012_NA <- FA_list[[7]]
saving_to_analysis_data(fa_Anderson_2012_NA)
fa_Giessner_2007 <- FA_list[[8]]
saving_to_analysis_data(fa_Giessner_2007)
fa_Norenzayan_2002 <- FA_list[[9]]
saving_to_analysis_data(fa_Norenzayan_2002)
fa_Zhong_2006 <- FA_list[[10]]
saving_to_analysis_data(fa_Zhong_2006)
fa_Monin_2001_most <- FA_list[[11]]
saving_to_analysis_data(fa_Monin_2001_most)
fa_Monin_2001_some <- FA_list[[12]]
saving_to_analysis_data(fa_Monin_2001_some)
fa_Cacioppo_1983_arg <- FA_list[[13]]
saving_to_analysis_data(fa_Cacioppo_1983_arg)
fa_Cacioppo_1983_nfc <- FA_list[[14]]
saving_to_analysis_data(fa_Cacioppo_1983_nfc)
fa_De_Fruyt_2000 <- FA_list[[15]]
saving_to_analysis_data(fa_De_Fruyt_2000)
fa_Albarracin_2008_verb <- FA_list[[16]]
saving_to_analysis_data(fa_Albarracin_2008_verb)
fa_Albarracin_2008_math <- FA_list[[17]]
saving_to_analysis_data(fa_Albarracin_2008_math)
fa_Shnabel_2008 <- FA_list[[18]]
saving_to_analysis_data(fa_Shnabel_2008)
fa_Vohs_2008 <- FA_list[[19]]
saving_to_analysis_data(fa_Vohs_2008)


rm(extracted_data_list)
```

# Results

## Measurement Reporting

### Transparency and Reusability

We coded `r nrow(coded_data_original)` measures used across the original and replication studies. Table \@ref(tab:QMPTable) lists the prevalence of different transparent measurement reporting practices in original and replication studies. Specifically, we calculated the proportion of measures that transparently reported on the item compared to the total number of measures to which the item was applicable. 

```{r QMPTable}
# create informative Measurement Practice Labels 
MPractice <- c("Reliability is reported", "Clear if the measure is newly created or not",    "Exact version of the measure is specified", "Factor analytical results are presented",   "Measured variable is defined", "Measure selection or creation is justified",    "Implemented operationalisation is justified", "Source of the measure is provided",    "Psychometric evidence from the study is given", "Pre-existing psychometric evidence is given",    "Administration format and place are described", "Administration procedure is described",   "Number of items is described", "Number of response options is described", "Recoding of responses is described", "Creation of the index is described", "Text or supplement has example items", "Administration format changes are mentioned",    "Administration format changes are justified", "Translations are mentioned",    "Translated measures are justified", "Change in N items/response options mentioned",    "Change in N items/response options are justified")   


# construct the QMP table to be printed 
QMP_table <- data.frame("Categories" = NA,
                        "Criterion" = MPractice, 
                        "Original" = paste0(format(round(1 - qmp_ratio_data$QMP_percentage_original, digits = 2), nsmall = 2), " (", qmp_ratio_data$N_applicable_original, ")"), 
                        "Replication" = paste0(format(round(1 - qmp_ratio_data$QMP_percentage_replication, digits = 2), nsmall = 2), " (", qmp_ratio_data$N_applicable_replication, ")"))   


QMP_table_formatted <- QMP_table[c(5, 3, 11, 12, 7, 17, 2, 6, 8, 9, 10, 1, 4, 13, 14, 15, 16, 18:23),]


QMP_table_formatted$Categories <- c("Definition", rep("Operationalisation", 5), rep("Selection/Creation", 7), rep("Quantification", 4), rep("Modification", 6))


# set the zero N Items to "- (0)"
QMP_table_formatted[18, 3] <- "- (0)"
QMP_table_formatted[19, 3] <- "- (0)"


# print the table in apa formatting 
apa_table(QMP_table_formatted, align = c("l", "l", "r", "r"), 
          caption = "Proportion of measures that met each of the criteria, for both original and replication studies. Proportions are calculated based on the total number of measures to which an item was applicable (in brackets).", escape = FALSE, placement = "htp", booktabs = TRUE)

```

```{r ReuseCompleteInfoPercent}
reuse_complete_percent_original <- sprintf("%.1f", (sum((coded_data_original$op_1_REV | is.na(coded_data_original$op_1_REV)) & (coded_data_original$op_2_REV | is.na(coded_data_original$op_2_REV)) & (coded_data_original$op_3 | is.na(coded_data_original$op_3)) & (coded_data_original$op_4 | is.na(coded_data_original$op_4)) & (coded_data_original$op_5_REV | is.na(coded_data_original$op_5_REV)) & coded_data_original$sel_existing != "Not Clearly Stated" & (coded_data_original$sel_1_REV | is.na(coded_data_original$sel_1_REV)) & (coded_data_original$quant_1_REV | is.na(coded_data_original$quant_1_REV)) & (coded_data_original$quant_2_REV | is.na(coded_data_original$quant_2_REV)) & (coded_data_original$quant_3_REV | is.na(coded_data_original$quant_3_REV)) & (coded_data_original$quant_4 | is.na(coded_data_original$quant_4))) / nrow(coded_data_original)) * 100)

reuse_complete_percent_replication <- sprintf("%.1f", (sum((coded_data_replications$op_1_REV | is.na(coded_data_replications$op_1_REV)) & (coded_data_replications$op_2_REV | is.na(coded_data_replications$op_2_REV)) & (coded_data_replications$op_3 | is.na(coded_data_replications$op_3)) & (coded_data_replications$op_4 | is.na(coded_data_replications$op_4)) & (coded_data_replications$op_5_REV | is.na(coded_data_replications$op_5_REV)) & coded_data_replications$sel_existing != "Not Clearly Stated" & (coded_data_replications$sel_1_REV | is.na(coded_data_replications$sel_1_REV)) & (coded_data_replications$quant_1_REV | is.na(coded_data_replications$quant_1_REV)) & (coded_data_replications$quant_2_REV | is.na(coded_data_replications$quant_2_REV)) & (coded_data_replications$quant_3_REV | is.na(coded_data_replications$quant_3_REV)) & (coded_data_replications$quant_4 | is.na(coded_data_replications$quant_4))) / nrow(coded_data_replications)) * 100)
```

For the description of the results, we highlight the reporting practices most relevant for reusing the measure in future research, as well as the modification reporting practices to understand the relation between original and replication studies. It was not always clear if the measure already existed or not (`r sprintf("%.0f", (1 - qmp_ratio_data$QMP_percentage_original[qmp_ratio_data$Practice == "select_or_create_clarity"]) * 100)`% original (N = `r qmp_ratio_data$N_applicable_original[qmp_ratio_data$Practice == "select_or_create_clarity"]`); `r sprintf("%.0f", (1 - qmp_ratio_data$QMP_percentage_replication[qmp_ratio_data$Practice == "select_or_create_clarity"]) * 100)`% replication (N = `r qmp_ratio_data$N_applicable_replication[qmp_ratio_data$Practice == "select_or_create_clarity"]`)), which also implies that in these cases we were unsure whether the same measure was used in original and replication studies. While the number of items (`r sprintf("%.0f", (1 - qmp_ratio_data$QMP_percentage_original[13]) * 100)`% original (N = `r qmp_ratio_data$N_applicable_original[13]`); `r sprintf("%.0f", (1 - qmp_ratio_data$QMP_percentage_replication[13]) * 100)`% replications (N = `r qmp_ratio_data$N_applicable_replication[13]`)) and response options (`r sprintf("%.0f", (1 - qmp_ratio_data$QMP_percentage_original[14]) * 100)`% original (N = `r qmp_ratio_data$N_applicable_original[14]`); `r sprintf("%.0f", (1 - qmp_ratio_data$QMP_percentage_replication[14]) * 100)`% replications (N = `r qmp_ratio_data$N_applicable_replication[14]`)) were usually reported, even such basic aspects were not always clearly reported. How the responses should be recoded if at all (`r sprintf("%.0f", (1 - qmp_ratio_data$QMP_percentage_original[15]) * 100)`% original (N = `r qmp_ratio_data$N_applicable_original[15]`); `r sprintf("%.0f", (1 - qmp_ratio_data$QMP_percentage_replication[15]) * 100)`% replication (N = `r qmp_ratio_data$N_applicable_replication[15]`)), and how an index was created from the items (`r sprintf("%.0f", (1 - qmp_ratio_data$QMP_percentage_original[16]) * 100)`% original (N = `r qmp_ratio_data$N_applicable_original[16]`); `r sprintf("%.0f", (1 - qmp_ratio_data$QMP_percentage_replication[16]) * 100)`% replication (N = `r qmp_ratio_data$N_applicable_replication[16]`)) was also often unclear. The operationalisation of the measure was usually reported clearly in the replication studies (format: `r sprintf("%.0f", (1 - qmp_ratio_data$QMP_percentage_replication[11]) * 100)`%, procedure: `r sprintf("%.0f", (1 - qmp_ratio_data$QMP_percentage_replication[12]) * 100)`%, justification: `r sprintf("%.0f", (1 - qmp_ratio_data$QMP_percentage_replication[7]) * 100)`%, example items: `r sprintf("%.0f", (1 - qmp_ratio_data$QMP_percentage_replication[17]) * 100)`%), but less so in original studies (format: `r sprintf("%.0f", (1 - qmp_ratio_data$QMP_percentage_original[11]) * 100)`%, procedure: `r sprintf("%.0f", (1 - qmp_ratio_data$QMP_percentage_original[12]) * 100)`%, justification: `r sprintf("%.0f", (1 - qmp_ratio_data$QMP_percentage_original[7]) * 100)`%, example items: `r sprintf("%.0f", (1 - qmp_ratio_data$QMP_percentage_original[17]) * 100)`%). The measures for which all criteria relevant for reuse were clearly reported on was `r reuse_complete_percent_original`% of all measures in original studies and `r reuse_complete_percent_replication`% in replications.

We observed that measurement was modified in some way for `r sprintf("%.0f", (sum(coded_data_original$mod_check == "True") / nrow(coded_data_original)) * 100)`% of measures in original studies. Modifications were more common from original to replication `r sprintf("%.1f", (sum(coded_data_replications$mod_check == "True") / nrow(coded_data_replications)) * 100)`%.  A justification that the modification did not cause issues for validity for all modified measures in original studies and `r sprintf("%.0f", (sum(coded_data_replications$mod_2_REV, na.rm = TRUE) + sum(coded_data_replications$mod_4_REV, na.rm = TRUE) + sum(coded_data_replications$mod_6_REV, na.rm = TRUE)) / (sum(!is.na(coded_data_replications$mod_2_REV)) + sum(!is.na(coded_data_replications$mod_4_REV)) + sum(!is.na(coded_data_replications$mod_6_REV))) * 100)`% in replications for instances were modifications occurred.

### Reliability Reporting

Figure \@ref(fig:ReliabilityReportingFlowDiagram) depicts a flowchart of measure types and reliability reporting in original and replication studies. First, it shows that almost half of the measures in both original (N = `r paste0(sum(coded_data_original$N_items == "1 item measure"), "/", nrow(coded_data_original))`) and replication research (N = `r paste0(sum(coded_data_replications$N_items == "1 item measure"), "/", nrow(coded_data_replications))`) were single-item measures. Second, reliability indicators were reported for multiple-item measures in only `r sum(coded_data_original$reliability_type != "Not Reported" & coded_data_original$reliability_type != "")` out of `r sum(coded_data_original$N_items == "multiple item measure")`  (`r sprintf("%.1f", (sum(coded_data_original$reliability_type != "Not Reported" & coded_data_original$reliability_type != "") / sum(coded_data_original$N_items == "multiple item measure")) * 100)` %) original studies and `r sum(coded_data_replications$reliability_type != "Not Reported" & coded_data_replications$reliability_type != "")` out of `r sum(coded_data_replications$N_items == "multiple item measure")` (`r sprintf("%.1f", (sum(coded_data_replications$reliability_type != "Not Reported" & coded_data_replications$reliability_type != "") / sum(coded_data_replications$N_items == "multiple item measure")) * 100)`%) replications, which was appreciably lower than found in Flake et al. [@flakeConstructValidityValidity2022]. on reliability coefficient reporting in both original studies (60.8%) and replications (37.1%). However, similarly to Flake et al. @flakeConstructValidityValidity2022, we observed that reliability reporting was more common in original studies as compared to replication studies, and that Cronbach’s Alpha was the most commonly reported reliability indicator in our sample as well.

```{r ReliabilityReportingFlowDiagram, fig.cap = "Reliability reporting flow diagram. Figure shows the number of measures as reported in both the replication protocols and original article, which meet the criterion in the box within the diagram and those criteria before it.", out.height = "60%"}
knitr::include_graphics(path = "../../SupplementaryMaterials/reliability_reporting_flow_diagram.png")
```

### Validity Reporting

For validity evidence the pattern was similar. Validity evidence was reported for only, `r apa_num(sum(coded_data_original$sel_psychometric_evidence_REV != "None" & coded_data_original$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)" | coded_data_original$sel_3_REV == TRUE, na.rm = TRUE), numerals = FALSE)` (`r sprintf("%.1f", (sum(coded_data_original$sel_psychometric_evidence_REV != "None" & coded_data_original$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)" | coded_data_original$sel_3_REV == TRUE, na.rm = TRUE) / sum(coded_data_original$N_items == "multiple item measure")) * 100)`%) multiple item measures in original studies and `r apa_num(sum(coded_data_replications$sel_psychometric_evidence_REV != "None" & coded_data_replications$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)" | coded_data_replications$sel_3_REV == TRUE, na.rm = TRUE), numerals = FALSE)` (`r sprintf("%.1f", (sum(coded_data_replications$sel_psychometric_evidence_REV != "None" & coded_data_replications$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)" | coded_data_replications$sel_3_REV == TRUE, na.rm = TRUE) / sum(coded_data_replications$N_items == "multiple item measure")) * 100)`%) in replications. These proportions are slightly larger than found by Flake et al. @flakeConstructValidityValidity2022 for both original (9.3%) and replication studies (6.2%). The reported psychometric indicators we could identify were `r apa_num(sum(coded_data_original$sel_psychometric_evidence_REV == "Exploratory Factor Analysis"), numerals = FALSE)` exploratory factor analyses in the original, and `r apa_num(sum(coded_data_original$sel_psychometric_evidence_text_REV == "convergent validity" | coded_data_original$sel_psychometric_evidence_text_REV == "convergent validitiy"), numerals = FALSE)` pieces of convergent validity evidence in original studies and `r apa_num(sum(coded_data_replications$sel_psychometric_evidence_text_REV == "convergent validity" | coded_data_replications$sel_psychometric_evidence_text_REV == "convergent validitiy"), numerals = FALSE)` in replications. Psychometric validity evidence in terms of convergent, discriminant, predictive, or factorial evidence from previous studies was reported for `r apa_num(sum(coded_data_original$sel_4 == TRUE, na.rm = TRUE), numerals = FALSE)` original studies and `r apa_num(sum(coded_data_replications$sel_4 == TRUE, na.rm = TRUE), numerals = FALSE)` replications. For single-item measures, validity evidence was reported in `r apa_num(sum(coded_data_original$sel_psychometric_evidence_text_REV[coded_data_original$N_items == "1 item measure"] != ""), numerals = FALSE, zero_string = "none")` of the original studies, and only `r apa_num(sum(coded_data_replications$sel_psychometric_evidence_text_REV[coded_data_replications$N_items == "1 item measure"] != ""), numerals = FALSE)` replication protocols (in both cases evidence of convergent validity). 

```{r validity_evidence_reporting, include = FALSE, eval = FALSE}
table(coded_data_original$sel_psychometric_evidence_REV)
table(coded_data_replications$sel_psychometric_evidence_REV)

# how much psychometric validity evidence was reported
sum(coded_data_original$sel_psychometric_evidence_REV != "None" & coded_data_original$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)" | coded_data_original$sel_3_REV == TRUE, na.rm = TRUE)
sum(coded_data_replications$sel_psychometric_evidence_REV != "None" & coded_data_replications$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)" | coded_data_replications$sel_3_REV == TRUE, na.rm = TRUE)

table(coded_data_original$sel_psychometric_evidence_text_REV)
table(coded_data_replications$sel_psychometric_evidence_text_REV)


# internal
table(coded_data_original$sel_3_REV)
table(coded_data_replications$sel_3_REV)
# external
table(coded_data_original$sel_4)
table(coded_data_replications$sel_4)

sum(coded_data_original$sel_4 == TRUE, na.rm = TRUE)
sum(coded_data_replications$sel_4 == TRUE, na.rm = TRUE)


# how many existing measures were used
sum(coded_data_original$sel_existing == "True, namely:")
sum(coded_data_replications$sel_existing == "True, namely:")

# for existing measures, how many had the specific version reported
sum(coded_data_original$op_1_REV[coded_data_original$sel_existing == "True, namely:"] == TRUE, na.rm = TRUE)
sum(coded_data_replications$op_1_REV[coded_data_replications$sel_existing == "True, namely:"] == TRUE , na.rm = TRUE)


# for measures how much exploratory factor analysis validity evidence was reported
sum(coded_data_original$sel_psychometric_evidence_REV == "Exploratory Factor Analysis")
sum(coded_data_replications$sel_psychometric_evidence_REV == "Exploratory Factor Analysis")


# for measures how much convergent validity evidence was reported
sum(coded_data_original$sel_psychometric_evidence_text_REV == "convergent validity")
sum(coded_data_replications$sel_psychometric_evidence_text_REV == "convergent validity")


# for single item measures was any validity evidence reported
apa_num(sum(coded_data_replications$sel_psychometric_evidence_text_REV[coded_data_replications$N_items == "1 item measure"] != ""), numerals = FALSE)

apa_num(sum(coded_data_original$sel_psychometric_evidence_text_REV[coded_data_original$N_items == "1 item measure"] != ""), numerals = FALSE)


# if an existing measure was used, how did that affect the reporting of reliability
# and validity evidence
table(coded_data_original$sel_existing, coded_data_original$reliability_type) # reliability does appear to indicate more reporting for existing reliabilities

table(coded_data_original$sel_existing, coded_data_original$sel_psychometric_evidence_REV) # not much to go on for psychometric validity evidence

# overall not substantive enough data too highlight in the article (would require a test to evaluate)
```

## Analysis of Item Responses

### Calculated Reliability Coefficients

We could calculate Cronbach’s Alpha for `r length(levels(calculated_reliability_lab_data$g))` measures across on average `r sprintf("%.1f", mean(table(calculated_reliability_lab_data$g)))` labs, for which the required raw data were available online. The average Cronbach’s Alpha coefficient across measures was `r apa_num(mean(measure_reliability_data$alpha))` with a standard deviation of `r apa_num(sd(measure_reliability_data$alpha))`. Figure \@ref(fig:PlotAlphaDistributions) displays the distributions of the calculated Cronbach’s Alpha scores from each lab for each measure, separated by successful and unsuccessful replication, based on the meta-analytic p-value (nominal Alpha = .05) for the global replication effect size estimate retrieved from the Many Labs reports. 

```{r alpha_distributions_data_prep}
### Preparation code for the alpha distribution figure
# creating an editable copy of the calculated_reliability_lab_data
calculated_alpha_plot_data <- calculated_reliability_lab_data

# calculating the average alpha per group
calculated_alpha_plot_data$avg.alpha <- ave(calculated_reliability_lab_data$alpha, 
                                            calculated_reliability_lab_data$g)

# making replication success a Boolean, so we can order the dataset by it.
calculated_alpha_plot_data$replication_success <- ifelse(
  calculated_alpha_plot_data$replication_success == "Yes", TRUE, FALSE)

# reordering the plot from succesfully replicated to unsuccesfully replicated, and then from least to most reliable.
calculated_alpha_plot_data <- calculated_alpha_plot_data[order(-calculated_alpha_plot_data$replication_success, -calculated_alpha_plot_data$avg.alpha),]

# Removing calculated alpha's that fell below 0 to make everythign fit nicely in the plot.
calculated_alpha_plot_data <- calculated_alpha_plot_data[calculated_alpha_plot_data$alpha > 0,]

# reordering the grouping variable factor levels
calculated_alpha_plot_data$g <- fct_inorder(as.factor(calculated_alpha_plot_data$g), ordered = NA)


# adding an index to which row in the plot data the row in measure_reliability_data belongs to
measure_reliability_data$graph_index <- NA

for (i in 1:nrow(measure_reliability_data)){
  measure_reliability_data$graph_index[which(unique(calculated_alpha_plot_data$g)[i] == measure_reliability_data$g)] <- i
}

```

```{r PlotAlphaDistributions, warning = FALSE,  fig.cap = "Distributions of calculated Cronbach’s Alpha coefficients calculated for the responses on a measure at each lab location, across the nineteen measures for which the required raw data was available. Cronbach’s Alpha values that fell below 0 were excluded. The green lines indicate the meta-analytic 95% prediction interval lower and upper bound. The blue triangles indicate the reported Cronbach’s Alpha coefficient for that measure from the original article, when reported. The N column indicates the number of labs that the measure was used in. The Tau column besides the figure shows the tau heterogeneity estimate based on a meta-analysis of the calculated reliabilities for each measure. Meta-analyses for which the Q-test for heterogeneity was significant at alpha = .05 are marked by a $*$. The Diff column shows the difference between reported reliability and the average reliability calculated from the Many Labs data for the applicable measures. The reported reliabilities that fell outside the 95% quantile of calculated reliability scores are marked by a $*$."}

# plot for distribution of alpha
ggplot(calculated_alpha_plot_data, aes(x = alpha, y = g)) +
  geom_boxplot(outlier.shape = NA) +
  geom_hline(yintercept = 6.5, color = "red", size = 1) +
  geom_point(alpha = 0.1) +
  
  # adding in the number of labs per measures
  geom_text(data = measure_reliability_data, label = as.numeric(table(calculated_alpha_plot_data$g)), x = 1.1, y = measure_reliability_data$graph_index, size = 2.8) +
  
  # adding in the tau values
  geom_text(data = measure_reliability_data, label = ifelse(measure_reliability_data$QEp < .05, paste0(format(measure_reliability_data$tau, digits = 1), "*"), paste0(format(measure_reliability_data$tau, digits = 1), " ")), x = 1.22, y = measure_reliability_data$graph_index, size = 2.8) +
  
  # adding in the difference in alpha coefficients
  geom_text(data = measure_reliability_data[c(1:8, 18),], label = ifelse(measure_reliability_data$significance_reported_coefficient[c(1:8, 18)], paste0(format(measure_reliability_data$coefficient_difference[c(1:8, 18)], digits = 2), "*"), paste0(format(measure_reliability_data$coefficient_difference[c(1:8, 18)], digits = 2), " ")), x = 1.38, y = measure_reliability_data[!is.na(measure_reliability_data$coefficient_difference),]$graph_index, size = 2.8) +
  
  # setting the theme
  theme_minimal() +
  theme(legend.position = "none", plot.margin = unit(c(1, 6.5, 1, 1), "lines")) +
  
  # adding the necessary indicative texts
  annotation_custom(grob = textGrob(label = "Not Replicated", hjust = 0, gp = gpar(fontsize = 10)), ymin = 7.25, ymax = 7.25, xmin = 0.01, xmax = 0.01) +
  annotation_custom(grob = textGrob(label = "Replicated", hjust = 0, gp = gpar(fontsize = 10)), ymin = 6, ymax = 6, xmin = 0.01, xmax = 0.01) +
  annotation_custom(grob = textGrob(label = "N", hjust = 0, gp = gpar(fontsize = 12)), ymin = 20.2, ymax = 20.2, xmin = 1.08, xmax = 1.08) +
  annotation_custom(grob = textGrob(label = "Tau", hjust = 0, gp = gpar(fontsize = 12)), ymin = 20.2, ymax = 20.2, xmin = 1.16, xmax = 1.16) +
  annotation_custom(grob = textGrob(label = "Diff", hjust = 0, gp = gpar(fontsize = 12)), ymin = 20.2, ymax = 20.2, xmin = 1.32, xmax = 1.32) +
  
  coord_cartesian(xlim = c(0, 1), clip = "off") +
  
  
  # adding the blue triangles for reported reliability and green prediction intervals
  geom_point(data = measure_reliability_data, mapping = aes(x = reported_coefficient, y = graph_index), color = "blue", shape = 17, size = 3) +
  # we remove 14 & 15 lower bound because they are below 0
  geom_point(data = measure_reliability_data[-c(14, 15),], mapping = aes(x = pi.lb, y = graph_index), color = "green", shape = 124, size = 2.5) + 
  geom_point(data = measure_reliability_data, mapping = aes(x = pi.ub, y = graph_index), color = "green", shape = 124, size = 2.5) + 
  
  ylab("") +
  xlab("Cronbach's alpha") 

```

We found statistically significant indication of heterogeneity across labs in Cronbach’s Alpha for `r sum(measure_reliability_data$QEp < .05)` of the `r nrow(measure_reliability_data)` measures. However, as noted earlier, a more accurate indication of the heterogeneity can be observed in the prediction intervals. The prediction intervals generally show larger indications of heterogeneity for some of the lower reliability measures and less for higher reliability measures when compared to the Tau test results. If we compare the reported reliability in the original study to the average calculated reliability in the replications, two things stand out. The reported reliabilities were typically lower than the average calculated reliabilities, and in this sample reliabilities were reported more often in the original study if the measures had higher average calculated reliabilities (\>.80) in the replications. 

### Unidimensionality Test

```{r PlotData1Factor, warning = FALSE}
# load in the FA_list data
FA_list <- list(fa_caruso_2012, fa_husnu_2010, fa_nosek_2002_math, fa_nosek_2002_art,
                fa_anderson_2012_swl, fa_anderson_2012_pa, fa_anderson_2012_na,
                fa_giessner_2007, fa_norenzayan_2002, fa_zhong_2006, fa_monin_2001_most,
                fa_monin_2001_some, fa_cacioppo_1983_arg, fa_cacioppo_1983_nfc, 
                fa_de_fruyt_2000, fa_albarracin_2008_verb, fa_albarracin_2008_math,
                fa_shnabel_2008, fa_vohs_2008)



# create the base dataframe
unidimensionality_graph_data <- data.frame(RMSEA = NA,
                                           RMSEA_se = NA,
                                           CFI = NA,
                                           SRMR = NA,
                                           fit_chisq = NA,
                                           fit_df = NA,
                                           fit_pvalue = NA,
                                           N_factors = NA,
                                           unidimensional_check_count = NA,
                                           g = NA)

measured_variables <- c("caruso_2012", "husnu_2010", "nosek_2002_math", "nosek_2002_art", "anderson_2012_swl", "anderson_2012_pa", "anderson_2012_na", "giessner_2007", "norenzayan_2002", "zhong_2006", "monin_2001_most", "monin_2001_some", "cacioppo_1983_arg", "cacioppo_1983_nfc", "de_fruyt_2000", "albarracin_2008_verb", "albarracin_2008_math", "shnabel_2008", "vohs_2008")


# adding the info from the FA_list to our data.
for (i in 1:19){

  g = measured_variables[i]
  
  unidimensionality_graph_data <- rbind(unidimensionality_graph_data, cbind(FA_list[[i]][1:9], g))

}

# removing placeholder first row
unidimensionality_graph_data <- unidimensionality_graph_data[-1, ]

# ordering the levels of g to match the order in the reliability graph
unidimensionality_graph_data$g <- factor(unidimensionality_graph_data$g, levels = c("nosek_2002_art", "nosek_2002_math", "shnabel_2008", "husnu_2010", "norenzayan_2002", "vohs_2008", "cacioppo_1983_arg", "anderson_2012_pa", "anderson_2012_na", "giessner_2007", "anderson_2012_swl", "caruso_2012", "zhong_2006", "monin_2001_some", "cacioppo_1983_nfc", "monin_2001_most", "albarracin_2008_verb", "albarracin_2008_math", "de_fruyt_2000"), labels = c("Nosek et al. (2002), Art", "Nosek et al. (2002), Math", "Shnabel & Nadler (2008)", "Husnu & Crisp (2010)", "Norenzayan et al. (2002)", "Vohs & Schooler (2008)", "Cacioppo et al. (1983), arg", "Anderson et al. (2012), PA", "Anderson et al. (2012), NA", "Giessner & Schubert, (2007)", "Anderson et al. (2012), SWL", "Caruso et al. (2012)", "Zhong & Lijenquist (2006)", "Monin & Miller (2001), some", "Cacioppo et al. (1983), nfc", "Monin & Miller (2001), most",  "Albarracín et al. (2008), exp 5 verb", "Albarracín et al. (2008), exp 5 math", "De Fruyt et al. (2000)"))

# adding a capped of version of the N_factors variable as a category variable
N_factors_capped <- rep(NA, nrow(unidimensionality_graph_data))

N_factors_capped[unidimensionality_graph_data$N_factors == 1] <- "1"
N_factors_capped[unidimensionality_graph_data$N_factors == 2] <- "2"
N_factors_capped[unidimensionality_graph_data$N_factors >= 3] <- "3+"

unidimensionality_graph_data$N_factors_capped <- as.factor(N_factors_capped)


### meta-analyzing RMSEA
assess_undidimensionality_heterogeneity <- function(data_on_RMSEA){
  temp_beta <- NA
  temp_tau <- NA
  temp_QEp <- NA
  rma_prediction <- NA
  temp_pi.lb <- NA
  temp_pi.ub <- NA
  
  tryCatch({
    # run a random effects meta-analysis
    rma_model <- rma(yi = data_on_RMSEA$RMSEA, sei = data_on_RMSEA$RMSEA_se, 
                     method = "REML", control = list(stepadj = 0.5, maxiter = 1000))
    # estimated coefficient
    temp_beta <- rma_model$beta
    
    # extract the relevant heterogeneity information
    temp_tau <- sqrt(rma_model$tau2)
    temp_QEp <- rma_model$QEp
    
    # get prediction intervals for alpha
    rma_prediction <- predict(rma_model)
    temp_pi.lb <- rma_prediction$pi.lb
    temp_pi.ub <- rma_prediction$pi.ub

    
  }, error = function(e) {
    
  })
  
  
  return(c(temp_beta, temp_tau, temp_QEp, temp_pi.lb, temp_pi.ub, data_on_RMSEA$g[1]))
}


unidimensionality_meta_list <- tapply(unidimensionality_graph_data, unidimensionality_graph_data$g, assess_undidimensionality_heterogeneity)

# convert data output to a dataframe
unidimensionality_meta_data <- do.call(rbind.data.frame, unidimensionality_meta_list)

names(unidimensionality_meta_data) <- c("beta", "tau", "QEp", "pi.lb", "pi.ub", "g")

# add the count of significant p_values for the exact test to the meta_dataframe

count_significant_results <- function(p){
  significance <- sum(p > .05, na.rm = TRUE)
  number_of_converged <- sum(!is.na(p))
  return(c(significance, number_of_converged))
}

p_meta_list <- tapply(unidimensionality_graph_data$fit_pvalue, unidimensionality_graph_data$g, count_significant_results)

p_meta_data <- do.call(rbind.data.frame, p_meta_list)

names(p_meta_data) <- c("p_count", "converged_count")

unidimensionality_meta_data <- cbind(unidimensionality_meta_data, p_meta_data)


# added the reporting or lack of reporting of psychometric validity evidence in the original to the meta data
unidimensionality_meta_data$reported_psych_val <- coded_data_original$sel_psychometric_evidence_REV[measure_reliability_data$coded_data_index] != "None" & coded_data_original$sel_psychometric_evidence_REV[measure_reliability_data$coded_data_index] != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)" | coded_data_original$sel_3_REV[measure_reliability_data$coded_data_index] == TRUE

# adding the number of non-converged factor models
non_converged_list <- tapply(unidimensionality_graph_data$RMSEA, unidimensionality_graph_data$g, is.na)

non_converged_vec <- rep(0, 19)

for (i in 1:19){
  non_converged_vec[i] <- sum(non_converged_list[[i]])
}

# creating a string that shows the ratio of nonconverged to total
convergence_rate_vec <- paste0(unidimensionality_meta_data$converged_count, "/", unidimensionality_meta_data$converged_count + non_converged_vec)

# adding a star for De Fruyt et al. (2000)
convergence_rate_vec <- c(convergence_rate_vec[1:18], paste0(convergence_rate_vec[19], "*"))

```

Figure \@ref(fig:Plot1FactorRMSEA) shows the result of our unidimensionality tests of `r nrow(unidimensionality_meta_data)` measures. The mean obtained RMSEAs were below the threshold of .08 for `r sum(as.numeric(tapply(unidimensionality_graph_data$RMSEA, unidimensionality_graph_data$g, mean, na.rm = TRUE)) < .08, na.rm = TRUE)` of these measures (`r sprintf("%.1f", sum(as.numeric(tapply(unidimensionality_graph_data$RMSEA, unidimensionality_graph_data$g, mean, na.rm = TRUE)) < .08, na.rm = TRUE) / nrow(unidimensionality_meta_data) * 100)`%). Furthermore, the RMSEA shows considerable variation across labs, with the standard deviation ranging from `r apa_num(min(tapply(unidimensionality_graph_data$RMSEA, unidimensionality_graph_data$g, sd, na.rm = TRUE)[!is.na(tapply(unidimensionality_graph_data$RMSEA, unidimensionality_graph_data$g, sd, na.rm = TRUE))]))` to `r apa_num(max(tapply(unidimensionality_graph_data$RMSEA, unidimensionality_graph_data$g, sd, na.rm = TRUE)[!is.na(tapply(unidimensionality_graph_data$RMSEA, unidimensionality_graph_data$g, sd, na.rm = TRUE))]))` (mean SD = `r apa_num(mean(tapply(unidimensionality_graph_data$RMSEA, unidimensionality_graph_data$g, sd, na.rm = TRUE)[!is.na(tapply(unidimensionality_graph_data$RMSEA, unidimensionality_graph_data$g, sd, na.rm = TRUE))]))`).

The parallel analysis results showed inconsistency both across labs. While `r sprintf("%.1f", (sum(unidimensionality_graph_data$N_factors_capped == "1", na.rm = TRUE) / nrow(unidimensionality_graph_data)) * 100)`% of labs returned a single factor solution, `r sprintf("%.1f", (sum(unidimensionality_graph_data$N_factors_capped == "2", na.rm = TRUE) / nrow(unidimensionality_graph_data)) * 100)`% returned a two-factor, and `r sprintf("%.1f", (sum(unidimensionality_graph_data$N_factors_capped == "3+", na.rm = TRUE) / nrow(unidimensionality_graph_data)) * 100)`% returned three or more factors. The remaining analyses did not converge. The lack of convergence was also an issue for the CFA used for the RMSEA check. The CFA failed to converge for `r sprintf("%.1f", sum(is.na(unidimensionality_graph_data$RMSEA)) / nrow(unidimensionality_graph_data) * 100)`% of labs. In most cases, the convergence problems were the result of either the severe misfit of our factor models or a lack of sample size to stably estimate these models. Furthermore, the results from the exact fit tests showed varying dimensionality across labs. In total, the factor analyses converged for eighteen measures. For `r apa_num(sum((unidimensionality_meta_data$p_count[1:18] / unidimensionality_meta_data$converged_count[1:18]) >= .75), numerals = FALSE)` of these measures, there was substantive evidence for a one-factor solution (statistical fit in >75% of labs), whereas for `r sum((unidimensionality_meta_data$p_count[1:18] / unidimensionality_meta_data$converged_count[1:18]) <= .25)` measures, there was low evidence for a one-factor solution (statistical fit in <25% of labs). The evidence for a one-factor solution for the remaining measures was mixed. 

```{r Plot1FactorRMSEA, warning = FALSE, fig.cap = "Distributions of calculated RMSEA of a single-factor model fit calculated for the responses on a measure at each lab location, across the nineteen measures for which raw data was available on which a factor model could be fitted. The red horizontal line separates the replicated from the non-replicated measures. The red vertical line indicates our .08 RMSEA cutoff value. The first number in the Conv. column besides the graph shows the number of labs per measure for which the single-factor CFA converged, with second number showing the total number of labs. The Sig. Prop. column indicates the proportion of converged studies for which the 1-factor CFA exact fit test returned a statistically significant p-value at Alpha = .05. The color of each dot shows the number of factors that were selected for that measure for that lab location based on the parallel analyses. * The Conscientiousness measure used in De Fruyt et al. (2000) consisted of two items, which are too few items to fit a factor model on. The factor analysis results can be ignored for this study."}

# plot for distribution of RMSEA and exact fit test
ggplot(unidimensionality_graph_data, aes(x = RMSEA, y = g, colour = as.factor(N_factors_capped))) +
  geom_boxplot(outlier.shape = NA, colour = "black") +
  geom_hline(yintercept = 6.5, color = "red", size = 1) +
  geom_vline(xintercept = 0.08, color = "red", size = 1) +
  geom_point(alpha = 0.3) +
  
  
  # adding in the convergence rate
  geom_text(data = unidimensionality_meta_data, label = format(convergence_rate_vec), x = 0.51, y = 1:19, size = 2.8, colour = "black") +
  
  # adding in the proportion of significant p-values
  geom_text(data = unidimensionality_meta_data, label = format(c(round(unidimensionality_meta_data$p_count[1:18] / unidimensionality_meta_data$converged_count[1:18], 2), "")), x = 0.58, y = 1:19, size = 2.8, colour = "black") +
  
  
  # setting the theme
  theme_minimal() +
  theme(legend.position = "bottom", plot.margin = unit(c(2, 5, 1, 1), "lines")) +
  
  
  # adding the necessary indicative texts
  annotation_custom(grob = textGrob(label = "Not Replicated", hjust = 0, gp = gpar(fontsize = 10)), ymin = 7.25, ymax = 7.25, xmin = 0.33, xmax = 0.33) +
  annotation_custom(grob = textGrob(label = "Replicated", hjust = 0, gp = gpar(fontsize = 10)), ymin = 6, ymax = 6, xmin = 0.37, xmax = 0.37) +
  annotation_custom(grob = textGrob(label = "1-Fac\nConv.", hjust = 0, gp = gpar(fontsize = 11)), ymin = 20.9, ymax = 20.9, xmin = 0.48, xmax = 0.48) +
  annotation_custom(grob = textGrob(label = "Sig.\nProp.", hjust = 0, gp = gpar(fontsize = 11)), ymin = 20.9, ymax = 20.9, xmin = 0.55, xmax = 0.56) +
  
  coord_cartesian(xlim = c(0, 0.45), clip = "off") +
  
  # we remove 7 & 15 lower bound because they are below 0
  # geom_point(data = unidimensionality_meta_data[c(1:6, 8:14, 16:17),], mapping = aes(x = pi.lb, y = c(1:6, 8:13, 15:17) ), color = "green", shape = 124, size = 2.5) + 
  # geom_point(data = unidimensionality_meta_data, mapping = aes(x = pi.ub, y = 1:19), color = "green", shape = 124, size = 2.5) + 
  
  scale_colour_manual(name = "N Factors (Parallel Analysis)", values = c("#0049b8", "#02bd8a", "#f2e30f"), na.value = "#000000") + 
  guides(colour = guide_legend(override.aes = list(alpha = 1))) +
  ylab("") +
  xlab("RMSEA (unidimensional CFA model)") 

```

Figure \@ref(fig:Plot1FactorSRMRCFI) shows the SRMR, and CFI fit indices for the same single factor model as was used to estimate the RMSEA and exact fit test. We found that, the mean SRMR was below .08 for `r apa_num((sum(tapply(unidimensionality_graph_data$SRMR, unidimensionality_graph_data$g, mean, na.rm = TRUE) < .08, na.rm = TRUE)), numerals = FALSE)` measures `r sprintf("%.1f", sum(tapply(unidimensionality_graph_data$SRMR, unidimensionality_graph_data$g, mean, na.rm = TRUE) < .08, na.rm = TRUE) / length(levels(unidimensionality_graph_data$g)) * 100)`%, while the mean CFI was above .90 for `r apa_num((sum(tapply(unidimensionality_graph_data$CFI, unidimensionality_graph_data$g, mean, na.rm = TRUE) > .90, na.rm = TRUE)), numerals = FALSE)` measures `r sprintf("%.1f", sum(tapply(unidimensionality_graph_data$CFI, unidimensionality_graph_data$g, mean, na.rm = TRUE) > .90, na.rm = TRUE) / length(levels(unidimensionality_graph_data$g)) * 100)`%. The mean standard deviation of the SRMR fit index across measures is `r apa_num(mean(tapply(unidimensionality_graph_data$SRMR, unidimensionality_graph_data$g, sd, na.rm = TRUE), na.rm = TRUE))`. For CFI this was `r apa_num(mean(tapply(unidimensionality_graph_data$CFI, unidimensionality_graph_data$g, sd, na.rm = TRUE), na.rm = TRUE))`. 

```{r Plot1FactorSRMRCFI, warning = FALSE, fig.cap = "Distributions of calculated SRMR and calculated CFI of a single-factor model fit calculated for the responses on a measure at each lab location, across the nineteen measures for which raw data was available on which a factor model could be fitted. The left figure shows the SRMR result, where the red vertical line indicates our .08 RMSR cutoff value. The right figure shows the CFI results, where the red vertical line indicates our .90 CFI cutoff value. In both figures the red horizontal line separate the replicated from the non-replicated measures."}

# plot for distribution of SRMR
SRMR_plot <- ggplot(unidimensionality_graph_data, aes(x = SRMR, y = g, colour = as.factor(N_factors_capped))) +
  geom_boxplot(outlier.shape = NA, colour = "black") +
  geom_point(alpha = 0.3) +
  geom_hline(yintercept = 6.5, color = "red", size = 1) +
  geom_vline(xintercept = 0.08, color = "red", size = 1) +

  # setting the theme
  theme_minimal() +
  theme(legend.position = "bottom", plot.margin = unit(c(1, .5, 1, 1), "lines")) +
  scale_x_continuous(breaks = round(c(0, 0.1, 0.2),1)) +
  
  # adding the necessary indicative texts
  annotation_custom(grob = textGrob(label = "Not Replicated", hjust = 0, gp = gpar(fontsize = 8)), ymin = 7.25, ymax = 7.25, xmin = 0.18, xmax = 0.18) +
  annotation_custom(grob = textGrob(label = "Replicated", hjust = 0, gp = gpar(fontsize = 8)), ymin = 6, ymax = 6, xmin = 0.18, xmax = 0.18) +

  coord_cartesian(xlim = c(0, 0.2), clip = "off") +

  scale_colour_manual(name = "N Factors (Parallel Analysis)", values = c("#0049b8", "#02bd8a", "#f2e30f"), na.value = "#000000") + 
  guides(colour = guide_legend(override.aes = list(alpha = 1))) +
  ylab("") +
  xlab("SRMR (unidimensional\nCFA model)") 


# plot for distribution of CFI
CFI_plot <- ggplot(unidimensionality_graph_data, aes(x = CFI, y = g, colour = as.factor(N_factors_capped))) +
  geom_boxplot(outlier.shape = NA, colour = "black") +
  geom_hline(yintercept = 6.5, color = "red", size = 1) +
  geom_vline(xintercept = 0.9, color = "red", size = 1) +
  geom_point(alpha = 0.3) +

  # setting the theme
  theme_minimal() +
  theme(legend.position = "none", plot.margin = unit(c(1, .5, 1, 1), "lines"), axis.text.y = element_blank()) +
  scale_x_continuous(breaks = round(c(0, 0.2, 0.4, 0.6, 0.8, 1),1)) +
  
  coord_cartesian(xlim = c(0, 1), clip = "off") +
  
  scale_colour_manual(name = "N Factors (Parallel Analysis)", values = c("#0049b8", "#02bd8a", "#f2e30f"), na.value = "#000000") + 
  guides(colour = guide_legend(override.aes = list(alpha = 1))) +
  ylab("") +
  xlab("CFI (unidimensional\nCFA model)") 


(SRMR_plot + CFI_plot)

```

Taken together, our factor analytic tests for unidimensionality showed that while almost all measures were unidimensional in at least one of the labs they were used in, very few were consistently unidimensional. The severity of non-unidimensionality depends on which check is referenced. Based on the RMSEA we would conclude that in most labs the measures were not unidimensional, whereas based on the model fit test we would conclude that in most they were unidimensional. Based on the CFI, SRMR, and parallel analysis the proportion of labs where the measure was unidimensional is around half. If we compile the results from all five unidimensionality checks, we observe that at least one of the five unidimensionality checks was passed for `r sprintf("%.1f", (sum(unidimensionality_graph_data$unidimensional_check_count >= 1, na.rm = TRUE) / sum(!is.na(unidimensionality_graph_data$unidimensional_check_count))) * 100)`% of the `r sum(!is.na(unidimensionality_graph_data$unidimensional_check_count))` labs with converged factor models, at least two for `r sprintf("%.1f", (sum(unidimensionality_graph_data$unidimensional_check_count >= 2, na.rm = TRUE) / sum(!is.na(unidimensionality_graph_data$unidimensional_check_count))) * 100)`%, at least three for `r sprintf("%.1f", (sum(unidimensionality_graph_data$unidimensional_check_count >= 3, na.rm = TRUE) / sum(!is.na(unidimensionality_graph_data$unidimensional_check_count))) * 100)`%, at least four for `r sprintf("%.1f", (sum(unidimensionality_graph_data$unidimensional_check_count >= 4, na.rm = TRUE) / sum(!is.na(unidimensionality_graph_data$unidimensional_check_count))) * 100)`%, and all five for `r sprintf("%.1f", (sum(unidimensionality_graph_data$unidimensional_check_count == 5, na.rm = TRUE) / sum(!is.na(unidimensionality_graph_data$unidimensional_check_count))) * 100)`%. The mean number of unidimensionality check passed was `r sprintf("%.2f", mean(unidimensionality_graph_data$unidimensional_check_count, na.rm = TRUE))`.

# Discussion

Valid and reliable measurement is crucial to robustly study psychological phenomena, but previous research has highlighted common substandard measurement and poor reporting of measures in psychological studies. In this study, we extracted measurement reporting information from `r nrow(coded_data_replications)` measures, reported in original and replication studies from the Many Labs projects, and calculated indices for reliability and unidimensionality based on available item response data. We found that information needed to reconstruct the measure was rarely fully reported, especially in original studies. Additionally, evidence on reliability and validity of measures was often not reported in these sets of impactful publications. Finally, recalculated Cronbach’s Alpha and unidimensionality indices showed inconsistent support for reliability and presumed factorial structures of measures across labs. Beyond exposing poor reporting of how measurement had taken place, our study of large-scale replication projects highlighted that many measures show varying reliabilities across labs and that many supposedly unidimensional measures fail to meet the requirement of unidimensionality as a core requisite of construct validity.

As in Flake et al. [@flakeConstructValidityValidity2022], many studies lacked sufficient information to fully reconstruct the measure. In original studies, administration format and procedure was not properly described for about one-third of applicable measures; the number of items and response options were unclear for roughly one-eighth; and details on how an index was calculated, item recoding, and example items were omitted for just under half. One notable difference compared to Flake et al. [@flakeConstructValidityValidity2022] is that overall replications provided more complete and transparent measurement reporting than original research. We believe that this is in part due to the structured format of the Many Labs protocols, which also sometimes included specific sections for declaring deviations from the original methodology.

About two-thirds of the measures in replications were modified in some way. Given how common modifications are, it is important to report them explicitly and document reliability and validity in replications. While minor differences are inevitable, substantial changes call into question whether these studies can still be considered “direct” replications. For example, the conscientiousness measure used for the replication of De Fruyt et al. [@defruytCloningersPsychobiologicalModel2000] was reduced from the original NEO-PI-R 48-item measure to a two-item measure in the replication. It is highly unlikely that a measure with such a significant change would have a comparable validity and reliability to the original.

Corroborating earlier meta-research [@beckmanHowReliableAre2004; @barryValidityReliabilityReporting2014; @flakeConstructValidationSocial2017; @maireadshawMeasurementPracticesLargescale2020; @flakeConstructValidityValidity2022], we found that most measures were used in original and replication studies without reporting a reliability coefficient or validity evidence in the form of results from a factor analysis or convergent validity. We note that around half of the measures in our sample of studies were single item measures. While it is possible to check the reliability and validity of single item measures, these methods often require multiple measures or studies to allow for validation [@leppinkWeNeedMore2017; @sarstedtSelectingSingleItems2016], an effort that is rarely taken. For example, only `r sum(coded_data_replications$sel_psychometric_evidence_text_REV[coded_data_replications$N_items == "1 item measure"] != "")` of the `r sum(coded_data_replications$N_items == "1 item measure")` single-item measures in our sample of replication studies and `r apa_num(sum(coded_data_original$sel_psychometric_evidence_text_REV[coded_data_original$N_items == "1 item measure"] != ""), numerals = FALSE, zero_string = "none")` in the original studies reported any validity evidence, in both cases convergent validity. Even among multiple item measures, reporting issues persisted. Here we observed that reliability and validity were more poorly reported in replications than in original studies. This could be because replications implicitly defer the responsibility of demonstrating the validity of the measure to the original study. However, as we noted before, and as our reliability and unidimensionality analyses show, a measure’s reliability and factorial structure can vary across contexts. Because measurement problems could affect replication outcomes, it is important to report this information for replication studies as well. However, since the replication protocols were written before data collection, it is understandable that this information was not included there. Still, we were unable to find these details in the available supplementary materials or in the Many Labs reports.

It is concerning that most studies neither discussed nor studied the reliability of their measures by means of measures of internal consistency and the validity by means of checks of convergent, discriminant, or predictive validity or the factorial nature of their measures. Especially given that our item response analysis showed that the reliability and unidimensionality often varied considerably across samples. While some measures did show consistently high reliability and stable unidimensionality, for a similar number of measures, both were low. Most measures however fell somewhere in between, such that the same measure’s reliability and unidimensionality met standards in some of the labs, while they were clearly below par in others. We also noted that some measures had convergence issues. Overall the inconsistent fit — and in extreme cases lack of convergence — of the unidimensional factor model suggests either that the measures were not truly unidimensional (and thus not construct valid as a measure of one targeted latent variable in this  context) or that our oversimplified factor model was inappropriate for the data. In addition to these potential explanations, low sample size and few items --- common in the replication measures --- likely contributed to the lack of convergence. Taken together, our results should not be interpreted as validity assessments of individual measures, which is beyond the scope of this article and inconsistent with our analytical approach. Rather, our findings indicate that unidimensionality --- as a prerequisite for construct validity --- is not universally met across labs, casting doubt on the construct validity of the measures included in our sample.

We observed patterns that may indicate bias in measurement reporting. If an original study reported a Cronbach’s Alpha, the corresponding replications typically showed relatively high reliability. Meanwhile, if no Cronbach’s Alpha was reported in the original study, the reliability in the replications was often relatively low. This suggests that authors may preferentially report reliability when it meets an acceptable threshold, consistent with prior research showing an excess of reported alphas around the commonly acceptable threshold of .70 [@husseyAberrantAbundanceCronbachs2023]. Combined with our evidence of inconsistent reliability and validity, such reporting bias can give readers a false sense of confidence in the measures.

Our intent is not to specifically criticize the Many Labs replications nor these original studies, but rather use them as illustrative examples. Regardless, we would argue that full transparent measurement reporting is a responsibility of both replications and original studies. Otherwise, based on our results and the results of earlier research [@maireadshawMeasurementPracticesLargescale2020; @husseyAberrantAbundanceCronbachs2023; @maassenDireDisregardMeasurement2025], there is reason to doubt the reliability and validity of many psychological measures, especially when used in a new context and not accompanied by transparent reporting or results of standard psychometric tests for reliability and factorial composition that researchers could apply readily. Important information on the reliability and validity of measures across contexts is often not reported despite the real possibility that the measures fail to meet basic psychometric standards that are relevant in understanding the robustness of psychological phenomena in different contexts [@maassenDireDisregardMeasurement2025]. Even though validating measures is challenging and resource intensive, its necessity cannot be ignored. Use of unvalidated measures might invalidate substantive interpretations drawn from both original and replication studies.

## Limitations & Future Research

The original studies and replications of the Many Labs projects may not be representative of typical original or replication research in psychology. Although we maintain the Many Labs projects are a relevant and high-quality source of studies, it may represent a standard that is not common throughout the field. Future research may wish to look at more representative replications and replications of more recent research, as well as a broader scope of original studies.

Another limitation inherent in our data is that replication protocols are typically shorter than research articles, which may prevent full reporting of measurement details and complicate direct comparisons of reporting practices. We believe, however, that protocols and articles remain largely comparable for three reasons. First, research articles are also often restricted in the space available for reporting measurement details [@gardinerEditorialMethodsPapers2019; @zogmaisterAssessingTransparencyMethods2024]. Second, the revised protocol allowed certain items --- such as reporting example items --- to be fulfilled via supplementary materials, similar to good practice in articles. Third, beyond the protocols, few other files contained measurement details, meaning the protocol represented most of the available information, just as an article does for the original study. In other words, if a detail was missing from the protocol, it was unlikely to be found elsewhere. A difference between research articles and protocols that was not resolved was that the protocols were written before data collection. This means the protocols cannot report measurement information derived from the data, such as reliability coefficients and psychometric validity information. Future research could examine replications that do provide such data to allow more direct comparisons.

There were additional sources of information we could have included. We could have used data from original studies to recalculate reliability and validity indicators to compare with the replication results. However, it is unlikely that a substantial number of studies would have shared their data (many were conducted before the OSF and other Open Science initiatives were launched). This is likely different for more recent research [@hardwickeEstimatingPrevalenceTransparency2022; @hamiltonPrevalencePredictorsData2023a]. 

We originally planned to formally test the difference in reported reliability between original and replication studies, and the relation between reliability and replication outcomes. However, due to the small number of reported reliabilities and measures for which reliability could be calculated, we were unable to perform these tests with sufficient power. Importantly, this limitation itself highlights a key finding: the inability to perform our preregistered analyses underscores the widespread lack of measurement information reporting in our sample.

## Recommendations

We see two key issues to address for better measurement practices: the common use of measures without established validity and the lack of transparent measurement reporting. Unfortunately, fixing these issues requires considerable time and resources. Fully validating a measure requires a lengthy and resource intensive research program spanning across years, a practice that understandably occurs only rarely beyond a handful of common (clinical and personality) questionnaires [@anvariFragmentedFieldConstruct2025]. Meanwhile, improving transparency in reporting is an issue that many have tried to address for other aspects of academic articles such as preregistration deviations [@willrothBestLaidPlans2024], and constraints on generality [@simonsConstraintsGeneralityCOG2017], but widespread success has remained elusive. Despite these challenges, we remain optimistic that even small interventions can accelerate improvement. First of all, these two issues are interdependent. Improving transparency in measurement reporting can in turn facilitate validation efforts. Moreover, even incremental progress, such as establishing validity for a subset of frequently used measures, can meaningfully enhance the overall quality and credibility of psychological research.

Thus, our first recommendation is that measurement reporting standards should be given greater prominence in psychological science. We found that reporting information on reliability and validity was the exception rather than the rule, and even basic information such as how many items were in the measure was reported with little consistency in our sample. The American Educational Research Association @americaneducationalresearchassociationStandardsEducationalPsychological2014 guidelines exist to guide researchers in the creation and validation of scales, but not how to report the measurement details when in use. The APA guidelines [@AmericanPsychologicalAssociation2020] do address many of the same reporting practices we assessed, however our findings and those of other research shows that uptake of these standards is still minimal. One aspect where reporting standards has seen some changes is in statistical reporting [@hardwickeStatisticalGuidanceAuthors2023]. Statistical reporting standards and guidelines have received a substantial push, including recommendations from the American Statistical Association [@wassersteinASAStatementPValues2016]. The uptake of statistical reporting guidelines is even seen among journals. Although, in practice guidelines on statistical reporting vary in their content and enforcement [@hardwickeStatisticalGuidanceAuthors2023]. Still, any improvement in measurement reporting standards is valuable. Therefore, a similar high-profile push for transparent measurement reporting would be of great importance and at least as warranted as efforts to improve scientific robustness or statistical reporting, since measurement data form the foundation of nearly all empirical quantitative research in psychology. 

Moreover, researchers should report and evaluate measures using more informative indicators. Cronbach’s Alpha alone provides limited information the scale quality and relies on strong assumptions [@cortinaWhatCoefficientAlpha1993; @sijtsmaUseMisuseVery2009]. Factor-analytic evidence, such as an assessment of unidimensionality, and McDonald’s Omega offer more informative alternatives and should be reported alongside or instead of Cronbach’s Alpha.

Our second recommendation is that the scientific community should focus on creating, reporting, and reusing measures in a way that allows systematic validation. To do so, measures first need to be reusable, which requires transparent measurement reporting. Researchers should then reuse existing validated measures rather than creating new, unvalidated ones. To support this, Elson et al. @elsonPsychologicalMeasuresArent2023 proposed an open repository of measurement protocols to facilitate the discovery of measures and building an evidence base. They also recommended that journals should implement the Standardisation Of BEhavior Research (SOBER) guidelines to address flexibility and norming in measurement, ensuring comparability across studies. Finally, when a measure is reused across different context, its reliability and validity should be assessed across multiple occasions to determine in which contexts, if any, it is valid.

Finally, we recommend that researchers seeking to replicate a study first evaluate the measurement of the original study before proceeding. Reliable and valid measurement is essential for an informative replication. If the original measures are unreliable, discrepancies between the original and replication results become more likely, thereby reducing the interpretability and value of both studies. Moreover, repeating a study that relies on invalid measures does little to advance substantive knowledge and instead risks perpetuating misleading or meaningless findings. Furthermore, to make meaningful replication possible, original studies must report sufficient measurement details to allow others to reconstruct the instruments and procedures used. Without such transparency, it is impossible to know whether differences between the original and replication studies reflect true effects or merely differences in measurement. When the original measures are unreliable, invalid, or insufficiently documented, we recommend that researchers instead use their resources to conduct a replication of a study with reliable, valid and well-documented measurement. When replicating another study is not an option, we advise the replicating researcher to first attempt a conceptual replication using a validated measurement. Afterwards, a direct replication can be performed based on the conceptual replication to further assess the robustness of effects or associations.

# Conclusion

Cumulative knowledge on psychological phenomena starts with our ability to accurately measure the constructs of interest. For this we need valid and reliable measurement. Yet, in our sample of Many Labs replications and original studies, reliability and construct validity evidence were rarely reported and, based on our analyses, seldom sufficient across contexts to assume that constructs were measured as intended. Furthermore, poor transparency in measurement reporting hinders the reuse of existing measures. Changes in both the use and reporting of measurement are necessary; without them, psychological effects lack the measurement foundation needed to justify their connection to true phenomena. Fortunately, even small improvements in reporting, data and materials sharing, and normalizing measurement validation can spark the proliferation of validated measurement.


## Conflicts of Interest

The author(s) declare that there were no conflicts of interest with respect to the authorship or the publication of this article.

## ORCID iDs

Cas Goos <https://orcid.org/0009-0005-3792-4148>

Marjan Bakker <https://orcid.org/0000-0001-9024-337X>

Jelte M. Wicherts <https://orcid.org/0000-0003-2415-2933>

Michèle B. Nuijten <https://orcid.org/0000-0002-1468-8585>

## Funding

The preparation of this article was supported by the Vici grant VI.C.221.100 awarded to Jelte M. Wicherts from the Dutch Research Council (NWO).

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
