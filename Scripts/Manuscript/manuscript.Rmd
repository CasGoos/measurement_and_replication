---
title             : "Assessing Reliable and Valid Measurement as a Prerequisite for Informative Replications in Psychology"
shorttitle        : "Measurement and Informative Replications"

author: 
  - name          : Cas Goos
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Professor Cobbenhagenlaan 125, 5037 DB, Tilburg, The Netherlands"
    email         : "c.goos@tilburguniversity.edu"

  - name          : Marjan Bakker
    affiliation   : "1"

  - name          : Jelte M. Wicherts
    affiliation   : "1"

  - name          : Michèle B. Nuijten
    affiliation   : "1"


authornote: |
  The authors made the following contributions. CG: Conceptualization, Data curation, Formal Analysis, Investigation, Methodology, Project Administration, Software, Visualization, Writing - Original Draft Preparation, Writing - Review & Editing; MB: Conceptualization, Supervision, Writing - Review & Editing; JW: Conceptualization, Supervision, Writing - Review & Editing; MN: Conceptualization, Project Administration, Supervision, Validation, Writing - Review & Editing.


abstract: |
  For a replication to be informative, measurement should be reliable and valid in both original and replication studies. 

keywords          : "reliability, construct validity, measurement, reporting, questionable measurement practices, replications, informative replications, psychology"
wordcount         : "1"

bibliography      : ["r-references.bib", "references.bib"]

floatsintext      : yes
linenumbers       : no
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

header-includes:
  - | 
    \makeatletter
    \renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-1em}%
      {\normalfont\normalsize\bfseries\typesectitle}}
    
    \renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-\z@\relax}%
      {\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
    \makeatother
  - \renewcommand\author[1]{}
  - \renewcommand\affiliation[1]{}
  - \authorsnames[1, 1, 1, 1]{Cas Goos, Marjan Bakker, Jelte M. Wicherts, Michèle B. Nuijten}
  - \authorsaffiliations{{Department of Methodology and Statistics, Tilburg School of Social and Behavioral Sciences, Tilburg University, Tilburg, NL.}}

csl               : "`r system.file('rmd', 'apa7.csl', package = 'papaja')`"
documentclass     : "apa7"

classoption       : man
output            : papaja::apa6_pdf
knit              : worcs::cite_all
---

```{r setup, include = FALSE}
# loading R libraries
library(papaja)
library(worcs)
library(lavaan)
library(psych)
library(metafor)
library(forcats)
library(ggplot2)
library(grid)


# loading script with helper functions
# source(file = "../helper_functions.R") !!!needs to still be created

# Code below loads all data. The raw data was loaded within the 'prepare_data.R script.
load_data()

# the code below removes the raw data to preserve disk space.
# If you want to rerun the data cleaning and preparations steps, DO NOT run
# this code.
# If you want to only rerun the analyses, YOU CAN run the code below.
rm(coded_data_initial_raw, coded_data_revised_raw, coded_data_vignette_raw,
   data_2.10.1, data_2.12, data_2.15, data_2.19.1, data_2.2, data_2.20, 
   data_2.23, data_2.3, data_2.4.1, data_2.4.2, data_2.8.2, data_3.5, data_5.1,
   data_5.4, data_5.5, data_5.7, data_5.9.1, data_ml1, data_ml3)

# creates a reference list for all used R packages and the installed R version 
# (does not include Rstudio)
r_refs("r-references.bib")
```

<!-- altering latex defaults to get better figure and table placement -->

\renewcommand{\arraystretch}{0.7}

<!-- reducing the line spacing within tables -->

\renewcommand{\topfraction}{.8}

<!-- max fraction of page for floats at top -->

\renewcommand{\bottomfraction}{.8}

<!-- max fraction of page for floats at bottom -->

\renewcommand{\textfraction}{.15}

<!-- min fraction of page for text -->

\renewcommand{\floatpagefraction}{.8}

<!-- min fraction of page that should have floats .66 -->

\setcounter{topnumber}{3} <!-- max number of floats at top of page -->

\setcounter{bottomnumber}{3} <!-- max number of floats at bottom of page -->

\setcounter{totalnumber}{4} <!-- max number of floats on a page -->

<!-- remember to use [htp] or [htpb] for placement -->

```{r analysis-preferences}
# Seed for random number generation
set.seed(26052025)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

```{r helper_functions}
##### PUT THIS ALL IN A SEPARATE R SCRIPT FILE.

# Function for storing data openly in the intermediate data folder
saving_to_intermediate_data <- function(data){
  open_data(data = data, filename = paste0(paste0(
    "Data/IntermediateData/", deparse(substitute(data))), ".csv"),
    codebook = NULL, value_labels = NULL) 
}

# Function for storing data openly in the analysis data folder
saving_to_analysis_data <- function(data){
  open_data(data = data, filename = paste0(paste0(
    "Data/AnalysisData/", deparse(substitute(data))), ".csv"),
    codebook = NULL, value_labels = NULL) 
}


# function for changing QMP data to their ratio equivalent
Create_QMP_descriptive_case <- function(practice_name, original_coded_var, replication_coded_var){
  # add practice name to first row
  Practice <- practice_name
  
  # Sum all QMP/GMP (Good Measurement Practice), keeping NA items out of
  # the equation
  original_n_QMPs <- sum(original_coded_var == "FALSE", na.rm = TRUE)
  replication_n_QMPs <- sum(replication_coded_var == "FALSE", na.rm = TRUE)
  
  original_n_GMPs <- sum(original_coded_var == "TRUE", na.rm = TRUE)
  replication_n_GMPs <- sum(replication_coded_var == "TRUE", na.rm = TRUE)
  
  # calculating the N of applicable items for originals
  N_applicable_original <- original_n_QMPs + original_n_GMPs
  
  # checking that we are not dividing by 0, and then calculating the ratio of 
  # QMPs for applicable items for originals
  if(N_applicable_original != 0){
    QMP_percentage_original <- round(original_n_QMPs / (N_applicable_original), 2)
  } else{
    QMP_percentage_original <- 0
  }                               
  
  # calculating the N of applicable items for replications
  N_applicable_replication <- replication_n_QMPs + replication_n_GMPs
  
  # checking that we are not dividing by 0, and then calculating the ratio of 
  # QMPs for applicable items for replications
  if(N_applicable_replication != 0){
    QMP_percentage_replication <- round(replication_n_QMPs / (N_applicable_replication), 2)
  } else{
    QMP_percentage_replication <- 0
  }     
  
  # calculating the Phi coefficient to estimate the relation between variables
  Phi <- phi(matrix(c(original_n_QMPs, original_n_GMPs, replication_n_QMPs, 
                      replication_n_GMPs), nrow = 2, byrow = TRUE))
  
  return(c(Practice, QMP_percentage_original, N_applicable_original, QMP_percentage_replication, N_applicable_replication, Phi))
}


```


<!-- Introduction -->

In psychological research we are often interested in establishing a robust effect involving one or more latent constructs (e.g., IQ and conscientiousness) that describes a psychological phenomenon (like increased conscientiousness causing a person to develop higher IQ). However in the wake of the replication crisis the robustness of effects in psychology has been called into question. The Reproducibility Project: Psychology [RPP\; @osc2015estimating] and other large-scale replication projects [@klein2014investigating; @ebersole2016many; @klein2018many; @klein2022many; @ebersole2020many] and typically yielded smaller effects in replications compared to original studies. Replicability is one of the key ways to establish the robustness of an effect (Nosek RRR & …). If an effect does not replicate, it is likely not robust. Numerous factors have been proposed to explain why the original effects did not appear robust, including questionable research practices [@simmons2011false; @john2012measuring; @cumming2014new; @wicherts2016degrees], questionable reporting practices [@bakker2011mis; @nuijten2016prevalence], biased publication decisions [@sterling1959publication; @bakker2012rules; @giner2012science], and more.

These replication efforts and the subsequent lines of research have been of instrumental value in determining the lack of robustness in effects, uncovering what the causes of this lack of robustness are, and spearheading initiatives like … to help improve the robustness of effects. However, robustness was only one aspect of the goal highlighted at the beginning of this article, the other aspect, the fact that the effect should describe a true psychological phenomenon has received relatively less attention, although this is changing (SOME CITATIONS). 

In this article, we contribute to the rising research investigating the soundness of measurement in research. After all, a true psychological phenomenon is requires accurate measurement on the relevant constructs. Our aim is to create a greater understanding of the extent to which this. We provide a descriptive account of the measurement reporting practices in the Many Labs replications and related original research, in tandem with the measure’s reliability and internal structure analyzed from the accompanying openly available data.


## Measurement Quality

To understand a psychological phenomenon you need to be able to accurately capture the relevant constructs. While this premise underlines the core idea behind the use of measures for psychological constructs, it is difficult to guarantee in practice. In general, there are two aspects of a measurement between which we distinguish: validity and reliability. What complicates things is that there are multiple aspects to validity and multiple ways to assess these aspects of validity and reliability. So a researcher needs to know which type of assessment for validity and reliability is important for a measure, and on top of that it can be a complicated task to establish validity and reliability.

Validity is an umbrella term for many different types of validity (construct, predictive, content, internal, external, etc.) and applies to (nearly) all aspects of the research design (CITATIONs INCLUDING VAZIRE et al.). For the purposes of this research we focus on construct validity of the applied measurement, which includes the measure and some aspects of the research design involving the administration of the measure. Construct validity refers to … . It is key to (relevant for nearly all measurement to some extent, because …) … . Evaluating it requires … and …, which in practice means … and … . !!!make sure that this section highlights something about construct validity relevant for our research that most psychologists may not actively realize.!!!

Reliability estimates provide an indication of how consistent the responses on a measurement are. …something about what this consistency is over [might relate to the different types: in CTT it is the correlation between scores on two equivalent forms of the test; but we are more focused on consistency as a basic requirement for an essentially valid measure, because no reliability means no validity]. Reliability is a critical first step to obtaining credible findings. Because if the scores on a measure are not consistent with themselves, it is unlikely that any effect associated with the measure can credibly be established. Psychometrics offers various ways to assess reliability empirically [@nunnally1978overview; @mellenbergh2011conceptual], … test-retest … interrater … internal consistency (this in turn relates back to construct validity)…. [something about the fact that reliability is sample specific and not measurement specific in nearly all cases of us assessing it]


## Measurement Reporting

To summarize the previous section, the measurement of a study need to be both valid and reliable to be able to assess the phenomenon of interest. However, recent research has shown that the reliability and validity can often not be discerned from the reported information. Questionable Measurement Practices (QMPs) raise doubts about a measurement’s validity [@flake2020measurement] and have been coined as a term analogous to Questionable Research Practices [QRPs\; @john2012measuring]. QMPs range from lack of transparency and unclear motivation in choice of measure to poor justification for modifications of a measure and procedure of an existing measure [@flake2020measurement]. 

@flake2022construct documented QMPs among 100 replications and their respective original articles from the RPP [@osc2015estimating]. They coded the number of measures, the number of items in the measure, and the information that was reported describing the measure and justifying its use. They observed limited reporting of evidence demonstrating the reliability and validity of the measurement. Furthermore, auxiliary information on the number of items, the response format, the scoring of the scale, was also not reported for several measures in the RPP. Furthermore, only eight of the 40 translated scales contained validity evidence for the translated version of the scale. Evidence showing that this and other modifications between original and replication did not invalidate the measurement was rare. 

!!!Especially worrisome because @shaw2020measurement shows that when running factor analyses on the available data the structural evidence does not always show a good construct validity, so that the lack of reported validity evidence may be hiding psychometrical skeletons in the closet; Reliability something … [Hussey’s work]. !!!

Further findings by  @flake2022construct and others [@flake2017construct; @shaw2020measurement] also show that QMPs not only limit the reported validity of a measurement in a study, the lack of information also creates challenges for replicating researchers. If it is not clear what items were used, how they were scored, and how they were administered, the reconstructed measure used in the replication may be assessing the relevant construct(s) differently, or even assess different construct(s). If different construct(s) are assessed, the replication may not be a test of the same phenomenon as the original. If constructs are assessed differently, the estimated effects in the replication cannot easily be substantially compared to the effects in the original study. In either case, the replication is hardly informative with regards to the phenomenon of interest. Given replication’s importance in cumulative knowledge gain in science, this is worrisome.


## Research Aim

Our aim is to further investigate the reliability, validity, and measurement reporting practices in the Many Labs replications and related original research. We provide a descriptive account of the measurement reporting practices in tandem with the measure’s reliability and validity evidence, obtained from the openly available data from the Many Labs replication projects.

Existing literature has looked at reported reliability, validity, and general measurement features in both original and replication studies before. Previous studies have also recalculated reliabilities from reported information and reanalyzed the validity from original data. Our research provides a unique contribution by combining the reported measurement information in original and replication studies, with reliability and validity re-analyses performed on openly available data from those replication studies, with the added benefit of having data across multiple labs so that variation in reliability and validity across multiple instances can be investigated. 

# Disclosures ???Where & How???

# Method

## Data Source
The data source is the Many Labs replication projects. These were a series of high profile replication projects wherein multiple psychological studies considered relevant by the community were chosen to be replicated across multiple labs over the world. Practices encountered with measurement in this sample of original articles and replications are a representative reflection of these practices more broadly.

Furthermore, because each study was replicated across multiple labs and the data collected from each lab is publicly available, we can calculate reliability coefficients and perform factor analyses for each measurement across multiple data points and investigate the variance in these estimates.

## Unit of Analysis
Our unit of analysis is a measure of a single psychological construct within a replication protocol/article. Multiple psychological constructs could be measured per original study or replication. We used the replication protocols to identify these measures. We did not include acquiescence bias checks, manipulation checks, pilot test measures, and measures added for exploratory analyses.

## Data Collection
The data consists of three main sources: replication datasets, replication protocols, and original study articles. We retrieved the data of Many Labs 1, 2, 3, & 5 [@klein2014investigating; @ebersole2016many; @klein2018many; @ebersole2020many] from their respective OSF pages. Many Labs 4 [@klein2022many] was excluded, as there was no publicly available replication protocol. Additionally, the replication of [@crosby2008we] in Many Labs 5 made use of videos and eye-tracking measures, which did not match this study’s focus on item-based measures. We checked the Many Labs data source prior to registering our coding protocol, to check the feasibility of which data could be extracted and how. However, the actual coding took place after registering the coding protocol … Further details can be found in supplement XXX.

### Replication Datasets
The replication datasets refer to the publicly available datasets containing the data obtained from all labs partaking in a Many Labs study. For the analyses, we extracted the scores on the items of each previously identified measure that met our inclusion criteria specified below. When scores could not be clearly identified, any available codebooks, analysis scripts, or study materials were used to identify the relevant scores.

To be included in our reliability recalculations and factor analysis checks, the measure had to be a scale consisting of multiple items. If cleaned data were available, we chose these over raw data, to ensure that variables were coded as intended (e.g., no reverse-coded items). We omitted pilot data from the analyses. These criteria and the nature of the available format(s) of each dataset resulted in suitable item score data from 19 replication sets spread across on average approximately 35 lab locations for our analyses.

```{r CleaningReplicationDatasetsData, include = FALSE, eval = FALSE}
##### This code can be used to rerun the data preparation to convert the raw
##### (input) data of the Many Labs replications into intermediate data.
##### However, because the intermediate data is also available in this project,
##### the manuscript can also be reproduced without running this code block.

### Below we extract the relevant data from the Many Labs' datasets. The first
### number in each dataset refers to the Many Labs project it is related to.
### The second number to which study the data is from in order of appearance in
### the Many Labs pre-registered protocols, or in the case of Many Labs 5, 
### within the OSF folder structure. If there is a third number, the study had 
### multiple relevant measures this refers to the order of appearance of the 
### measure where the data is from within the study's description in the Many 
### Labs protocol or OSF folder structure. A fourth number indicates which part
### of the data of this measure was taken, in case the measure assessed multiple
### constructs, as these were treated separately for some analyses.

## ML 1
# 1.3
data_1.3_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[22:29])
colnames(data_1.3_clean)[1] <- "g"
# 1.10
data_1.10_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[108:115])
colnames(data_1.10_clean)[1] <- "g"
# 1.11
data_1.11_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[73:76])
colnames(data_1.11_clean)[1] <- "g"
# 1.12.1
# not found
# 1.12.3
data_1.12.3.1_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[54:59])
colnames(data_1.12.3.1_clean)[1] <- "g"
data_1.12.3.2_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[60:65])
colnames(data_1.12.3.2_clean)[1] <- "g"

## ML 2
# 2.2
data_2.2_clean <- cbind(as.factor(data_2.2[[5]]), data_2.2[6:11])
colnames(data_2.2_clean)[1] <- "g"
# 2.3
# data does not appear suitable
# 2.4.1
data_2.4.1_clean <- cbind(as.factor(data_2.4.1[[5]]), data_2.4.1[6:11])
colnames(data_2.4.1_clean)[1] <- "g"
# 2.4.2
data_2.4.2_clean <- cbind(as.factor(data_2.4.2[[5]]), data_2.4.2[6:14])
colnames(data_2.4.2_clean)[1] <- "g"
# 2.8.2
data_2.8.2_clean <- cbind(as.factor(data_2.8.2[[6]]), data_2.8.2[9:13])
colnames(data_2.8.2_clean)[1] <- "g"
# 2.10.1
data_2.10.1_clean <- cbind(as.factor(data_2.10.1[[5]]), data_2.10.1[6:11])
colnames(data_2.10.1_clean)[1] <- "g"
# 2.12.1
data_2.12.1_clean <- cbind(as.factor(data_2.12[[5]]), data_2.12[c(6,7,8,9,10,31,32,33,34,35)]) 
data_2.12.1_clean[3465:6905,2:6] <- data_2.12.1_clean[3465:6905,7:11]
data_2.12.1_clean <- data_2.12.1_clean[1:6]
colnames(data_2.12.1_clean)[1] <- "g"
# 2.12.2
data_2.12.2_clean <- cbind(as.factor(data_2.12[[5]]), data_2.12[c(11,14,15,18,19,22,24,27,28,29,36,39,40,43,44,47,49,52,53,54)]) 
data_2.12.2_clean[3465:6905,2:11] <- data_2.12.2_clean[3465:6905,12:21]
data_2.12.2_clean <- data_2.12.2_clean[1:11]
colnames(data_2.12.2_clean)[1] <- "g"
# 2.12.3
data_2.12.3_clean <- cbind(as.factor(data_2.12[[5]]), data_2.12[c(12,13,16,17,20,21,23,25,26,30,37,38,41,42,45,46,48,50,51,55)]) 
data_2.12.3_clean[3465:6905,2:11] <- data_2.12.3_clean[3465:6905,12:21]
data_2.12.3_clean <- data_2.12.3_clean[1:11]
colnames(data_2.12.3_clean)[1] <- "g"
# 2.15
data_2.15_clean <- cbind(as.factor(data_2.15[[5]]), data_2.15[8:12])
colnames(data_2.15_clean)[1] <- "g"
# 2.19.1
# difficult to extract
# 2.19.2
# difficult to extract
# 2.20
data_2.20_clean <- cbind(as.factor(data_2.20[[5]]), data_2.20[6:45]) 
data_2.20_clean[3729:7396,2:21] <- data_2.20_clean[3729:7396,22:41]
data_2.20_clean <- data_2.20_clean[1:21] 
# coding so all 1's means somebody used rule-based grouping strategy
data_2.20_clean[,c(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)] <- ifelse(data_2.20_clean[,c(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)] == 1, 1, 0)
data_2.20_clean[,c(3, 5, 7, 9, 11, 13, 15, 17, 19, 21)] <- ifelse(data_2.20_clean[,c(3, 5, 7, 9, 11, 13, 15, 17, 19, 21)] == 2, 1, 0)
colnames(data_2.20_clean)[1] <- "g"
# 2.23
data_2.23_clean <- cbind(as.factor(data_2.23[[5]]), data_2.23[c(7,8,12,13,15)])
colnames(data_2.23_clean)[1] <- "g"


## ML 3
# 3.2.1
data_3.2.1 <- cbind(as.factor(data_ml3[[1]]), data_ml3[77:86] - 1)
data_3.2.1.1_clean <- na.omit(data_3.2.1[1:6])
colnames(data_3.2.1.1_clean)[1] <- "g"
data_3.2.1.2_clean <- na.omit(data_3.2.1[c(1, 7:11)])
colnames(data_3.2.1.2_clean)[1] <- "g"
# 3.5
# data appears unusable
# 3.7.1
data_3.7.1_clean <- na.omit(cbind(as.factor(data_ml3[[1]]), data_ml3[38:42]))
colnames(data_3.7.1_clean)[1] <- "g"
# 3.7.2
data_3.7.2_clean <- na.omit(cbind(as.factor(data_ml3[[1]]), data_ml3[89:94]))
colnames(data_3.7.2_clean)[1] <- "g"
# 3.8.1
# a single measure was reported
# 3.8.2
data_3.8.2_clean <- na.omit(cbind(as.factor(data_ml3[[1]]), data_ml3[29:30])) 
colnames(data_3.8.2_clean)[1] <- "g"


## ML 5
# 5.1.1
data_5.1.1_clean <- cbind(as.factor(data_5.1[[2]]), data_5.1[13:27])
colnames(data_5.1.1_clean)[1] <- "g"
# 5.1.2
data_5.1.2_clean <- cbind(as.factor(data_5.1[[2]]), data_5.1[28:33])
colnames(data_5.1.2_clean)[1] <- "g"
# 5.4
data_5.4_clean <- cbind(as.factor(data_5.4[[1]]), data_5.4[18:41])
colnames(data_5.4_clean)[1] <- "g"
# 5.5.1 & 5.5.2
# from this dataset it appears that this data will be difficult to use.
# 5.5.2
# also difficult to use
# 5.7 
data_5.7_clean <- cbind(as.factor(data_5.7[[3]]), data_5.7[c(25, 34, 35, 36, 37, 38, 39, 40, 41, 42)])
colnames(data_5.7_clean)[1] <- "g"
# 5.9.1
data_5.9.1_clean <- na.omit(cbind(as.factor(data_5.9.1[[4]]), data_5.9.1[c(79, 83, 87, 91, 95, 98, 101)]))
colnames(data_5.9.1_clean)[1] <- "g"


### Saving to Intermediate Data Folder
### !!!! OMIT INTERMEDIATE DATA & KEEP FUNCTION THAT REMOVE ALL NON RAW DATA 
### WHEN LOADING THE DATA IN!!!!
saving_to_intermediate_data(data_1.3_clean)
saving_to_intermediate_data(data_1.10_clean)
saving_to_intermediate_data(data_1.11_clean)
saving_to_intermediate_data(data_1.12.3.1_clean)
saving_to_intermediate_data(data_1.12.3.2_clean)
saving_to_intermediate_data(data_2.10.1_clean)
saving_to_intermediate_data(data_2.12.1_clean)
saving_to_intermediate_data(data_2.12.2_clean)
saving_to_intermediate_data(data_2.12.3_clean)
saving_to_intermediate_data(data_2.15_clean)
saving_to_intermediate_data(data_2.20_clean)
saving_to_intermediate_data(data_2.23_clean)
saving_to_intermediate_data(data_3.2.1.1_clean)
saving_to_intermediate_data(data_3.2.1.2_clean)
saving_to_intermediate_data(data_3.7.1_clean)
saving_to_intermediate_data(data_3.7.2_clean)
saving_to_intermediate_data(data_3.8.2_clean)
saving_to_intermediate_data(data_5.1.1_clean)
saving_to_intermediate_data(data_5.1.2_clean)
saving_to_intermediate_data(data_5.4_clean)
saving_to_intermediate_data(data_5.7_clean)
saving_to_intermediate_data(data_5.9.1_clean)

```

### Replication Protocols
The replication protocols refer to the publicly available protocols describing the background, methodology, and analysis of each set of replications of a given original study across multiple labs. These were retrieved from the OSF pages of the Many Labs projects (the search strategy and OSF file locations can be found in the data retrieval information supplementary document; URL https://github.com/CasGoos/measurement_and_replication/blob/master/SupplementaryMaterials/data_retrieval_information.Rmd).

### Original Articles
We identified and retrieved all original study articles using the citations for these articles in each replication protocol.

```{r CleaningCodedData, include = FALSE, eval = FALSE}
##### This code can be used to rerun the data preparation to convert the raw 
##### (input) data of the coded data into intermediate data. However, because 
##### the intermediate data is also available in this project, the manuscript  
##### can also be reproduced without running this code block.
# Selecting the relevant rows and columns for the data
coded_data_initial_sel <- coded_data_initial_raw[3:160, 18:57]
coded_data_revised_sel <- coded_data_revised_raw[3:160, 18:38]
coded_data_vignette_sel <- coded_data_vignette_raw[3:160, 2]

# Combining the datasets
coded_data_full <- cbind(coded_data_initial_sel, 
                         cbind(coded_data_revised_sel, coded_data_vignette_sel))

# filtering out unnecessary double columns
coded_data_full <- cbind(coded_data_full[, 1:40], coded_data_full[, 45:62])


### data preparation
# renaming columns
colnames(coded_data_full) <- c("many_labs_version", "rep_org", "title", "measure_name", 
      "variable_name", "multi", "variable_order", "N", "N_items", 
      "hypothesis_support", "reliability_type", "reliability_type_text", 
      "reliability_coeff", "def_1", "op_version", "op_1", "op_2", "op_3", "op_4", 
      "op_5", "sel_existing", "sel_existing_text", "sel_1", "sel_2", "sel_3", 
      "sel_4", "sel_psychometric_evidence", "sel_psychometric_evidence_text", 
      "quant_1", "quant_2", "quant_3", "quant_4", "mod_check", "mod_1", "mod_2", 
      "mod_3", "mod_4", "mod_5", "mod_6", "mod_time", "op_1_REV", "op_2_REV",
      "op_5_REV", "sel_1_REV", "sel_3_REV", "sel_psychometric_evidence_REV", 
      "sel_psychometric_evidence_text_REV", "quant_1_REV", "quant_2_REV", 
      "quant_3_REV", "mod_check_REV", "mod_1_REV", "mod_2_REV", "mod_3_REV", 
      "mod_4_REV", "mod_5_REV", "mod_6_REV", "inseperable_material")
  
# renaming rows
rownames(coded_data_full) <- 1:nrow(coded_data_full)

# fixing some coding mistakes
coded_data_full$variable_name[79] <- "quote attribution effect"
coded_data_full$N[3] <- "5284"
coded_data_full$N[158] <- "1202"
coded_data_full$reliability_type[50] <- "Not Reported"
coded_data_full$reliability_type[127] <- "Not Reported"
coded_data_full$op_1[145] <- "False"
coded_data_full$op_3[121] <- "True"
coded_data_full$op_5[157] <- "True"
coded_data_full$sel_1[59] <- "True"
coded_data_full$quant_2[113] <- "True"
coded_data_full$mod_time[1] <- "Before"

# removing missing entry 77
coded_data_full <- data.frame(coded_data_full)[-77,]

# Many labs 2.25 and 2.26, as well as 3.4 and 3.5 (for replications) were coded 
# in reverse order thus need to be swapped in right order. Additionally, some
# of the entries were included later than following their order, due to some
# minor coding oversights.
Coded_Data_Full_Restructured <- coded_data_full[c(1:18, 148, 19:23, 147, 24:28, 
                                                  153, 29:40, 154, 42, 41, 155, 
                                                  43:49, 156, 51, 50, 52:76, 
                                                  157, 77:88, 149:152, 89:146),]


# rename the rownames to match the new order
rownames(Coded_Data_Full_Restructured) <- 1:nrow(Coded_Data_Full_Restructured)

# changing the variable types for each column to better represent their 
# intended variable type
class(Coded_Data_Full_Restructured$many_labs_version) <- "numeric"
Coded_Data_Full_Restructured$many_labs_version <- as.factor(Coded_Data_Full_Restructured$many_labs_version)
Coded_Data_Full_Restructured$rep_org <- droplevels(as.factor(Coded_Data_Full_Restructured$rep_org))
Coded_Data_Full_Restructured$multi <- droplevels(as.factor(Coded_Data_Full_Restructured$multi))
Coded_Data_Full_Restructured$variable_order <- droplevels(as.factor(Coded_Data_Full_Restructured$variable_order))
class(Coded_Data_Full_Restructured$N) <- "numeric"
Coded_Data_Full_Restructured$N_items <- droplevels(as.factor(Coded_Data_Full_Restructured$N_items))
Coded_Data_Full_Restructured$hypothesis_support <- droplevels(as.factor(Coded_Data_Full_Restructured$hypothesis_support))
levels(Coded_Data_Full_Restructured$hypothesis_support) <- c("No", "Unclear", "Yes")
Coded_Data_Full_Restructured$reliability_type <- droplevels(as.factor(Coded_Data_Full_Restructured$reliability_type))
class(Coded_Data_Full_Restructured$reliability_coeff) <- "numeric"
Coded_Data_Full_Restructured$def_1 <- as.logical(Coded_Data_Full_Restructured$def_1)
Coded_Data_Full_Restructured$op_1 <- as.logical(Coded_Data_Full_Restructured$op_1)
Coded_Data_Full_Restructured$op_2 <- as.logical(Coded_Data_Full_Restructured$op_2)
Coded_Data_Full_Restructured$op_3 <- as.logical(Coded_Data_Full_Restructured$op_3)
Coded_Data_Full_Restructured$op_4 <- as.logical(Coded_Data_Full_Restructured$op_4)
Coded_Data_Full_Restructured$op_5 <- as.logical(Coded_Data_Full_Restructured$op_5)
Coded_Data_Full_Restructured$sel_existing <- droplevels(as.factor(Coded_Data_Full_Restructured$sel_existing))
Coded_Data_Full_Restructured$sel_1 <- as.logical(Coded_Data_Full_Restructured$sel_1)
Coded_Data_Full_Restructured$sel_2 <- as.logical(Coded_Data_Full_Restructured$sel_2)
Coded_Data_Full_Restructured$sel_3 <- as.logical(Coded_Data_Full_Restructured$sel_3)
Coded_Data_Full_Restructured$sel_4 <- as.logical(Coded_Data_Full_Restructured$sel_4)
Coded_Data_Full_Restructured$sel_psychometric_evidence <- droplevels(as.factor(Coded_Data_Full_Restructured$sel_psychometric_evidence))
Coded_Data_Full_Restructured$quant_1 <- as.logical(Coded_Data_Full_Restructured$quant_1)
Coded_Data_Full_Restructured$quant_2 <- as.logical(Coded_Data_Full_Restructured$quant_2)
Coded_Data_Full_Restructured$quant_3 <- as.logical(Coded_Data_Full_Restructured$quant_3)
Coded_Data_Full_Restructured$quant_4 <- as.logical(Coded_Data_Full_Restructured$quant_4)
Coded_Data_Full_Restructured$mod_check <- droplevels(as.factor(Coded_Data_Full_Restructured$mod_check))
Coded_Data_Full_Restructured$mod_1 <- as.logical(Coded_Data_Full_Restructured$mod_1)
Coded_Data_Full_Restructured$mod_2 <- as.logical(Coded_Data_Full_Restructured$mod_2)
Coded_Data_Full_Restructured$mod_3 <- as.logical(Coded_Data_Full_Restructured$mod_3)
Coded_Data_Full_Restructured$mod_4 <- as.logical(Coded_Data_Full_Restructured$mod_4)
Coded_Data_Full_Restructured$mod_5 <- as.logical(Coded_Data_Full_Restructured$mod_5)
Coded_Data_Full_Restructured$mod_6 <- as.logical(Coded_Data_Full_Restructured$mod_6)
Coded_Data_Full_Restructured$mod_time <- droplevels(as.factor(Coded_Data_Full_Restructured$mod_time))
Coded_Data_Full_Restructured$op_1_REV <- as.logical(Coded_Data_Full_Restructured$op_1_REV)
Coded_Data_Full_Restructured$op_2_REV <- as.logical(Coded_Data_Full_Restructured$op_2_REV)
Coded_Data_Full_Restructured$op_5_REV <- as.logical(Coded_Data_Full_Restructured$op_5_REV)
Coded_Data_Full_Restructured$sel_1_REV <- as.logical(Coded_Data_Full_Restructured$sel_1_REV)
Coded_Data_Full_Restructured$sel_3_REV <- as.logical(Coded_Data_Full_Restructured$sel_3_REV)
Coded_Data_Full_Restructured$sel_psychometric_evidence_REV <- droplevels(as.factor(Coded_Data_Full_Restructured$sel_psychometric_evidence_REV))
Coded_Data_Full_Restructured$quant_1_REV <- as.logical(Coded_Data_Full_Restructured$quant_1_REV)
Coded_Data_Full_Restructured$quant_2_REV <- as.logical(Coded_Data_Full_Restructured$quant_2_REV)
Coded_Data_Full_Restructured$quant_3_REV <- as.logical(Coded_Data_Full_Restructured$quant_3_REV)
Coded_Data_Full_Restructured$mod_check_REV <- droplevels(as.factor(Coded_Data_Full_Restructured$mod_check_REV))
Coded_Data_Full_Restructured$mod_1_REV <- as.logical(Coded_Data_Full_Restructured$mod_1_REV)
Coded_Data_Full_Restructured$mod_2_REV <- as.logical(Coded_Data_Full_Restructured$mod_2_REV)
Coded_Data_Full_Restructured$mod_3_REV <- as.logical(Coded_Data_Full_Restructured$mod_3_REV)
Coded_Data_Full_Restructured$mod_4_REV <- as.logical(Coded_Data_Full_Restructured$mod_4_REV)
Coded_Data_Full_Restructured$mod_5_REV <- as.logical(Coded_Data_Full_Restructured$mod_5_REV)
Coded_Data_Full_Restructured$mod_6_REV <- as.logical(Coded_Data_Full_Restructured$mod_6_REV)
Coded_Data_Full_Restructured$inseperable_material <- droplevels(as.factor(Coded_Data_Full_Restructured$inseperable_material)

                                                                
                                                                

# the moral foundations questionnaire in original 2.4 is reported using all 5 of 
# its factors, whereas in replication 2.4 only the two overarching groups of 
# binding and individualizing foundations are described. For that reason a 
# shortened original dataset will be used for any direct comparisons between 
# original and replication coding.

Coded_Data_Full_Shortened <- Coded_Data_Full_Restructured[c(1:94, 96, 99:157),] 
Coded_Data_Full_Shortened[c(95,96),5] <- c("individualizing moral foundations", "binding moral foundations")
Coded_Data_Full_Shortened[95,13] <- NA
Coded_Data_Full_Shortened[96,13] <- NA


# separating the replication and original data from each other.
coded_data_replications <- Coded_Data_Full_Shortened[1:77,]
coded_data_original <- Coded_Data_Full_Shortened[78:154,]

# exporting cleaned data
saving_to_intermediate_data(coded_data_replications)
saving_to_intermediate_data(coded_data_original)

```

## Measures
### Coding Protocol
We developed a coding protocol to extract reported measurement information from the original articles and replication protocols. We extracted the reported reliability coefficient and type of index (Cronbach’s Alpha, retest, interrater, etc.) when present. Similarly, we coded if any construct validity evidence, such as a factor analysis, was presented alongside the measure. We additionally included twenty items that were based on the QMP table presented in Flake & Fried. These items recorded whether or not certain information was reported and were all coded to be either true, false, or not applicable if not relevant for that measure (e.g., reporting results from a factor analysis for single-item measures). Example items can been in Table 1.

```{r QMPCodingInfoTable, warning = FALSE}
QMP_info_dataframe <- data.frame(Category = c("Definition", "", "", "Operationalisation", "", "", "", "Selection/Creation", "", "", "Quantification", "Modification", "", "", "", "", ""),
           'N Questions' = c("1", "", "", "5", "", "", "", "4", "", "", "4", "6", "", "", "", "", ""),
           'Example Question' = c("A psychological/sociological definition",
            "is given to the name of the measured",
            "variable within the paper.", 
            "The administration format (pen-and-",
            "paper/computer) and environment (in",
            "public/in a lab) are described (Note:",
            "both should be present for a true rating).", 
            "The source of the scale is provided",
            "(in case the scale was newly developed",
            "this should be clearly stated).", 
            "The number of items are described.", 
            "Any format changes are mentioned",
            "(paper-and-pencil <–> computer), if no",
            "changes were made to the format, and",
            "this was mentioned then code as No",
            "modification. If it is not clear, then code",
            "as False."))

# making the column names look less robot speak-y.
colnames(QMP_info_dataframe) <- c("Category", "N Questions", "Example Question")


# transfer the data to an APA table for printing
apa_table(
  QMP_info_dataframe, align = c("l", "r", "l")
  , caption = "Information of QMP coding variables per category."
  , note = "N Questions refers only to the questions used for calculating QMP ratios. Selection and creation share a category as the justifications and requirements in selecting a measure are similar to those for creating a new measure."
  , escape = FALSE, placement = "htp", booktabs = TRUE)

```

After the initial coding, we made minor revisions to 14 of the 20 QMP items in the preregistered coding protocol, because we considered them overly stringent in practice. For example, in the initial protocol, an example item had to be present within the article or protocol itself, or else this was counted as a QMP. In the revised protocol, references to online appendices with example items were also considered sufficient for this item. The analyses, tables, and figures presented in this article are all based on the revised coding protocol, the results of the equivalent analyses based on QMPs obtained with our initial protocol can be found in Supplementary Analyses D.



We initially intended to form a QMP index using these QMP items and explore relations in the data using this index, but this index could not be validated properly (more information on the index can be found in the Supplement). These items are therefore currently used on their own to give a descriptive overview of the reporting practices in our sample.

```{r Calculating_QMP_Data, include = FALSE, eval = FALSE}
original_QMP_data <- coded_data_original[c("def_1", "sel_2", "op_4", "reliability_type", "sel_existing", "op_version", "op_1_REV", "sel_1_REV", "sel_3_REV", "sel_4", "sel_psychometric_evidence", "op_2_REV", "op_3", "quant_1_REV", "quant_2_REV", "quant_3_REV", "quant_4", "op_5_REV", "mod_1_REV", "mod_2_REV", "mod_3_REV", "mod_4_REV", "mod_5_REV", "mod_6_REV")]

replication_QMP_data <- coded_data_replications[c("def_1", "sel_2", "op_4", "reliability_type", "sel_existing", "op_version", "op_1_REV", "sel_1_REV", "sel_3_REV", "sel_4", "sel_psychometric_evidence", "op_2_REV", "op_3", "quant_1_REV", "quant_2_REV", "quant_3_REV", "quant_4", "op_5_REV", "mod_1_REV", "mod_2_REV", "mod_3_REV", "mod_4_REV", "mod_5_REV", "mod_6_REV")]



# create an empty dataset to add a recoded TRUE and FALSE response for QMPs to 
QMP_data <- data.frame(delete_this = rep(NA, 77))


QMP_data$reliability_reported_org <- ifelse(original_QMP_data$reliability_type != "Not Reported" & original_QMP_data$reliability_type != "" & !is.na(original_QMP_data$reliability_type), TRUE, ifelse(original_QMP_data$reliability_type == "Not Reported", FALSE, NA))

QMP_data$reliability_reported_rep <- ifelse(replication_QMP_data$reliability_type != "Not Reported" & replication_QMP_data$reliability_type != "" & !is.na(replication_QMP_data$reliability_type), FALSE, ifelse(replication_QMP_data$reliability_type == "Not Reported", TRUE, NA))


# add if clearly specified if measure existed or not
QMP_data$select_or_create_clarity_org <- original_QMP_data$sel_existing == "Not Clearly Stated"

QMP_data$select_or_create_clarity_rep <- replication_QMP_data$sel_existing == "Not Clearly Stated"


# add if version was clearly specified
QMP_data$version_clarity_org <- original_QMP_data$op_1_REV == FALSE | original_QMP_data$op_version == "" & original_QMP_data$sel_existing == "True, namely:"

QMP_data$version_clarity_rep <- replication_QMP_data$op_1_REV == FALSE | replication_QMP_data$op_version == "" & replication_QMP_data$sel_existing == "True, namely:"


# add if factor structure was analysed
QMP_data$factor_analysis_org <- ifelse(original_QMP_data$sel_psychometric_evidence == "None", FALSE, ifelse(original_QMP_data$sel_psychometric_evidence != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)", TRUE, NA))

QMP_data$factor_analysis_rep <- ifelse(replication_QMP_data$sel_psychometric_evidence == "None", FALSE, ifelse(replication_QMP_data$sel_psychometric_evidence != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)", TRUE, NA))

# direct copies
QMP_data$justified_definition_org <- original_QMP_data$def_1
QMP_data$justified_definition_rep <- replication_QMP_data$def_1
QMP_data$justified_selection_org <- original_QMP_data$sel_2
QMP_data$justified_selection_rep <- replication_QMP_data$sel_2
QMP_data$justified_operationalisation_org <- original_QMP_data$op_4
QMP_data$justified_operationalisation_rep <- replication_QMP_data$op_4
QMP_data$source_org <- original_QMP_data$sel_1_REV
QMP_data$source_rep <- replication_QMP_data$sel_1_REV
QMP_data$in_psychometric_org <- original_QMP_data$sel_3_REV
QMP_data$in_psychometric_rep <- replication_QMP_data$sel_3_REV
QMP_data$out_psychometric_org <- original_QMP_data$sel_4
QMP_data$out_psychometric_rep <- replication_QMP_data$sel_4
QMP_data$admin_format_org <- original_QMP_data$op_2_REV
QMP_data$admin_format_rep <- replication_QMP_data$op_2_REV
QMP_data$admin_procedure_org <- original_QMP_data$op_3
QMP_data$admin_procedure_rep <- replication_QMP_data$op_3
QMP_data$N_items_org <- original_QMP_data$quant_1_REV
QMP_data$N_items_rep <- replication_QMP_data$quant_1_REV
QMP_data$N_response_org <- original_QMP_data$quant_2_REV
QMP_data$N_response_rep <- replication_QMP_data$quant_2_REV
QMP_data$recoding_org <- original_QMP_data$quant_3_REV
QMP_data$recoding_rep <- replication_QMP_data$quant_3_REV
QMP_data$aggregation_org <- original_QMP_data$quant_4
QMP_data$aggregation_rep <- replication_QMP_data$quant_4
QMP_data$example_items_org <- original_QMP_data$op_5_REV
QMP_data$example_items_rep <- replication_QMP_data$op_5_REV
QMP_data$mod_admin_format_org <- original_QMP_data$mod_1_REV
QMP_data$mod_admin_format_rep <- replication_QMP_data$mod_1_REV
QMP_data$mod_admin_format_support_org <- original_QMP_data$mod_2_REV
QMP_data$mod_admin_format_support_rep <- replication_QMP_data$mod_2_REV
QMP_data$mod_language_org <- original_QMP_data$mod_3_REV
QMP_data$mod_language_rep <- replication_QMP_data$mod_3_REV
QMP_data$mod_language_support_org <- original_QMP_data$mod_4_REV
QMP_data$mod_language_support_rep <- replication_QMP_data$mod_4_REV
QMP_data$mod_N_items_or_response_org <- original_QMP_data$mod_5_REV
QMP_data$mod_N_items_or_response_rep <- replication_QMP_data$mod_5_REV
QMP_data$mod_N_items_or_response_support_org <- original_QMP_data$mod_6_REV
QMP_data$mod_N_items_or_response_support_rep <- replication_QMP_data$mod_6_REV


QMP_data <- QMP_data[,!(names(QMP_data) %in% "delete_this")]

QMP_data$source_org <- original_QMP_data$sel_1_REV
QMP_data$source_rep <- replication_QMP_data$sel_1_REV

QMP_data$select_or_create_clarity_org <- original_QMP_data$sel_existing == "Not Clearly Stated"
QMP_data$select_or_create_clarity_rep <- replication_QMP_data$sel_existing == "Not Clearly Stated"

QMP_data$version_clarity_org <- original_QMP_data$op_1_REV == FALSE | original_QMP_data$op_version == "" & original_QMP_data$sel_existing == "True, namely"
QMP_data$version_clarity_rep <- replication_QMP_data$op_1_REV == FALSE | replication_QMP_data$op_version == "" & replication_QMP_data$sel_existing == "True, namely"


### QMP ratio data
# create the empty QMP ratio dataset
QMP_ratio_data <- data.frame(Practice = rep("", ncol(QMP_data)/2),
                   QMP_percentage_original = rep(0, ncol(QMP_data)/2),
                   N_applicable_original = rep(0, ncol(QMP_data)/2),
                   QMP_percentage_replication = rep(0, ncol(QMP_data)/2),
                   N_applicable_replication = rep(0, ncol(QMP_data)/2),
                   Phi = rep(0, ncol(QMP_data)/2))


# we loop through the length of the number of Measurement Practices (MPs) 
# ignoring doubles due to having both original and replication
for(i in 1:(ncol(QMP_data)/2)){
  # we create the name of the variable by taking the name and removing the 
  # _org suffix. Then we for each column except the NA initial column
  # we take the original MP and the related replication MP, and calculate
  # their QMP
  QMP_ratio_data[i,] <- Create_QMP_descriptive_case(substr(names(QMP_data)[i*2], 1, nchar(names(QMP_data)[i*2]) - 4), QMP_data[[(i*2)-1]], QMP_data[[i*2]])
  }

# store QMP ratio data in the analysis data folder
saving_to_analysis_data(QMP_ratio_data)
```

Recalculating Data
We furthermore, used the replication datasets to calculate reliability for each measure with appropriate data for each lab the measure was used in. We calculated both Cronbach’s alpha, as well as its standard error using formulas 2 & 3 from Duhachek and Lacobucci (2004) to calculate the standard error in the meta-analysis. Then combining these two values we conducted a meta-analysis of the reliability, also commonly referred to as a Reliability Generalization (RG) Meta-Analysis (Botella & Suero, 2012; López-Ibáñez et al., 2024; Vacha-Haase, 1998). This enabled us to quantify the degree of true variation (or heterogeneity) in reliability coefficients across lab locations. We performed the RG meta-analysis using the rma function from the metafor R package (v4.4-0 ; Viechtbauer, 2010) and default settings. We implemented no correction for bias, because the Many Labs replications were not at risk of publication bias.

Our analyses will focus on Cronbach’s Alpha, because Cronbach’s Alpha is used more commonly in research allowing us to make comparisons between calculated and reported reliabilities.  Furthermore, we were able to calculate the standard error of alpha to be used in the Reliability Generalization meta-analysis. However, we additionally estimated McDonald’s Omega, since it has been argued to be a more informative measure of reliability than alpha, while also simultaneously providing some validity evidence [@crutzen2017scale; @deng2017testing]. Omega was calculated using the omega function in the *psych* R package [*v`r getNamespaceVersion("psych")[[1]]`*; @R-psych]. Default arguments were used in the function except the nfactors argument, which was set to 1. 

Finally, We probed the three assumptions of Cronbach’s alpha for our included nineteen measures. First, we tested for unidimensionality using the fit indices of a one-dimensional Confirmatory Factor Analysis (CFA), and the outcome from a parallel test. !!! explain parallel test !!! . Second, we compared the unidimensional model to a unidimensional model that was restricted to be Tau equivalent (equal loading of all items on the common factor), and used model comparison to test whether the restricted model fitted significantly worse or not. Third, for the unidimensional model, we tested the assumption of uncorrelated errors by freely estimated the five highest correlations between individual item pairs and then used a model comparison to test whether the model with freely estimated errors fitted significantly better or not. Besides testing the assumptions for Cronbach’s alpha, the results regarding unidimensionality combined with McDonald’s Omega also gives us some indication of the construct validity, since these measures were checked to all have been used to create a singular index. If a unidimensional model would thus lack fit according to our tests, it would put into question the use of these measures to estimate a singular variable of interest.

```{r calculated_reliability_across_labs, include = FALSE, eval = FALSE}
# creating an empty data frame to insert all the responses into
calculated_reliability_lab_data <- data.frame(alpha = 0, omega.tot = 0, 
                                          omega.hier = 0, ASE = 0, g = 0)

# Combining the data together 1.3, & 5.4 were omitted, because data was not
# recorded in usable numeric format
extracted_score_data <- list(data_1.10_clean, data_1.11_clean, 
    data_1.12.3.1_clean, data_1.12.3.2_clean, data_2.12.1_clean, 
    data_2.12.2_clean, data_2.12.3_clean, data_2.15_clean, data_2.20_clean, 
    data_2.23_clean, data_3.2.1.1_clean, data_3.2.1.2_clean, data_3.7.1_clean,
    data_3.7.2_clean, data_3.8.2_clean, data_5.1.1_clean, data_5.1.2_clean, 
    data_5.7_clean, data_5.9.1_clean)


# obtaining the omega and alpha values for a measure in one lab.
get_omega_and_alpha_values <- function(Data){
  # first we calculate alpha and ase separately in case the omega function goes haywire
  alpha <- psych::alpha(Data)$total[["std.alpha"]]
  ase <- psych::alpha(Data)$total[["ase"]]
  
  # try to calculate the omega
  result <- c(NA, NA)
  tryCatch({
      result <- omega(Data)
    }, error = function(e) {
      result <- c(NA, NA)
    }, warning = function(w) {
      result <- c(NA, NA)
    })
  
  # combine the information in one vector
  omega_and_alpha_vec <- as.numeric(c(alpha, result[c(4, 1)], ase))
  
  return(omega_and_alpha_vec)
}


# calculate the alpha, omega.tot, omega.hier, & ASE for all relevant datasets
# for each lab.
for (i in 1:length(extracted_score_data)){
  calculated_reliability_instance <- tapply(extracted_score_data[[i]][-1], 
                                            extracted_score_data[[i]]$g, 
                                            get_omega_and_alpha_values)
  
  calculated_reliability_instance <- data.frame(matrix(unlist(
    calculated_reliability_instance), ncol = 4, byrow = TRUE))
  
  # make sure the var names match the complete dataframe
  colnames(calculated_reliability_instance) <- c("alpha", "omega.tot", 
                                                 "omega.hier", "ASE")
  # adding the measure as a group (g) indicator
  calculated_reliability_instance$g <- i
  
  # adding this measure's data to the total
  calculated_reliability_lab_data <- rbind(calculated_reliability_lab_data, 
                                       calculated_reliability_instance)
}


# removing the empty first row
calculated_reliability_lab_data <- calculated_reliability_lab_data[-1,]

# making sure group (g) is a factor
calculated_reliability_lab_data$g <- as.factor(calculated_reliability_lab_data$g)

# indexing the meta-analysis results with a specific index relating to a 
# row (measure) in coded_data_replications
calculated_reliability_lab_data$coded_data_index <- c(rep(10, 36), rep(11, 36), 
  rep(14, 36), rep(14, 36), rep(29, 74), rep(30, 74), rep(31, 74), rep(34, 61), 
  rep(39, 60), rep(42, 58), rep(51, 21), rep(51, 21), rep(59, 20), rep(60, 21), 
  rep(61, 21), rep(65, 4), rep(66, 4), rep(74, 8), rep(76, 5))

# changing the group variable to reflect measure descriptions from the text.
# checked using:
# coded_data_replications[unique(calculated_reliability_data$reporting_index), 3], and
# coded_data_replications[unique(calculated_reliability_data$reporting_index), 5]
levels(calculated_reliability_lab_data$g) <- c("Caruso et al. (2012)", 
    "Husnu & Crisp (2010)", "Nosek et al. (2002), Math", "Nosek et al. (2002), Art", 
    "Anderson et al. (2012), SWL", "Anderson et al. (2012), PA", 
    "Anderson et al. (2012), NA", "Giessner & Schubert, (2007)", 
    "Norenzayan et al. (2002)", "Zhong & Lijenquist (2006)", 
    "Monin & Miller (2001), most", "Monin & Miller (2001), some", 
    "Cacioppo et al. (1983), arg",  "Cacioppo et al. (1983), nfc", 
    "De Fruyt et al. (2000)", "Albarracín et al. (2008), exp 5 verb", 
    "Albarracín et al. (2008), exp 5 math", "Shnabel & Nadler (2008)",
    "Vohs & Schooler (2008)")
  
calculated_reliability_lab_data$g <- factor(calculated_reliability_lab_data$g, 
    labels = c("Caruso et al. (2012)", 
    "Husnu & Crisp (2010)", "Nosek et al. (2002), Math", 
    "Nosek et al. (2002), Art", "Anderson et al. (2012), SWL", 
    "Anderson et al. (2012), PA", "Anderson et al. (2012), NA", 
    "Giessner & Schubert, (2007)", "Norenzayan et al. (2002)", 
    "Zhong & Lijenquist (2006)", "Monin & Miller (2001), most", 
    "Monin & Miller (2001), some", "Cacioppo et al. (1983), arg",  
    "Cacioppo et al. (1983), nfc", "De Fruyt et al. (2000)", 
    "Albarracín et al. (2008), exp 5 verb", 
    "Albarracín et al. (2008), exp 5 math", "Shnabel & Nadler (2008)", 
    "Vohs & Schooler (2008)"))

# adding whether or not an effect replicated based on what was coded from the 
# replication report.
calculated_reliability_lab_data$replication_success <- c(coded_data_replications[
    calculated_reliability_lab_data$coded_data_index, "hypothesis_support"])



# store per lab calculated reliability data in the analysis data folder
saving_to_analysis_data(calculated_reliability_lab_data)
```

```{r averaged_reliability, include = FALSE, eval = FALSE}
# function to assess the heterogeneity in the calculated Cronbach's Alpha values
# and get Cronbach's Alpha prediction intervals
assess_heterogeneity<- function(data_on_alpha){
  # run a random effects meta-analysis
  rma_model <- rma(yi = data_on_alpha$alpha, sei = data_on_alpha$ASE, 
                   method = "REML", control = list(stepadj = 0.5, maxiter = 1000))
  
  # extract the relevant heterogeneity information
  temp_tau <- sqrt(rma_model$tau2)
  temp_QEp <- rma_model$QEp
  
  # get prediction intervals for alpha
  rma_prediction <- predict(rma_model)
  temp_pi.lb <- rma_prediction$pi.lb
  temp_pi.ub <- rma_prediction$pi.ub
  
  return(c(temp_tau, temp_QEp, temp_pi.lb, temp_pi.ub))
}

# function to convert the lab specifc reliability to averaged
convert_reliability_data_to_avg <- function(reliability_data){
  # conducting the reliability-generalization meta-analysis for each measure
  heterogeneity_results <- assess_heterogeneity(reliability_data[c(1, 4)])
  
  # getting the avearage reliability scores for each measure
  avg_reliabilities <- colMeans(reliability_data[1:4], na.rm = TRUE)
  
  # indicates the measure
  g <- reliability_data$g[[1]]
  
  # we need one of the indices per measure as they are all the same across labs
  coded_data_index <- reliability_data$coded_data_index[[1]]
  
  # we need one of the indices per measure as they are all the same across labs
  replication_success <- reliability_data$replication_success[[1]]
  
  # extracting the reported reliability coefficient 
  coefficient_reported <- coded_data_original$reliability_coeff[coded_data_index]
  
  # calculating the difference between reported and calculated average 
  # reliability coefficient
  coeficient_difference <- coefficient_reported - avg_reliabilities[[1]]
  
  # testing whether or not (for those studies that had a reported alpha) if
  # it was out of the 95% bounds around the mean calculated alpha
  population_95_bounds <- quantile(reliability_data$alpha, probs = c(0.025, 0.975))
  
  significance_reported_coefficient <- coefficient_reported < population_95_bounds[1] | 
                                          coefficient_reported > population_95_bounds[2]
  
  # return all the data as a single row in the dataframe
  return(data.frame(alpha = avg_reliabilities[[1]], omega.tot = avg_reliabilities[[2]], 
                    omega.hier = avg_reliabilities[[3]], ASE = avg_reliabilities[[4]], 
                    tau = heterogeneity_results[[1]], QEp = heterogeneity_results[[2]],
                    pi.lb = heterogeneity_results[[3]], pi.ub = heterogeneity_results[[4]], 
                    g = g, coded_data_index = coded_data_index, 
                    replication_success, reported_coefficient = coefficient_reported, 
                    coefficient_difference = coeficient_difference, 
                    significance_reported_coefficient = significance_reported_coefficient))
}

# calculate the average reliability data + heterogeneity test + reported and calculated
# reliability coefficient comparison
avg_reliability_list <- tapply(calculated_reliability_lab_data, calculated_reliability_lab_data$g, convert_reliability_data_to_avg)

# convert data output to a dataframe
measure_reliability_data <- do.call(rbind.data.frame, avg_reliability_list)


# store per measure  reliability data in the analysis data folder
saving_to_analysis_data(measure_reliability_data)
```

# Results
## Reliability & Validity reporting
Figure 1 depicts the flow of measures in relation to reliability reporting. First, it shows that almost half of the measures in both replication (N = 37) and original research (N = 35) were single-item measures. Only thirteen measures in original and four in replications were reported with a reliability indicator. Cronbach’s Alpha was the most commonly reported reliability indicator.

```{r ReliabilityReportingFlowDiagram, fig.cap = "Reliability reporting flow diagram. Figure shows the number of measures as reported in both the replication protocols and original article, which meet the criterion in the box within the diagram and those criteria before it.", out.height = "60%"}
knitr::include_graphics(path = "../../SupplementaryMaterials/reliability_reporting_flow_diagram.png")
```

Validity evidence was even less common than reliability evidence. Only, `r sum(coded_data_original$sel_psychometric_evidence_REV != "None" & coded_data_original$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)" | coded_data_original$sel_3_REV == TRUE, na.rm = TRUE)` original studies and `r sum(coded_data_replications$sel_psychometric_evidence_REV != "None" & coded_data_replications$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)" | coded_data_replications$sel_3_REV == TRUE, na.rm = TRUE)` replications reported psychometric evidence on the . Most commonly exploratory factor analyses or convergent validity information were presented. Only `r sum(coded_data_original$sel_4 == TRUE, na.rm = TRUE)` original studies & `r sum(coded_data_replications$sel_4 == TRUE, na.rm = TRUE)` replications reported psychometric validity evidence from previous studies. 

```{r code on the reporting of validity evidence}
table(coded_data_original$sel_psychometric_evidence_REV)
table(coded_data_replications$sel_psychometric_evidence_REV)

sum(coded_data_original$sel_psychometric_evidence_REV != "None" & coded_data_original$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)" | coded_data_original$sel_3_REV == TRUE, na.rm = TRUE)
sum(coded_data_replications$sel_psychometric_evidence_REV != "None" & coded_data_replications$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)" | coded_data_replications$sel_3_REV == TRUE, na.rm = TRUE)

table(coded_data_original$sel_psychometric_evidence_text_REV)
table(coded_data_replications$sel_psychometric_evidence_text_REV)


# internal
table(coded_data_original$sel_3_REV)
table(coded_data_replications$sel_3_REV)
# external
table(coded_data_original$sel_4)
table(coded_data_replications$sel_4)

sum(coded_data_original$sel_4 == TRUE, na.rm = TRUE)
sum(coded_data_replications$sel_4 == TRUE, na.rm = TRUE)



sum(coded_data_original$sel_existing == "True, namely:")
sum(coded_data_replications$sel_existing == "True, namely:")

sum(coded_data_original$op_1_REV[coded_data_original$sel_existing == "True, namely:"] == TRUE, na.rm = TRUE)
sum(coded_data_replications$op_1_REV[coded_data_replications$sel_existing == "True, namely:"] == TRUE , na.rm = TRUE)

```

It was clear that existing measures were used in `r sum(coded_data_original$sel_existing == "True, namely:")` original and `r sum(coded_data_replications$sel_existing == "True, namely:")` replication cases, with specific version reported in `r sum(coded_data_original$op_1_REV[coded_data_original$sel_existing == "True, namely:"] == TRUE, na.rm = TRUE)` original and `r sum(coded_data_replications$op_1_REV[coded_data_replications$sel_existing == "True, namely:"] == TRUE , na.rm = TRUE)` replication cases.


???check if reporting of reliability/validity (ALL QMPS) more commonm for existing and sourced measures than new ones (relates to the idea of reporting bias I explore in the piece)???


## Questionable Measurement Practices

Besides Reliability and validity evidence it is also relevant to know what was measured to evaluate if it was measured well. What we found however was that for XXX original and XXX replication a clear variable assessed by the measurement was defined. The logic behind how that variable was operationalized was reported for XXX original and XXX replication. Finally, a reason was given for selecting an existing measure or creating a new measure in XXX original and XXX replication.

Several items also indicated how the measurement was implemented: XXX the administration format was indicated, XXX the assessment environment described, XXX the steps to administer the measure were described. To recreate/understand the items of the measure we looked at if they reported: XXX the number of items were described, XXX the number of response options were described, XXX any example items were shown. Finally, to use the item scores for analyses you need to know the following: XXX any recoding was explained, XXX the calculation of an index (if used) was described.

The concept of consistency in assessing a phenomenon across original and replication is important but also complex and difficult to access because … . However, we do we have some information on when modifications were made to certain aspects of the measure between original and replication. For XXX replications any administration format change was indicated, and for XXX of those that choice was justified/validated. For XXX replications any assessment environment change was indicated, and for XXX of those that choice was justified/validated. For XXX replications any change in the number of items or the number of response options was indicated, and for XXX of those that choice was justified/validated … some practical examples of numbers of items being changed ???to showcase that happening and prep that for discussion as being a reasonable reason to say that the same phenomenon may not even be assessed at that point???.

Table 2 below shows for both original and replication for all of the QMP items the ratio of QMPs and for how many of the 77 measures the QMP was applicable. Meanwhile, the Phi coefficient indicates the “correlation” between the MP being questionable or good in the original and the same for the replication. The average Phi was … .


```{r qmp_table}
# create informative Measurement Practice Labels
MPractice <- c("Reliability is reported", "It is clear if the measure existed or is newly created", 
  "Specifc version of the measure is specified", "Validity evidence from a factor analysis is presented",
  "The measured variable is defined", "The choice of measure selection or creation is justified", 
  "The implemented operationalisation is justified", "The source of the measure is provided", 
  "Psychometric evidence from the study is given", "Psychometric evidence from an earlier study is given", 
  "The administration format and environment are described", "The administration procedure is described",
  "The number of items are described", "The number of response options are described", 
  "The recoding of responses is described", "The creation of the index is described", 
  "Example items are presented in text or supplement", "Any administration format changes are mentioned", 
  "Administration format changes are justified/validated", "Any translations are mentioned", 
  "Translated measures are justified/validated", "Any changes in number of items or response options are mentioned", 
  "Changes in number of items or response options are justified/validated")


# construct the QMP table to be printed
QMP_table <- data.frame("Practice" = MPractice,
                        "QMP original" = paste0(qmp_ratio_data$QMP_percentage_original, 
                                                " (", qmp_ratio_data$N_applicable_original, ")"),
                        "QMP Replication" = paste0(qmp_ratio_data$QMP_percentage_replication, 
                                                " (", qmp_ratio_data$N_applicable_replication, ")"),
                        "Phi" = qmp_ratio_data$Phi)


# Changing column names to be less computer speak-y looking
colnames(QMP_table) <- c("Practice", "QMP org", "QMP rep", "Phi")


# print the table in apa formatting
apa_table(
  QMP_table, align = c("l", "r", "r", "r")
  , caption = "Ratio of QMPs and number of applicable measures, for original and replication"
  , note = "The first number in QMP org and QMP rep is the ratio of measures for which the practice in that row was not conducted well by our standards. The number between brackets indicates the number of measures for which the practice was applicable."
  , escape = FALSE, placement = "htp")

```


## Measurement Reliability
The average calculated Cronbach’s alpha coefficient across replication sets was 0.787 with a standard deviation 0.167. Figure 2 displays the distributions of the calculated Cronbach’s Alpha scores from each lab for each measure, separated by successful and unsuccessful replication, based on the meta-analytic p-value (alpha , .05) retrieved from the Many Labs reports.

```{r alpha_distributions_data_prep}
### PREPARE CODE FOR THE ALPHA DISTRIBUTIONS FIGURE 
calculated_alpha_plot_data <- calculated_reliability_lab_data

# calculating the average alpha per group
calculated_alpha_plot_data$avg.alpha <- ave(calculated_reliability_lab_data$alpha, 
                                            calculated_reliability_lab_data$g)

# making replication success a Boolean which can affect order
calculated_alpha_plot_data$replication_success <- ifelse(
  calculated_alpha_plot_data$replication_success == "Yes", TRUE, FALSE)

# reordering the plot from least to most reliable split by replication success
calculated_alpha_plot_data <- calculated_alpha_plot_data[order(-calculated_alpha_plot_data$replication_success, -calculated_alpha_plot_data$avg.alpha),]

# Making sure there are no calculated alpha's that fell below 0 in the plot.
calculated_alpha_plot_data <- calculated_alpha_plot_data[calculated_alpha_plot_data$alpha > 0,]

# reordering 
calculated_alpha_plot_data$g <- fct_inorder(as.factor(calculated_alpha_plot_data$g), ordered = NA)


# adding an index to which row in the plot data the row in measure_reliability_data belongs to
measure_reliability_data$graph_index <- NA

for (i in 1:nrow(measure_reliability_data)){
  measure_reliability_data$graph_index[which(unique(calculated_alpha_plot_data$g)[i] == measure_reliability_data$g)] <- i
}

```


```{r plot_alpha_distributions, warning = FALSE,  fig.cap = "Distributions of calculated Cronbach’s alpha coefficients (> 0) calculated for the responses on a measure at each lab location, across the nineteen measures for which raw data was available from which Cronbach’s alpha coefficients could be calculated. The green lines indicate the meta-analytic prediction interval lower and upper bound. The blue triangles indicate the reported alpha coefficient for that measure from the original article, when reported. The Tau column besides the figure shows the tau heterogeneity estimate based on a meta-analysis of the calculated reliabilities for each measure. Meta-analyses for which the Q-test for heterogeneity was significant at alpha < .05 are in black, while non-significant results are in grey. The Diff column shows the difference between reported reliability and the average reliability calculated from the Many Labs data for the applicable measures, the reported reliabilities that fell outside the 95% quantile of calculated reliability scores are shown in black, otherwise in grey."}
measure_reliability_data[!is.na(measure_reliability_data$coefficient_difference),]$graph_index


# plot for distribution of alpha
ggplot(calculated_alpha_plot_data, aes(x = alpha, y = g)) +
  geom_boxplot(outlier.shape = NA) +
  geom_hline(yintercept = 6.5, color = "red", size = 1) +
  geom_point(alpha = 0.1) +
  
  # adding in the tau values
  geom_text(data = measure_reliability_data, label = format(measure_reliability_data$tau, digits = 1), x = 1.15, y = measure_reliability_data$graph_index, color = ifelse(measure_reliability_data$QEp < .05, "black", "grey"), size = 2.8) +
  
  # adding in the difference in alpha coefficients
  geom_text(data = measure_reliability_data[c(1:8, 18),], label = format(measure_reliability_data$coefficient_difference[c(1:8, 18)], digits = 2), x = 1.28, y = measure_reliability_data[!is.na(measure_reliability_data$coefficient_difference),]$graph_index, color = ifelse(measure_reliability_data$significance_reported_coefficient[c(1:8, 18)], "black", "grey"), size = 2.8) +
  
  # setting the theme
  theme_minimal() +
  theme(legend.position = "none", plot.margin = unit(c(1, 6.5, 1, 1), "lines")) +
  
  # adding the necessary indicative texts
  annotation_custom(grob = textGrob(label = "Not Replicated", hjust = 0, gp = gpar(fontsize = 10)), ymin = 7.25, ymax = 7.25, xmin = 0.01, xmax = 0.01) +
  annotation_custom(grob = textGrob(label = "Replicated", hjust = 0, gp = gpar(fontsize = 10)), ymin = 6, ymax = 6, xmin = 0.01, xmax = 0.01) +
  annotation_custom(grob = textGrob(label = "Tau", hjust = 0, gp = gpar(fontsize = 12)), ymin = 20.2, ymax = 20.2, xmin = 1.1, xmax = 1.1) +
  annotation_custom(grob = textGrob(label = "Diff", hjust = 0, gp = gpar(fontsize = 12)), ymin = 20.2, ymax = 20.2, xmin = 1.24, xmax = 1.24) +
  
  coord_cartesian(xlim = c(0, 1), clip = "off") +
  
  
  # adding the blue triangles for reported reliability and green prediction intervals
  geom_point(data = measure_reliability_data, mapping = aes(x = reported_coefficient, y = graph_index), color = "blue", shape = 17, size = 3) +
  # we remove 14 & 15 lower bound because they are below 0
  geom_point(data = measure_reliability_data[-c(14, 15),], mapping = aes(x = pi.lb, y = graph_index), color = "green", shape = 124, size = 2.5) + 
  geom_point(data = measure_reliability_data, mapping = aes(x = pi.ub, y = graph_index), color = "green", shape = 124, size = 2.5) + 
  
  ylab("") +
  xlab("Cronbach's alpha") 



```

We found some indication of heterogeneity for fourteen of the nineteen measures. We observe that for most measures the reported reliabilities were generally lower than the reliabilities calculated for the replication labs. We also observed that original reliabilities were more commonly reported for those measures that had higher average reliabilities among the replications (>.80) compared to on average less reliable measures. 

However,  to be able to interpret Cronbach’s alpha, the underlying assumptions have to be met: undimensionality, uncorrelated item errors, and Tau equivalence. Our CFA results show (summarized in Table 3 below) that … .

```{r validity_and_alpha_assumption_tests, include = FALSE, eval = FALSE}
# function that runs all our checks for Cronbach's alpha. Also functions as some basic validity checks
validity_and_alpha_assumption_tests <- function(data){
  ### Tests for Unidimensionality
  # Test 1a: obtaining single factor model cfa RMSEA value for evaluation
  RMSEA_values <- c(NA, NA)
  tryCatch({
    # creating the base function for the model (it is unidimensional)
    model_free <- "Factor =~ " 

    for (item_name in names(data)[-1]){
      # adding all of the items from the scale to the model formula
      model_free <- paste0(model_free, item_name, " + ")
    }
      
    # trimming the excess " + "
    model_free <- substring(model_free, 1, nchar(model_free) - 3)

    # fitting the free coefficient estimated single dimensional model
    fit.modelfree <- cfa(model_free, 
                              data = data, 
                              std.lv = TRUE, 
                              estimator = "MLM")

    rmsea <- fitMeasures(fit.modelfree)["rmsea"] 
    rmsea.ci.upper <- fitMeasures(fit.modelfree)["rmsea.ci.upper"]
    rmsea.se <- (rmsea.ci.upper - rmsea) / 1.645
    RMSEA_values <- c(rmsea, rmsea.se)
    
  }, error = function(e) {
    RMSEA_values <- c(NA, NA)
  })
  
  
  # Test 1b: conducting parallel test to check if one factor solution is best.
  parallel_N_factors <- NA
  tryCatch({
      parallel_N_factors <- suppressWarnings(fa.parallel(data[-1], fa = "fa", plot = FALSE))$nfact

    }, error = function(e) {
      parallel_N_factors <- NA
  })
  
  # adding a single factor check indicator value
  if(is.null(RMSEA_values[[1]]) | is.null(parallel_N_factors)){
    single_factor <- NA
  } else{
    single_factor <- ifelse(RMSEA_values[[1]] < 0.08 | parallel_N_factors == 1, "Yes", "No")
  }
  
  
  ### Test 2: Test for Tau equivalence
  Tau_results <- c(NA, NA, NA)
  tryCatch({
    # creating the base function for the fixed model (it is unidimensional)
    model_tau_restrict <- "Factor =~ " 
  
    for (item_name in names(data)[-1]){
      # adding all of the items from the scale to the model formula
      # estimated with the same coefficient a to emulate a Tau equivalent model
      model_tau_restrict <- paste0(model_tau_restrict, "a*", item_name, " + ")
    }
    
    # trimming the excess " + "
    model_tau_restrict <- substring(model_tau_restrict, 1, nchar(model_tau_restrict) - 3)
    
    # fitting the tau restricted model
    fit.modeltaurestrict <- cfa(model_tau_restrict, 
                                     data = data, 
                                     std.lv = TRUE, 
                                     estimator = "MLM")
    
    #summary(fit.modelfree, fit.measures = TRUE)
    #summary(fit.modeltaurestrict, fit.measures = TRUE)
    
    # conducting the fit test between the free coefficient model and the tau
    # restricted model
    fit.test <- anova(fit.modelfree, fit.modeltaurestrict)
    
    # extract the relevant results
    Tau_results <- fit.test[2, c("Chisq diff", "Df diff", "Pr(>Chisq)")]
    }, error = function(e) {
      Tau_results <- c(NA, NA, NA)
  })
  
  ### Test 3: Assessment of uncorrelated errors
  error_cor_results <- rep(NA, 4)
  
  
  tryCatch({
    # copying the tau restricted model as the baseline.
    model_freed_errors <- model_tau_restrict
    fit.modelfreed <- fit.modeltaurestrict
    
    # create empty vector to put data into
    spec.all.vec <- rep(NA, 5)
    
    # looping through five instances of freeing the highest mod index error 
    # covariance parameter estimation. We loop 5 times rather than adding the top 5
    # highest mod index, because when one parameter is freed it will affect the extent 
    # to which other error covariances impact the fit since (part of) their additional 
    # explained variance may have already been captured in the parameter just freed.
    for (i in 1:5) {
      modindex_highest <- suppressWarnings(modindices(fit.modelfreed, sort = TRUE))[1,]
      
      # store the fully standardized estimated coefficient change for the highest
      # mod index freed parameter
      spec.all.vec[i] <- modindex_highest[[7]]
      
      # add the freeing of the parameter into the model code
      model_freed_errors <- paste(model_freed_errors, "\n", modindex_highest[[1]], modindex_highest[[2]], modindex_highest[[3]])
      
      # rerun the model
      fit.modelfreed  <- cfa(model_freed_errors, 
                             data = data, 
                             std.lv = TRUE, 
                             estimator = "MLM")
    }
  
    # perform a model comparison between the model with the top 5 error covariances
    # freely estimated and the model with Tau equivalence.
    fit.test2 <- anova(fit.modeltaurestrict, fit.modelfreed)
    
    error_cor_test_results <- fit.test2[2, c("Chisq diff", "Df diff", "Pr(>Chisq)")]
    
    # calculate the mean of the fully standardized estimated coefficient change for
    # all the highest mod index freed parameters
    spec.all.mean <- mean(spec.all.vec)
    
    # store all error related results
    error_cor_results <- unlist(c(spec.all.mean, error_cor_test_results))  
    
    }, error = function(e) {
      error_cor_results <- NA
  })
  
  
  ### storing all of the data
  FA_data <- c(RMSEA_values, parallel_N_factors, single_factor, 
               Tau_results, error_cor_results)
  
  # returning the FA data
  return(FA_data)
}



extracted_data_list <- list(data_1.10_clean, data_1.11_clean, data_1.12.3.1_clean, data_1.12.3.2_clean, data_2.12.1_clean, data_2.12.2_clean, data_2.12.3_clean, data_2.15_clean, data_2.20_clean, data_2.23_clean, data_3.2.1.1_clean, data_3.2.1.2_clean, data_3.7.1_clean, data_3.7.2_clean, data_3.8.2_clean, data_5.1.1_clean, data_5.1.2_clean, data_5.7_clean, data_5.9.1_clean)


# list to store the data in
FA_list <- list(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
                NA, NA, NA, NA, NA, NA, NA, NA, NA)

# running the validity/alpha assumption checks for all of the extracted datasets
for (i in 1:length(extracted_data_list)){
  # applying the function across each individual lab
  FA_data <- tapply(extracted_data_list[[i]], extracted_data_list[[i]]$g,
                    validity_and_alpha_assumption_tests)

  # transforming the returned list to a dataframe
  FA_data_frame <- do.call(rbind.data.frame, FA_data)
  
  # Setting the column names
  names(FA_data_frame) <- c("RMSEA", "RMSEA_se", "N_factors", "unidimensional", "Tau_chisq_diff", 
                            "Tau_Df_diff", "Tau_p_diff", "mean_spec", "err_chisq_diff", 
                            "err_Df_diff", "err_p_diff")
  
  # append results to the overarching FA results list
  FA_list[[i]] <- FA_data_frame
}




# store all dataframes with the list of the assumption test data in 
# the analysis data folder
fa_Caruso_2012 <- FA_list[[1]]
saving_to_analysis_data(fa_Caruso_2012)
fa_Husnu_2010 <- FA_list[[2]]
saving_to_analysis_data(fa_Husnu_2010)
fa_Nosek_2002_Math <- FA_list[[3]]
saving_to_analysis_data(fa_Nosek_2002_Math)
fa_Nosek_2002_Art <- FA_list[[4]]
saving_to_analysis_data(fa_Nosek_2002_Art)
fa_Anderson_2012_SWL <- FA_list[[5]]
saving_to_analysis_data(fa_Anderson_2012_SWL)
fa_Anderson_2012_PA <- FA_list[[6]]
saving_to_analysis_data(fa_Anderson_2012_PA)
fa_Anderson_2012_NA <- FA_list[[7]]
saving_to_analysis_data(fa_Anderson_2012_NA)
fa_Giessner_2007 <- FA_list[[8]]
saving_to_analysis_data(fa_Giessner_2007)
fa_Norenzayan_2002 <- FA_list[[9]]
saving_to_analysis_data(fa_Norenzayan_2002)
fa_Zhong_2006 <- FA_list[[10]]
saving_to_analysis_data(fa_Zhong_2006)
fa_Monin_2001_most <- FA_list[[11]]
saving_to_analysis_data(fa_Monin_2001_most)
fa_Monin_2001_some <- FA_list[[12]]
saving_to_analysis_data(fa_Monin_2001_some)
fa_Cacioppo_1983_arg <- FA_list[[13]]
saving_to_analysis_data(fa_Cacioppo_1983_arg)
fa_Cacioppo_1983_nfc <- FA_list[[14]]
saving_to_analysis_data(fa_Cacioppo_1983_nfc)
fa_De_Fruyt_2000 <- FA_list[[15]]
saving_to_analysis_data(fa_De_Fruyt_2000)
fa_Albarracin_2008_verb <- FA_list[[16]]
saving_to_analysis_data(fa_Albarracin_2008_verb)
fa_Albarracin_2008_math <- FA_list[[17]]
saving_to_analysis_data(fa_Albarracin_2008_math)
fa_Shnabel_2008 <- FA_list[[18]]
saving_to_analysis_data(fa_Shnabel_2008)
fa_Vohs_2008 <- FA_list[[19]]
saving_to_analysis_data(fa_Vohs_2008)

rm(extracted_data_list)



```

```{r internal_structure_information}
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

FA_aggregate_data <- data.frame(RMSEA = rep(NA, 19), 
                                RMSEA_se = rep(NA, 19), 
                                RMSEA_global_mean = rep(NA, 19), 
                                RMSEA_mean_sd = rep(NA, 19), 
                                N_factors_mode = rep(NA, 19),
                                unidimensional_ratio = rep(NA, 19),
                                Tau_N_P_10 = rep(NA, 19),
                                Tau_N_P_05 = rep(NA, 19),
                                Tau_N_P_01 = rep(NA, 19),
                                global_mean_spec = rep(NA, 19),
                                global_sd_spec = rep(NA, 19),
                                err_N_P_10 = rep(NA, 19),
                                err_N_P_05 = rep(NA, 19),
                                err_N_P_01 = rep(NA, 19))



### recreate FA_list
FA_list <- list(fa_caruso_2012, fa_husnu_2010, fa_nosek_2002_math, fa_nosek_2002_art,
                fa_anderson_2012_swl, fa_anderson_2012_pa, fa_anderson_2012_na,
                fa_giessner_2007, fa_norenzayan_2002, fa_zhong_2006, fa_monin_2001_most,
                fa_monin_2001_some, fa_cacioppo_1983_arg, fa_cacioppo_1983_nfc, 
                fa_de_fruyt_2000, fa_albarracin_2008_verb, fa_albarracin_2008_math,
                fa_shnabel_2008, fa_vohs_2008)



for (i in 1:19){
  data <- FA_list[[i]]
  
  tryCatch({
    RMSEA_estimate <- rma(yi = as.numeric(RMSEA), sei = as.numeric(RMSEA_se), data = data) # inverse variance weighting is on by default
    FA_aggregate_data$RMSEA[i] <- RMSEA_estimate$b
    FA_aggregate_data$RMSEA_se[i] <- RMSEA_estimate$se
    
  }, error = function(e) {
    
  })
  
  FA_aggregate_data$RMSEA_global_mean[i] <- mean(as.numeric(data$RMSEA), na.rm = TRUE)
  FA_aggregate_data$RMSEA_mean_sd[i] <- sd(as.numeric(data$RMSEA), na.rm = TRUE)
  
  FA_aggregate_data$N_factors_mode[i] <- Mode(data$N_factors)
  
  if("Yes" %in% data$unidimensional){
    FA_aggregate_data$unidimensional_ratio[i] <- table(data$unidimensional)[["Yes"]] / length(data$unidimensional)
  } else{
    FA_aggregate_data$unidimensional_ratio[i] <- 0
  }
  
  # count number of Tau differences (don't know anything better for now)
  FA_aggregate_data$Tau_N_P_10[i] <- sum(as.numeric(data$Tau_p_diff) < .1, na.rm = TRUE) / length(na.omit(data$Tau_p_diff))
  FA_aggregate_data$Tau_N_P_05[i] <- sum(as.numeric(data$Tau_p_diff) < .05, na.rm = TRUE) / length(na.omit(data$Tau_p_diff))
  FA_aggregate_data$Tau_N_P_01[i] <- sum(as.numeric(data$Tau_p_diff) < .01, na.rm = TRUE) / length(na.omit(data$Tau_p_diff))
  
  FA_aggregate_data$global_mean_spec[i] <- mean(as.numeric(data$mean_spec), na.rm = TRUE)
  FA_aggregate_data$global_sd_spec[i] <- sd(as.numeric(data$mean_spec), na.rm = TRUE)
  
  # count number of err differences (don't know anything better for now)
  FA_aggregate_data$err_N_P_10[i] <- sum(as.numeric(data$err_p_diff) < .1, na.rm = TRUE) / length(na.omit(data$err_p_diff))
  FA_aggregate_data$err_N_P_05[i] <- sum(as.numeric(data$err_p_diff) < .05, na.rm = TRUE) / length(na.omit(data$err_p_diff))
  FA_aggregate_data$err_N_P_01[i] <- sum(as.numeric(data$err_p_diff) < .01, na.rm = TRUE) / length(na.omit(data$err_p_diff))
}




### PLOTs
# ggplot(FA_aggregate_data[-15,], aes(x = RMSEA_global_mean, y = 0)) +
#   geom_boxplot(outlier.shape = NA, width = 2) +
#   geom_point(alpha = 0.7) +
#   xlab("Global Mean of RMSEA for Single Factor Model") +
#   ylim(-10, 10) +
#   theme_apa(base_size = 20)
# 
# ggplot(FA_aggregate_data[-15,], aes(x = unidimensional_ratio, y = 0)) +
#   geom_boxplot(outlier.shape = NA, width = 2) +
#   geom_point(alpha = 0.7) +
#   ylim(-10, 10) +
#   xlab("Ratio of Labs with Unidimensional Factor Solution") +
#   theme_apa(base_size = 20)
# 
# ggplot(FA_aggregate_data[-15,], aes(x = N_factors_mode)) +
#   geom_histogram(stat = "count", colour = "#28292A") +
#   scale_y_continuous(breaks = seq(0, 12, by = 2)) +
#   xlab("Mode of N Factors Selected by Parallel Analysis") +
#   theme_apa(base_size = 20)

# apa_table(
#   FA_aggregate_data, align = c("l", rep("r", 13))
#   , caption = "Internal Structure indicators of applicable measures"
#   , note = "Because of a lack of convergence in the meta-analytic model, there are no meta-analytic RMSEA results. Measure fifteen had failed to deliver results for most steps during the individual factor analysis phase of the analyses. "
#   , escape = FALSE, placement = "htp")


print(FA_aggregate_data)
```


Besides testing for Cronbach’s assumptions, we can also use the more informative Omega coefficient. … This one shows XXX, compared to our CFA + reliability results this shows … .

```{r omega_distributions_data_prep}
# checking for each data row that they have a calculated omega coefficient,
# and only extracting those.
calculated_omega_plot_data <- calculated_reliability_lab_data[!is.na(calculated_reliability_lab_data$omega.tot), ]

# making replication success a boolean which can affect order
calculated_omega_plot_data$replication_success <- ifelse(
  calculated_omega_plot_data$replication_success == "Yes", TRUE, FALSE)
  
# calculating the average omega per group
calculated_omega_plot_data$avg.omega <- ave(calculated_omega_plot_data$omega.tot, calculated_omega_plot_data$g)

# reordering the plot from least to most reliable split by replication success
calculated_omega_plot_data <- calculated_omega_plot_data[order(-calculated_omega_plot_data$replication_success, -calculated_omega_plot_data$avg.omega),]
calculated_omega_plot_data$g <- fct_inorder(as.factor(calculated_omega_plot_data$g), ordered = NA)


table(calculated_omega_plot_data$g)

# reorder the total values to fit with the omega ordering
table(calculated_reliability_lab_data$g)[c(15, 16, 17, 14, 11, 18, 4, 3, 10, 6,
                                           5, 8, 19, 7, 13, 2, 1, 12, 9)]

```

```{r  plot_omega_distributions, warning = FALSE,  fig.cap = "Distributions of calculated McDonald’s Omega t  calculated for the responses on a measure at each lab location, across the eighteen measures for which raw data was available from which McDonald’s Omega t coefficients could be calculated. The N.Omg column besides the figure shows the number of lab locations for which an Omega coefficient could be successfully calculated. Due to lack of underlying model convergence an Omega coefficient could not be calculated for all lab locations, or any lab locations for the Der Fruyt et al. (2000) measure. The N.tot column shows the total number of lab locations at which the measure was taken. Any difference between N.omg and N.tot can be seen as an indication of the instability to get a stable estimate of an underlying facto model for some lab locations for that measure."}
# plot for omega
ggplot(calculated_omega_plot_data, aes(x = omega.tot, y = g)) +
  geom_boxplot(outlier.shape = NA) +
  geom_point(alpha = 0.1) +
  
  # adding labels to show the number of labs per measure for which a total omega could be calculated
  geom_text(data = calculated_omega_plot_data[1:18,], label = table(calculated_omega_plot_data$g)[1:18], x = 1.15, y = 1:18, size = 2.8) +
  # adding labels to show the number of labs per measure
  geom_text(data = calculated_omega_plot_data[1:18,], label = table(calculated_reliability_lab_data$g)[c(15, 16, 17, 14, 11, 18, 4, 3, 10, 6, 5, 8, 19, 7, 13, 2, 1, 12)], x = 1.35, y = 1:18, size = 2.8) +
  
  # theming
  theme_minimal() +
  theme(legend.position = "none", plot.margin = unit(c(1, 6.5, 1, 1), 
                                                     "lines")) +
  
  # adding the replication line
  geom_hline(yintercept = 6.5, color = "red", size = 1) +
  
  # adding the necessary indicative texts
  annotation_custom(grob = textGrob(label = "Not Replicated", hjust = 0, gp = gpar(fontsize = 10)), ymin = 7.25, ymax = 7.25, xmin = 0.01, xmax = 0.01) +
  annotation_custom(grob = textGrob(label = "Replicated", hjust = 0, gp = gpar(fontsize = 10)), ymin = 6, ymax = 6, xmin = 0.01, xmax = 0.01) +
  annotation_custom(grob = textGrob(label = "N.Omg", hjust = 0, gp = gpar(fontsize = 12)), ymin = 19.2, ymax = 19.2, xmin = 1.08, xmax = 1.08) +
  annotation_custom(grob = textGrob(label = "N.Tot", hjust = 0, gp = gpar(fontsize = 12)), ymin = 19.2, ymax = 19.2, xmin = 1.28, xmax = 1.28) +
  
  coord_cartesian(xlim = c(0, 1), clip = "off") +
  
  ylab("") +
  xlab("Omega")

```




## Conflicts of Interest

The author(s) declare that there were no conflicts of interest with respect to the authorship or the publication of this article.

## ORCID iDs

Cas Goos <https://orcid.org/0009-0005-3792-4148>

Marjan Bakker <https://orcid.org/0000-0001-9024-337X>

Jelte M. Wicherts <https://orcid.org/0000-0003-2415-2933>

Michèle B. Nuijten <https://orcid.org/0000-0002-1468-8585>

## Funding

The preparation of this article was supported by the Veni grant VI.Veni.201G.003 awarded to Michèle Nuijten, and the Vici grant VI.C.221.100 awarded to Jelte M. Wicherts from the Dutch Research Council (NWO).

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
