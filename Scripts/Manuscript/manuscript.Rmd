---
title             : "An Empirical Assessment of Reliable and Valid Measurement as a Prerequisite for Informative Replications"
shorttitle        : "Measurement and Informative Replications"

author: 
  - name          : "Goos, C."
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Professor Cobbenhagenlaan 125, 5037 DB, Tilburg, The Netherlands"
    email         : "c.goos@tilburguniversity.edu"
    role: # Contributorship roles (e.g., CRediT, https://credit.niso.org/)
      - "Conceptualization"
      - "Data curation"
      - "Formal Analysis"
      - "Investigation"
      - "Methodology"
      - "Project Administration"
      - "Software"
      - "Visualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
  - name          : "Bakker, M."
    affiliation   : "1"
    role:
      - "Conceptualization"
      - "Supervision"
      - "Writing - Review & Editing"
  - name          : "Wicherts, J.M."
    affiliation   : "1"
    role:
      - "Conceptualization"
      - "Supervision"
      - "Writing - Review & Editing"
  - name          : "Nuijten, M.B."
    affiliation   : "1"
    role:
      - "Conceptualization"
      - "Project Administration"
      - "Supervision"
      - "Validation"
      - "Writing - Review & Editing"

affiliation:
  - id            : "1"
    institution   : "Tilburg University"

authornote: |
  1 Department of Methodology and Statistics, Tilburg School of Social and Behavioral Sciences, Tilburg University, Tilburg, NL. 


abstract: |
  The credibility of psychological science and the ability to replicate earlier findings require good measurement.
   Here, we assessed the reliability and measurement reporting of 77 measures within 56 Many Labs replications and related original psychological studies. 
   We investigated their associations with replicability.
   The results indicated that not all measures in the original and replication studies were sufficiently reliable across contexts. 
   Additionally, evidence on reliability and validity was rarely reported. 
   Finally, incompleteness in measurement reporting in replication studies was associated with lower replicability of the findings tested with that measure. 
   These findings indicate a worrying lack of reported validating evidence for measurement in the literature, and its harm to credibility.
   We offer suggestions on improving measurement validation and reporting practices, and argue that researchers should evaluate the reported measurement information when deciding what studies to replicate.


keywords          : "reliability, validity, measurement, reporting, replicability, credibility"
wordcount         : "135"

bibliography      : ["r-references.bib", "references.bib"]

floatsintext      : yes
linenumbers       : no
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

header-includes:
  - |
    \makeatletter
    \renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-1em}%
      {\normalfont\normalsize\bfseries\typesectitle}}
    
    \renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-\z@\relax}%
      {\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
    \makeatother

csl               : "`r system.file('rmd', 'apa7.csl', package = 'papaja')`"
documentclass     : "apa7"

classoption       : "man"
output            : papaja::apa6_pdf
knit              : worcs::cite_all
---

```{r setup, include = FALSE}
# loading R libraries
library(papaja)
library(worcs)
library(tidyr)
library(betareg)
library(lmerTest)
library(psych)
library(forcats)
library(GPArotation)
library(ggplot2)
library(ggridges)
library(GGally)
library(ggforce)
library(grid)
library(gridExtra)
library(metafor)

# loading source script
source(file = "../source_script.R")

# Code below loads the processed data. The raw data was prepared for analysis in 'prepare_data.R.
load_data() 

# creates a reference list for all used R packages and the installed R version (does not automatically include Rstudio)
r_refs("r-references.bib")

# manually retrieving the Rstudio version
rstudioapi::versionInfo()

```

<!-- altering latex defaults to get better figure and table placement -->

\renewcommand{\arraystretch}{0.7}

<!-- reducing the line spacing within tables -->

\renewcommand{\topfraction}{.8}

<!-- max fraction of page for floats at top -->

\renewcommand{\bottomfraction}{.8}

<!-- max fraction of page for floats at bottom -->

\renewcommand{\textfraction}{.15}

<!-- min fraction of page for text -->

\renewcommand{\floatpagefraction}{.8}

<!-- min fraction of page that should have floats .66 -->

\setcounter{topnumber}{3} <!-- max number of floats at top of page -->

\setcounter{bottomnumber}{3} <!-- max number of floats at bottom of page -->

\setcounter{totalnumber}{4} <!-- max number of floats on a page -->

<!-- remember to use [htp] or [htpb] for placement -->

```{r analysis-preferences}
# Seed for random number generation
set.seed(17042023)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

<!-- Introduction -->

For solid scientific progress in psychology, we need to be able to rely on previous findings - findings should be credible. Unfortunately, psychology seems to face several threats to the credibility of its findings. In this article, we investigate how the credibility of psychological findings may be affected by problems in psychological measurement, and in turn, how those problems may affect subsequent replications.

## Credibility & Replicability

A prerequisite for a credible finding is that it is replicable [@vazire2022CredibilityReplicabilityImproving]. This means that the observed effect should be comparable across the original study and subsequent replications if the differences between them are believed to be of little relevance to the effect [@nosek2022replicability].

But replications often fail. In 2015, the Replication Project: Psychology (RPP) attempted replications of 100 research findings in psychology, they observed a substantial discrepancy between the replication findings and the original findings [@osc2015estimating]. Similarly, the Many Labs projects aimed to replicate the effects reported in original articles across different labs in various locations around the world [@klein2014investigating; @ebersole2016many; @ebersole2020many; @klein2018many; @klein2022many], and often found smaller effects in replications compared to original research. Furthermore, the estimates of the effect varied between different lab locations.

In response to this string of failed replications, there was widespread alarm regarding the credibility and robustness of findings in psychological science [@hughes2018psychology; @giner2019crisis], and efforts to explain failures to replicate. The factors that have been proposed include, questionable research practices [@simmons2011false; @john2012measuring; @cumming2014new; @wicherts2016degrees], questionable reporting practices [@bakker2011mis; @nuijten2016prevalence], and biased publication decisions [@sterling1959publication; @bakker2012rules; @giner2012science]. Another factor that has gained more traction recently is the impact of measurement-related challenges on replicability and credibility.

## Credibility & Measurement

Psychological constructs, such as affective state or intelligence, can not be measured directly [@flake2017construct]. Psychological measurements involve random measurement errors and uncertainties about whether the targeted construct is indeed well reflected in the scores retrieved from the measurement procedure. 

The first issue relates to the concept of reliability, which gives an indication of how consistent the responses on a measurement are. Reliability is a critical first step to obtaining credible findings. Because if the scores on a measure are not consistent with themselves, it is unlikely that any effect associated with the measure can credibly be established. Psychometrics offers various ways to assess reliability empirically [e.g. @nunnally1978overview; @mellenbergh2011conceptual].

The second issue relates to the concept of validity, specifically construct validity, or the overall extent to which a measure measures what it is supposed to  measure [@borsboom2004concept]. Construct validity of test scores is often assessed by comparing the test scores’ empirical associations to the theoretical associations of the constructs [@cronbach1955construct]. A researcher cannot claim to have found a credible psychological effect, if the measures do not relate to the psychological construct [@cook2002experimental].

## Replicability & Measurement

While reliable and valid measurement, as well as replicability are important elements that independently contribute to the overall credibility of a finding, there may also be a relation between them. Indeed, recent studies have illustrated that both the reliability of the measure [@stanley2014expectations] and the reporting of information relevant to demonstrating the validity of the measurement [@shaw2020measurement; @flake2017construct; @flake2020measurement] may be related to the chance of successfully replicating a psychological finding. This study aims to investigate the relation of reliability and measurement reporting with replicability, using data and reports from the large scale Many Labs replication projects.

## Many Labs Sample Description

Because the Many Labs projects involved multiple lab locations over the world directly replicating the same effect, their data allows us to study the variability in these measures across different contexts. Furthermore, the Many Labs projects used preregistered and documented structured protocols, we therefore believe the measurement use in these projects represent a high standard within the field. Any issues in measurement here is thus taken as an indication that other replications are equally if not more likely to face the same issues. 

## Existing Research

### Reliability

While reliability is generally considered a measure specific feature, this is false. The reliability of a measure varies between samples, and thus  [@cho2015cronbach; @pauly2018resampling]. Besides random variation due the sampling error, the variations can also reflect true variation in reliability, also known as reliability heterogeneity [@vacha-haase1998ReliabilityGeneralizationExploring]. For instance, measures are known to generally show lower reliability in more homogeneous samples as compared to heterogeneous samples [@pike1998reliability].

The resulting variations in the measurement scores could contribute to the discrepancy between effects in original and replication research, as well as within replications of the same original effect. After all it has long been known that noise in measurement suppresses observed effects and associations [@spearman1904proof]. @stanley2014expectations simulated data of items on a scale based on levels of sampling error and reliability similar to what is standard in psychological research. The results showed that the variation due to measurement error was substantial enough to cause replications of a positive small effect to observe anywhere between a medium negative effect and a large positive effect [@stanley2014expectations]. In summary, studies using unreliable measures are less replicable, even when the measurement is equally reliable between original and replication.

### Measurement Reporting

Despite the importance of having reliable and valid measures, research has shown that Questionable Measurement Practices (QMPs) are not uncommon. QMPs are practices that raise doubts about a measurement's validity [@flake2020measurement], and have been coined as a conjugate term to Questionable Research Practices [QRPs, @john2012measuring]. QMPs range from lack of transparency and unclear motivation in choice of measure, to poor justification for modifications of measure and procedure of any sourced measures [@flake2020measurement]. More specifically,  @flake2020measurement identified six key questions than an article should answer to avoid QMPs: the definition of the construct, how the measure was selected, how the measure was operationalized, how the measure was quantified, whether it was modified or not and why, and the reasons and details of creating a measure if applicable. If these questions can be answered based on the reported measurement information, then reporting is sufficiently transparent to promote a more cumulative psychological science.

@flake2022construct documented QMPs among 100 replications and their respective original articles from the RPP [@osc2015estimating]. They coded the number of measures, the number of items in the measure, and the information that was reported describing the measure and providing evidence justifying its use. Besides limited reporting of validity and reliability evidence for the coded measures, @flake2022construct found that several of the measures in the RPP did not report the number of items, the response format, or the scoring of the scale. Furthermore, only eight of the 40 translated scales contained validity evidence for the translated version of the scale. Measures were similarly modified between original and replication in other ways without evidence showing that the modification did not invalidate the measurement.

Further findings by @flake2022construct and others [@flake2017construct; @shaw2020measurement] also illustrated how QMPs create challenges for replicating researchers. In order to reconstruct the measurement, replication researchers need to know the items that were used, how they were presented to the subjects, and how to compute the scores. If the measure is not reconstructed exactly, constructs measured by original and replication may differ. leading to differences in effect size estimates irrespective of the credibility of the original effect.

### Research Contribution

Our study investigated the relation of reliability and measurement reporting with replicability, using data and reports from the large scale Many Labs replication projects. First, this study expands on the research by @stanley2014expectations. Our goal was to see whether reliability was related to replication success on empirical replication data. Specifically, we investigated whether replicated studies differed in their reliability from non-replicated studies.  @stanley2014expectations assumed reliability to be constant across studies, which we know to be an oversimplification. Therefore, to add further context to the relation between reliability and replication, this study also investigated the variation in reliability as observed across labs.

Furthermore, this research conceptually replicates the study by @flake2022construct on QMPs in replications and original research, now in the context of the Many Labs projects. We then expand on @flake2022construct by exploring associations between QMPs and the replicability of psychological findings.

## Research Questions & Hypotheses

### Reliability

Measurement reliability varies across samples. It is crucial for evaluating replications that differences between replication and original are understood and accounted for.

-   RQ1a. What is the reliability in replications of psychological research?
-   RQ1b. What is the reliability in original psychological research?

The Many Labs studies were intended as direct replications. In direct replications, any deviations from the original research are believed to be irrelevant for testing the same effect as the original study [@nosek2022replicability]. In that case, the reliability of measurements in replications should also not deviate systematically from the original research.

-   H1. No difference is present in the reliability between replication research and original psychological research.

Reliability has the potential to vary not only between replication and original research, but also across replicating labs assessing the same effect [@vacha-haase1998ReliabilityGeneralizationExploring].

-   RQ2. Do reliability estimates differ between replicating labs?

Heterogeneity in reliability may be contributing strongly to the variation in reliability between labs. However, for effect sizes there is little empirical evidence of widespread heterogeneity among the Many Labs replications [@klein2018many; @olsson2020heterogeneity]. We expect the same for the heterogeneity in reliability coefficients.

-   H2. There is no significant variation in the reliability estimates of replications of the same original study.

To see if reliability is related to replication success, we investigated whether successfully replicated studies differ in the reliability of their measures as compared to non-replicated studies.

-   RQ3. What is the association between replication study reliability and replication outcome?

Greater reliability means the variance around the estimate of the true effect is decreased [@nunnally1994assessment]. Assuming that the true effect is not null, then the statistical conclusions tested with reliable measures are more likely to converge to significance.

-   H3. Reliability in replication research is positively associated with successful replication of an original finding.

### Measurement Reporting

This study aims to conceptually replicate the findings on measurement reporting of @flake2022construct.

-   RQ4a. How common are QMPs in replications of psychological research?
-   RQ4b. How common are QMPs in original psychological research?

@flake2022construct found that QMPs were overall more common in the RPP replications as compared to the original research. We did not hypothesize this to be the case in the Many Labs sample, because of the use of structured protocols documenting the measurement.

-   H4. QMPs are more frequent in original psychological research than in replication research.

QMPs obscure information relevant to mimic measurement from original research in subsequent replication attempts. The resulting deviations in measurement may be partly responsible for deviations in replication and original effects [@flake2022construct]. 

-   RQ5. What is the association between QMPs in replication studies and replication outcome?

A reduction in QMPs corresponds to greater transparency in reporting of measurement. In line with earlier research finding other transparency related practices to be associated with more robust estimates of effects [@wicherts2011willingness; @protzko2020high], we expect:

-   H5. QMPs in replications are negatively associated with replication success.

QMPs obscure information about a particular measurement. This in turn may cause issues for reconstructing that measurement in subsequent replication attempts. 

-   RQ6. What is the association between QMPs in original research and QMPs in replications of psychological research?

@flake2022construct already proposed that QMPs in original articles may cause issues for recreating measurements for replication research. @shaw2020measurement already found such a spill-over effect for validity. In line with this research we expect:

-   H6. The total number of QMPs in original psychological research is positively related to the total number of QMPs in replication research.

# Disclosures

### Preregistration

We preregistered data collection, coding protocol, and planned analyses: <https://osf.io/9r8yt/>. Deviations from the preregistration are explicitly mentioned in the text.

### Data, materials, and online resources

This manuscript was created in RStudio [@R-Rstudio] with R Version `r paste0(R.Version()$major, ".", R.Version()$minor)` [@R-base], and generated using the Workflow for Open Reproducible Code in Science [WORCS version 0.1.1, @vanlissaWORCS2021] to ensure reproducibility and transparency. All code and data used to generate this manuscript and its results are available at: <https://github.com/CasGoos/measurement_and_replication> and <https://osf.io/9r8yt/>.

### Reporting

We report how we determined all data exclusions, all manipulations, and all measures in the study. Our sample size was predetermined by the number of studies in the Many Labs projects.

### Ethical Approval

This research was approved by the Tilburg University School of Social and Behavioral Sciences Ethical Review Board (nr. TSB_TP_REMA06).

# Method

## Sample

### Data Source

The data used for the analyses consisted of three main sources: replication datasets, replication protocols, and original study articles. The data came from the Many Labs replication projects. Specifically, data from Many Labs 1, 2, 3, & 5 [@klein2014investigating; @klein2018many; @ebersole2016many; @ebersole2020many]. Many Labs 4 [@klein2022many] was excluded, as there was no publicly available replication protocol. Additionally, the replication of [@crosby2008we] in Many Labs 5 made use of videos and eye-tracking measures, which did not match this study's focus on item-based measures.

### Unit of analysis

The unit of analysis in this study was a measure of a psychological construct used in the primary analysis that was being replicated.  We used the the replication protocols to identify these measures. Acquiescence bias checks, manipulation checks, pilot test measures, and measures added for exploratory analyses were not included. 

## Data Collection

We retrieved the data on the replication protocols, and replication datasets of Many Labs 1, 2, 3, & 5 from their respective OSF pages: <https://osf.io/wx7ck/>, <https://osf.io/8cd4r/>, <https://osf.io/ct89g/>, & <https://osf.io/7a6rd/>. Both the replication protocols and replication datasets were scanned through to ensure that the planned analyses were feasible. However, no coding or analysis of either of them had taken place before the analyses were preregistered. Further details on the search strategy can be found in the [coding protocol information file](../../SupplementaryMaterials/CodingProtocols/coding_protocol_information.Rmd) in the supplementary materials.

### Replication Datasets

The replication datasets refer to the publicly available datasets containing the data obtained from all labs that took part in a Many Labs study. For the analyses, we extracted the scores on the items of each previously identified measure that met our inclusion criteria specified below. When scores could not be clearly identified, any available codebooks, analysis scripts or study materials were used to identify the relevant scores.

To be included in the planned analyses on calculated reliabilities, the measure had to be a scale consisting of multiple items. If cleaned data was available this was chosen over raw data, to ensure that variables were coded as intended (e.g. no reverse-coded items). Pilot data were omitted from the analyses entirely. These criteria resulted in suitable item score data from `r apa_num(nrow(data_h3_avg))` replication sets spread across on average approximately `r apa_num(mean(table(data_h3_multiple$g)), digits = 0)` lab locations for the analyses of Hypotheses 2 & 3.

```{r CleaningReplicationDatasetsData, include = FALSE, eval = FALSE}
##### Data already available, code does not need to be run!
### ML 1
# 1.3
data_1.3_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[22:29])
colnames(data_1.3_clean)[1] <- "g"
# 1.10
data_1.10_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[108:115])
colnames(data_1.10_clean)[1] <- "g"
# 1.11
data_1.11_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[73:76])
colnames(data_1.11_clean)[1] <- "g"
# 1.12.1
# not found
# 1.12.3
data_1.12.3.1_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[54:59])
colnames(data_1.12.3.1_clean)[1] <- "g"
data_1.12.3.2_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[60:65])
colnames(data_1.12.3.2_clean)[1] <- "g"

### ML 2
# 2.2
data_2.2_clean <- cbind(as.factor(data_2.2[[5]]), data_2.2[6:11])
colnames(data_2.2_clean)[1] <- "g"
# 2.3
# data does not appear suitable
# 2.4.1
data_2.4.1_clean <- cbind(as.factor(data_2.4.1[[5]]), data_2.4.1[6:11])
colnames(data_2.4.1_clean)[1] <- "g"
# 2.4.2
data_2.4.2_clean <- cbind(as.factor(data_2.4.2[[5]]), data_2.4.2[6:14])
colnames(data_2.4.2_clean)[1] <- "g"
# 2.8.2
data_2.8.2_clean <- cbind(as.factor(data_2.8.2[[6]]), data_2.8.2[9:13])
colnames(data_2.8.2_clean)[1] <- "g"
# 2.10.1
data_2.10.1_clean <- cbind(as.factor(data_2.10.1[[5]]), data_2.10.1[6:11])
colnames(data_2.10.1_clean)[1] <- "g"
# 2.12.1
data_2.12.1_clean <- cbind(as.factor(data_2.12[[5]]), data_2.12[c(6,7,8,9,10,31,32,33,34,35)]) 
data_2.12.1_clean[3465:6905,2:6] <- data_2.12.1_clean[3465:6905,7:11]
data_2.12.1_clean <- data_2.12.1_clean[1:6]
colnames(data_2.12.1_clean)[1] <- "g"
# 2.12.2
data_2.12.2_clean <- cbind(as.factor(data_2.12[[5]]), data_2.12[c(11,14,15,18,19,22,24,27,28,29,36,39,40,43,44,47,49,52,53,54)]) 
data_2.12.2_clean[3465:6905,2:11] <- data_2.12.2_clean[3465:6905,12:21]
data_2.12.2_clean <- data_2.12.2_clean[1:11]
colnames(data_2.12.2_clean)[1] <- "g"
# 2.12.3
data_2.12.3_clean <- cbind(as.factor(data_2.12[[5]]), data_2.12[c(12,13,16,17,20,21,23,25,26,30,37,38,41,42,45,46,48,50,51,55)]) 
data_2.12.3_clean[3465:6905,2:11] <- data_2.12.3_clean[3465:6905,12:21]
data_2.12.3_clean <- data_2.12.3_clean[1:11]
colnames(data_2.12.3_clean)[1] <- "g"
# 2.15
data_2.15_clean <- cbind(as.factor(data_2.15[[5]]), data_2.15[8:12])
colnames(data_2.15_clean)[1] <- "g"
# 2.19.1
# difficult to extract
# 2.19.2
# difficult to extract
# 2.20
data_2.20_clean <- cbind(as.factor(data_2.20[[5]]), data_2.20[6:45]) 
data_2.20_clean[3729:7396,2:21] <- data_2.20_clean[3729:7396,22:41]
data_2.20_clean <- data_2.20_clean[1:21] 
# coding so all 1's means somebody used rule-based grouping strategy
data_2.20_clean[,c(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)] <- ifelse(data_2.20_clean[,c(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)] == 1, 1, 0)
data_2.20_clean[,c(3, 5, 7, 9, 11, 13, 15, 17, 19, 21)] <- ifelse(data_2.20_clean[,c(3, 5, 7, 9, 11, 13, 15, 17, 19, 21)] == 2, 1, 0)
colnames(data_2.20_clean)[1] <- "g"
# 2.23
data_2.23_clean <- cbind(as.factor(data_2.23[[5]]), data_2.23[c(7,8,12,13,15)])
colnames(data_2.23_clean)[1] <- "g"


### ML 3
# 3.2.1
data_3.2.1 <- cbind(as.factor(data_ml3[[1]]), data_ml3[77:86] - 1)
data_3.2.1.1_clean <- na.omit(data_3.2.1[1:6])
colnames(data_3.2.1.1_clean)[1] <- "g"
data_3.2.1.2_clean <- na.omit(data_3.2.1[c(1, 7:11)])
colnames(data_3.2.1.2_clean)[1] <- "g"
# 3.5
# data appears unusable
# 3.7.1
data_3.7.1_clean <- na.omit(cbind(as.factor(data_ml3[[1]]), data_ml3[38:42]))
colnames(data_3.7.1_clean)[1] <- "g"
# 3.7.2
data_3.7.2_clean <- na.omit(cbind(as.factor(data_ml3[[1]]), data_ml3[89:94]))
colnames(data_3.7.2_clean)[1] <- "g"
# 3.8.1
# a single measure was reported
# 3.8.2
data_3.8.2_clean <- na.omit(cbind(as.factor(data_ml3[[1]]), data_ml3[29:30])) 
colnames(data_3.8.2_clean)[1] <- "g"


### ML 5
# 5.1.1
data_5.1.1_clean <- cbind(as.factor(data_5.1[[2]]), data_5.1[13:27])
colnames(data_5.1.1_clean)[1] <- "g"
# 5.1.2
data_5.1.2_clean <- cbind(as.factor(data_5.1[[2]]), data_5.1[28:33])
colnames(data_5.1.2_clean)[1] <- "g"
# 5.4
data_5.4_clean <- cbind(as.factor(data_5.4[[1]]), data_5.4[18:41])
colnames(data_5.4_clean)[1] <- "g"
# 5.5.1 & 5.5.2
# from this dataset it appears that this data will be difficult to use.
# 5.5.2
# also difficult to use
# 5.7 
data_5.7_clean <- cbind(as.factor(data_5.7[[3]]), data_5.7[c(25, 34, 35, 36, 37, 38, 39, 40, 41, 42)])
colnames(data_5.7_clean)[1] <- "g"
# 5.9.1
data_5.9.1_clean <- na.omit(cbind(as.factor(data_5.9.1[[4]]), data_5.9.1[c(79, 83, 87, 91, 95, 98, 101)]))
colnames(data_5.9.1_clean)[1] <- "g"

### to summarize
# total number of likely usable: 15 (1.10, 1.11, 1.12.3.1, 1.12.3.2, 2.10.1, 
# 2.12.1, 2.12.2, 2.12.3, 2.15, 2.23, 3.7.1, 3.7.2, 3.8.2, 5.7, 5.9.1)
# total number of maybe usable: 6 (1.3, 2.20, 3.2.1.1, 3.2.1.2, 5.1.1, 5.4)
# total alpha coefficient comparison usable: 3 
#                                           1.11: 0.82
#                                           3.7.2: 0.67
#                                           5.9.1: 0.84


### Saving to Intermediate Data Folder
open_data(data = data_1.3_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_1.3_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_1.3_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_1.3_clean))), ".yml")) 

open_data(data = data_1.10_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_1.10_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_1.10_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_1.10_clean))), ".yml")) 

open_data(data = data_1.11_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_1.11_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_1.11_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_1.11_clean))), ".yml")) 

open_data(data = data_1.12.3.1_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_1.12.3.1_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_1.12.3.1_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_1.12.3.1_clean))), ".yml")) 

open_data(data = data_1.12.3.2_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_1.12.3.2_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_1.12.3.2_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_1.12.3.2_clean))), ".yml")) 

open_data(data = data_2.10.1_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_2.10.1_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_2.10.1_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_2.10.1_clean))), ".yml")) 

open_data(data = data_2.12.1_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_2.12.1_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_2.12.1_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_2.12.1_clean))), ".yml")) 

open_data(data = data_2.12.2_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_2.12.2_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_2.12.2_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_2.12.2_clean))), ".yml")) 

open_data(data = data_2.12.3_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_2.12.3_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_2.12.3_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_2.12.3_clean))), ".yml")) 

open_data(data = data_2.15_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_2.15_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_2.15_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_2.15_clean))), ".yml")) 

open_data(data = data_2.20_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_2.20_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_2.20_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_2.20_clean))), ".yml")) 

open_data(data = data_2.23_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_2.23_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_2.23_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_2.23_clean))), ".yml")) 

open_data(data = data_3.2.1.1_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_3.2.1.1_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_3.2.1.1_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_3.2.1.1_clean))), ".yml")) 

open_data(data = data_3.2.1.2_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_3.2.1.2_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_3.2.1.2_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_3.2.1.2_clean))), ".yml")) 

open_data(data = data_3.7.1_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_3.7.1_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_3.7.1_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_3.7.1_clean))), ".yml")) 

open_data(data = data_3.7.2_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_3.7.2_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_3.7.2_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_3.7.2_clean))), ".yml")) 

open_data(data = data_3.8.2_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_3.8.2_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_3.8.2_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_3.8.2_clean))), ".yml")) 

open_data(data = data_5.1.1_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_5.1.1_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_5.1.1_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_5.1.1_clean))), ".yml")) 

open_data(data = data_5.1.2_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_5.1.2_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_5.1.2_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_5.1.2_clean))), ".yml")) 

open_data(data = data_5.4_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_5.4_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_5.4_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_5.4_clean))), ".yml")) 

open_data(data = data_5.7_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_5.7_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_5.7_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_5.7_clean))), ".yml")) 

open_data(data = data_5.9.1_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_5.9.1_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_5.9.1_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_5.9.1_clean))), ".yml")) 

```

### Replication Protocols

The replication protocols refer to the publicly available protocols describing the background, methodology, and analysis of each set of replications replicating a single original study across multiple labs. These were retrieved from the OSF pages of the Many Labs projects (the search strategy and OSF file locations can be found in the [Data retrieval information](../../SupplementaryMaterials/data_retrieval_information.Rmd) supplementary document.

### Original Articles

The original study articles were identified using the citations for these articles in each replication protocol. All articles could be retrieved.

```{r CleaningCodedData, include = FALSE, eval = FALSE}
##### Data already available, code does not need to be run!
# Selecting the relevant rows and columns for the data
coded_data_initial_sel <- coded_data_initial_raw[3:160, 18:57]
coded_data_revised_sel <- coded_data_revised_raw[3:160, 18:38]
coded_data_vignette_raw <- coded_data_vignette_raw[3:160, 2]

# Combining the datasets
coded_data_full <- cbind(coded_data_initial_sel, 
                         cbind(coded_data_revised_sel, coded_data_vignette_raw))

# filtering out unnecessary double columns
coded_data_full <- cbind(coded_data_full[, 1:40], coded_data_full[, 45:62])



### creating the cleaned dataset using the functions in the source code 
coded_data_clean <- calculating(recoding(restructuring(fixing(
  renaming(coded_data_full)))))



### Saving to Intermediate Data Folder
# Splitting data into replication and original
coded_data_replications <- coded_data_clean[1:77,]
coded_data_original <- coded_data_clean[78:157,]

# difference in dataset row number is due to the fact that the moral foundations
# questionnaire in original 2.4 is reported using all 5 of its factors, whereas
# in replication 2.4 only the two overarching groups of binding and 
# individualizing foundations are described.
# For that reason a shortened original dataset will be used for any direct
# comparisons between original and replication coding.
coded_data_original_shortened <- coded_data_original[c(1:17, 19, 22:80),] 
coded_data_original_shortened[c(18,19),5] <- 
  c("individualizing moral foundations", "binding moral foundations")
coded_data_original_shortened[18,13] <- NA
coded_data_original_shortened[19,13] <- NA


# exporting cleaned data
open_data(data = coded_data_replications, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(coded_data_replications))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(coded_data_replications))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(coded_data_replications))), ".yml")) 

open_data(data = coded_data_original, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(coded_data_original))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(coded_data_original))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(coded_data_original))), ".yml")) 

open_data(data = coded_data_original_shortened, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(coded_data_original_shortened))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(coded_data_original_shortened))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(coded_data_original_shortened))), ".yml")) 


# clearing up memory space by removing raw files from the environment
rm(coded_data_initial_raw, coded_data_revised_raw, coded_data_vignette_raw, 
   data_2.10.1, data_2.12, data_2.15, data_2.19.1, data_2.2, data_2.20, 
   data_2.23, data_2.3, data_2.4.1, data_2.4.2, data_2.8.2, data_3.5, data_5.1,
   data_5.4, data_5.5, data_5.7, data_5.9.1, data_ml1, data_ml3)

```

## Measures {#Measures}

We developed and used a coding protocol (the [coding protocol](../../SupplementaryMaterials/CodingProtocols/Measurement_Error_Reporting_Revised_Coding_Protocol.pdf) is available in the supplementary materials) to extract information on the reported measurement evidence and reporting quality for both replication protocols and original studies.

### Measures of Reliability

Because data from the Many Labs replications were available for each lab location within each replication set, it was possible to calculate reliabilities for any item scale measure used for each lab location separately. This made it possible to assess the variation of reliabilities of the same measure across different contexts.

Cronbach's Alpha was calculated as the main reliability index used in the analyses, as it remains the most commonly used indicator of the reliability of a measure [@flake2017construct], thus allowing for comparisons with the reliabilities reported in original articles. Cronbach's Alpha was calculated using the alpha function from the psych R package [@R-psych] with default arguments. Omega was pre-registered as an additional index of reliability, because it is regarded by numerous psychometricians as a more informative alternative to Cronbach’s Alpha [@crutzen2017scale; @deng2017testing]. The results based on Omega can be found in [Appendix C](AppendixScripts/Appendix_multilevel_analyses.Rmd).

```{r ReliabilityValidityTestabilityCheck, include = FALSE, warning = FALSE}
## Checking the measures to recalculate the reliability and factor analysis for.
## The question: "Are they psychometrics scales or not?", is key here.
View(coded_data_replications)

# Likely reliability: 1.10, 1.11, 1.12.3, 2.4.1, 2.4.2, 2.8.2, 2.10.1, 2.12.1, 
# 2.12.2, 2.12.3, 2.15, 2.19.2, 2.23, 3.7.1, 3.7.2, 3.8.1, 3.8.2, 5.7, 5.9.1
# Likely factor: 1.10, 1.11, 1.12.3, 2.4.1, 2.4.2, 2.8.2, 2.10.1, 2.12.1, 2.12.2,
# 2,12,3, 2.15, 2.19.2, 2.23, 3.7.1, 3.7.2, 3.8.1, 3.8.2, 5.7, 5.9.1
# Maybe reliability: 1.3, 1.12.1, 2.2, 2.3, 2.19.1, 2.20, 3.2.1, 3.5, 5.1.1, 
# 5.1.2, 5.4, 5.5.1, 5.5.2
# Maybe factor: 1.3, 1.12.1, 2.3, 2.19.1, 2.20, 3.2.1, 3.5, 5.1.1, 5.1.2, 5.4,
# 5.5.1, 5.5.2

# 1.9 is difficult, might be a scale as dv rather than voting behavior.
```

### Measures of Measurement Reporting

We extracted the reported reliability coefficient and type of index (Cronbach's Alpha, retest, interrater, etc.) of a measure from both the replication protocols and the original articles when present. Similarly, we coded the presence of any validity evidence, such as a factor analysis, that was presented alongside the measure.

The tests for Hypotheses 4, 5, & 6 were all based on the QMPs coded for both original articles and replication protocols. We included items based on @flake2022construct, and additional items to further assess transparent measurement reporting as laid out in @flake2020measurement. In total, we coded `r sum(colnames(coded_data_replications) == "def1" | colnames(coded_data_replications) == "op_1" | colnames(coded_data_replications) == "op_2" | colnames(coded_data_replications) == "op_3" | colnames(coded_data_replications) == "op_4" | colnames(coded_data_replications) == "op_5" | colnames(coded_data_replications) == "sel_1" | colnames(coded_data_replications) == "sel_2" | colnames(coded_data_replications) == "sel_3" | colnames(coded_data_replications) == "sel_4" | colnames(coded_data_replications) == "quant_1" | colnames(coded_data_replications) == "quant_2" | colnames(coded_data_replications) == "quant_3" | colnames(coded_data_replications) == "quant_4" | colnames(coded_data_replications) == "mod_1" | colnames(coded_data_replications) == "mod_2" | colnames(coded_data_replications) == "mod_3" | colnames(coded_data_replications) == "mod_4" | colnames(coded_data_replications) == "mod_5" | colnames(coded_data_replications) == "mod_6")` different QMPs, categorized into five QMP type categories based on @flake2020measurement. The QMP categories and example items can be seen in Table \@ref(tab:QMPCodingInfoTable).

```{r QMPCodingInfoTable, warning = FALSE}
QMP_info_dataframe <- data.frame(Category = c("Definition", "", "", "Operationalisation", "", "", "", "Selection/Creation", "", "", "Quantification", "Modification", "", "", "", "", ""),
           'N Questions' = c("1", "", "", "5", "", "", "", "4", "", "", "4", "6", "", "", "", "", ""),
           'Example Question' = c("A psychological/sociological definition",
            "is given to the name of the measured",
            "variable within the paper.", 
            "The administration format (pen-and-",
            "paper/computer) and environment (in",
            "public/in a lab) are described (Note:",
            "both should be present for a true rating).", 
            "The source of the scale is provided",
            "(in case the scale was newly developed",
            "this should be clearly stated).", 
            "The number of items are described.", 
            "Any format changes are mentioned",
            "(paper-and-pencil <–> computer), if no",
            "changes were made to the format, and",
            "this was mentioned then code as No",
            "modification. If it is not clear, then code",
            "as False."))

# making the column names look less robot speak-y.
colnames(QMP_info_dataframe) <- c("Category", "N Questions", "Example Question")


# transfer the data to an APA table for printing
apa_table(
  QMP_info_dataframe, align = c("l", "r", "l")
  , caption = "Information of QMP coding variables per category."
  , note = "N Questions refers only to the questions used for calculating QMP ratios. Selection and creation share a category as the justifications and requirements in selecting a measure are similar to those for creating a new measure."
  , escape = FALSE, placement= "htp", booktabs = TRUE)

```

QMPs were all coded to be either true, false, or not applicable if not relevant for that measure (e.g. reporting results from a factor analysis for single item measures). For the analyses we calculated a ratio (both per QMP type and overall QMP) based on the number on the number of true responses divided by the number of responses coded to be applicable.

After the initial coding, we made a small revision to the preregistered coding protocol, because `r length(coded_data_revised_raw[1, c("Op1", "Op2", "Op5", "Sel1", "Sel3", "Quant1", "Quant2", "Quant3", "Mod1", "Mod2", "Mod3", "Mod4", "Mod5", "Mod6")])` of the `r length(coded_data_initial_raw[1, c("Def1", "Op1", "Op2", "Op3", "Op4", "Op5", "Sel1", "Sel2", "Sel3", "Sel4", "Quant1", "Quant2", "Quant3", "Quant4", "Mod1", "Mod2", "Mod3", "Mod4", "Mod5", "Mod6")])` QMP items were considered too stringent in some of their criteria for what constituted a QMP. For example, in the initial protocol an example item had to be present within the article or protocol itself, or else this was counted as a QMP. In the revised protocol, references to online appendices with example items were also considered sufficient for this item. The analyses, tables and figures presented in this article are all based on the revised coding protocol, the equivalent results based on QMPs obtained with our initial protocol can be found in [Appendix D](AppendixScripts/Appendix_initial_QMP_analyses.Rmd).

### Measure of Replication Success

We used the published reports of the Many Labs projects [@klein2014investigating; @ebersole2016many; @ebersole2020many; @klein2018many] to determine replication success based on the reported significance of the meta-analytic effect. An effect was considered successfully replicated if the meta-analytic effect was in the same direction as the original effect and had a p-value lower than .05.

```{r FinalPreparationData, include = FALSE, eval = FALSE}
##### Data already available, code does not need to be run!
### this code prepares the various pieces of data to be ready to use for the 
### analyses.
## confirmatory analyses
# for hypothesis 4
data_h4 <- data_prep_H4(coded_data_original_shortened, coded_data_replications)
# for hypothesis 5
data_h5 <- data_prep_H5(coded_data_replications)
# for hypothesis 6
data_h6 <- data_prep_H6(coded_data_original_shortened, coded_data_replications)

# for hypothesis 1
data_h1 <- data_prep_H1(coded_data_original_shortened, coded_data_replications)
# for hypothesis 2
data_h2 <- data_prep_H2(data_1.10_clean, data_1.11_clean, data_1.12.3.1_clean, data_1.12.3.2_clean, data_2.12.1_clean, data_2.12.2_clean, data_2.12.3_clean, data_2.15_clean, data_2.20_clean, data_2.23_clean, data_3.2.1.1_clean, data_3.2.1.2_clean, data_3.7.1_clean, data_3.7.2_clean, data_3.8.2_clean, data_5.1.1_clean, data_5.1.2_clean, data_5.7_clean, data_5.9.1_clean)
# for hypothesis 3
data_h3_multiple <- Data_prep_H3_multiple(data_h5, data_h2)
data_h3_avg <- Data_prep_H3_avg(data_h5, data_h3_multiple)


## graphs
# plot accompanying the results for Hypotheses 4, 5, & 6.
plot_456_data <- data_prep_plot_456(data_h6)
# plot accompanying the results for Hypotheses 4, & 6
plot_46_data <- data_prep_plot_46(data_h6[c(3, 6, 9, 12, 15, 18, 36, 39, 42, 45, 48, 51)])
plot_46_data_rev <- data_prep_plot_46(data_h6[c(3, 21, 24, 27, 30, 33, 36, 54, 57, 60, 63, 66)])
# plot accompanying the results for Hypotheses 2, & 3
plot_23_data_alpha <- data_prep_plot_23_alpha(data_h3_multiple)
plot_23_data_omega <- data_prep_plot_23_omega(data_h3_multiple)
plot_23_data_reported_alpha <- data_prep_plot_23_alpha_reported(coded_data_original_shortened, data_h3_avg, data_h3_multiple)


### Exporting the final datasets for the analyses
# the open data
open_data(data = data_h4, filename = paste0(paste0(
  "Data/AnalysisData/", deparse(substitute(data_h4))), ".csv"),
  codebook = paste0(paste0(
    "Data/AnalysisData/codebook_", 
    deparse(substitute(data_h4))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/AnalysisData/value_labels_", 
    deparse(substitute(data_h4))), ".yml")) 

open_data(data = data_h6, filename = paste0(paste0(
  "Data/AnalysisData/", deparse(substitute(data_h6))), ".csv"),
  codebook = paste0(paste0(
    "Data/AnalysisData/codebook_", 
    deparse(substitute(data_h6))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/AnalysisData/value_labels_", 
    deparse(substitute(data_h6))), ".yml")) 

open_data(data = data_h5, filename = paste0(paste0(
  "Data/AnalysisData/", deparse(substitute(data_h5))), ".csv"),
  codebook = paste0(paste0(
    "Data/AnalysisData/codebook_", 
    deparse(substitute(data_h5))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/AnalysisData/value_labels_", 
    deparse(substitute(data_h5))), ".yml")) 

open_data(data = data_h1, filename = paste0(paste0(
  "Data/AnalysisData/", deparse(substitute(data_h1))), ".csv"),
  codebook = paste0(paste0(
    "Data/AnalysisData/codebook_", 
    deparse(substitute(data_h1))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/AnalysisData/value_labels_", 
    deparse(substitute(data_h1))), ".yml")) 

open_data(data = plot_456_data, filename = paste0(paste0(
  "Data/AnalysisData/", deparse(substitute(plot_456_data))), ".csv"),
  codebook = paste0(paste0(
    "Data/AnalysisData/codebook_", 
    deparse(substitute(plot_456_data))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/AnalysisData/value_labels_", 
    deparse(substitute(plot_456_data))), ".yml")) 

open_data(data = plot_46_data, filename = paste0(paste0(
  "Data/AnalysisData/", deparse(substitute(Plot_46_data))), ".csv"),
  codebook = paste0(paste0(
    "Data/AnalysisData/codebook_", 
    deparse(substitute(Plot_46_data))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/AnalysisData/value_labels_", 
    deparse(substitute(Plot_46_data))), ".yml")) 

open_data(data = plot_46_data_rev, filename = paste0(paste0(
  "Data/AnalysisData/", deparse(substitute(plot_46_data_rev))), ".csv"),
  codebook = paste0(paste0(
    "Data/AnalysisData/codebook_", 
    deparse(substitute(plot_46_data_rev))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/AnalysisData/value_labels_", 
    deparse(substitute(plot_46_data_rev))), ".yml")) 

open_data(data = data_h2, filename = paste0(paste0(
  "Data/AnalysisData/", deparse(substitute(data_h2))), ".csv"),
  codebook = paste0(paste0(
    "Data/AnalysisData/codebook_", 
    deparse(substitute(data_h2))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/AnalysisData/value_labels_", 
    deparse(substitute(data_h2))), ".yml")) 

open_data(data = data_h3_multiple, filename = paste0(paste0(
  "Data/AnalysisData/", deparse(substitute(data_h3_multiple))), ".csv"),
  codebook = paste0(paste0(
    "Data/AnalysisData/codebook_", 
    deparse(substitute(data_h3_multiple))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/AnalysisData/value_labels_", 
    deparse(substitute(data_h3_multiple))), ".yml")) 

open_data(data = data_h3_avg, filename = paste0(paste0(
  "Data/AnalysisData/", deparse(substitute(data_h3_avg))), ".csv"),
  codebook = paste0(paste0(
    "Data/AnalysisData/codebook_", 
    deparse(substitute(data_h3_avg))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/AnalysisData/value_labels_", 
    deparse(substitute(data_h3_avg))), ".yml")) 

open_data(data = plot_23_data_alpha, filename = paste0(paste0(
  "Data/AnalysisData/", deparse(substitute(plot_23_data_alpha))), ".csv"),
  codebook = paste0(paste0(
    "Data/AnalysisData/codebook_", 
    deparse(substitute(plot_23_data_alpha))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/AnalysisData/value_labels_", 
    deparse(substitute(plot_23_data_alpha))), ".yml")) 

open_data(data = plot_23_data_omega, filename = paste0(paste0(
  "Data/AnalysisData/", deparse(substitute(plot_23_data_omega))), ".csv"),
  codebook = paste0(paste0(
    "Data/AnalysisData/codebook_", 
    deparse(substitute(plot_23_data_omega))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/AnalysisData/value_labels_", 
    deparse(substitute(plot_23_data_omega))), ".yml")) 

open_data(data = plot_23_data_reported_alpha, filename = paste0(paste0(
  "Data/AnalysisData/", deparse(substitute(plot_23_data_reported_alpha))), ".csv"),
  codebook = paste0(paste0(
    "Data/AnalysisData/codebook_", 
    deparse(substitute(plot_23_data_reported_alpha))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/AnalysisData/value_labels_", 
    deparse(substitute(plot_23_data_reported_alpha))), ".yml")) 
```

## Analyses

Every hypothesis test in this study was a two-sided test with an alpha of .05. No correction for multiple testing was applied, to ensure that the rate of false negatives remained low. Our hypotheses are about associations that to the authors' knowledge have been proposed in the literature, but not yet empirically tested on real data. In this exploratory context, false negatives were considered more harmful than false positives.

# Results

## Descriptives

The number of replication sets, measures, and measures per study that were extracted, as well as the proportion of successful replications can be seen in Table \@ref(tab:ReplicationRatioTable), for each of the Many Labs separately and in total.

```{r ReplicationRatioTable, warning = FALSE}
# construct the replication ratio table to be printed
replication_ratio_table <- data.frame("Many Labs Version" = c("1", "2", "3", "5", "Total"),
                                      "N Total" = c(table(coded_data_replications$many_labs_version), nrow(coded_data_replications)), 
                                      "N Replicated" = c(table(coded_data_replications$many_labs_version, coded_data_replications$hypothesis_support)[, "Yes"], table(coded_data_replications$hypothesis_support)["Yes"]))

# calculate the replication ratio from the N measures whose studies were replicated
# and the N total measures, per Many Labs study.
replication_ratio_table$"Replication Ratio" <- round(replication_ratio_table$N.Replicated / replication_ratio_table$N.Total, digits = 2)

# Changing column names to be less computer speak-y looking
colnames(replication_ratio_table) <- c("Many Labs Version", "Nr. Measures$^a$", 
                                       "Nr. Replicated", "Replication Ratio")


# adding a superscript for a later footnote
replication_ratio_table$"Nr. Replicated"[3] <- "$3^a$"

  
# print the table in apa formatting
apa_table(
  replication_ratio_table, align = c("l", "r", "r", "r")
  , caption = "Ratio of measures for which the effect was considered replicated, across many labs projects"
  , note = "Nr. measures refers to the total number of primary measures extracted, while Nr. replicated displays the number of measures for which the associated effect was replicated successfully. $^a$replication was assessed as unclear for three other measures, since their effect was only partially replicated. These have been treated as not replicated within this table and in further analyses throughout the article. "
  , escape = FALSE, placement = "htp", midrules = 4)

```

Our data had a multilevel structure. Each replication was nested within one of four Many Labs projects. Furthermore, `r sum(coded_data_replications$appearance != 1)` replications used more than one primary measure, resulting in `r nrow(coded_data_replications)` measures in both `r sum(coded_data_replications$appearance == 1)` replications protocols and `r sum(coded_data_original$appearance == 1)` original articles[^1]. Of these `r nrow(coded_data_replications)` measures `r sum(coded_data_replications$mod_check == "True")` were modified from the original to the replication. The total sample size in the replications was on average `r round(mean(coded_data_replications[coded_data_replications$appearance == 1]$N), 0)` which was approximately `r round(mean(coded_data_replications[coded_data_replications$appearance == 1]$N) / mean(coded_data_original_shortened[coded_data_original_shortened$appearance == 1 & !is.na(coded_data_original_shortened$N),]$N), 0)` times larger than the average in original studies.

[^1]: Initially the original articles contained 3 more measures than the replication protocols. This difference was due to the way that the moral foundations questionnaire was framed in the original articles compared to in the original article. In the original article it was framed as measuring five different moral foundations, while in the replication protocol the measure assessed the two overarching categories that were used to test the main effect in both the original and replication research. The measurement information reported was comparable across all five categories, and thus it was deemed that the measurement could be reduced to reflect two overarching categories facilitate easier comparison between measurement in original and replication.

## Measurement Reliability

If data from a multiple item scale could be accessed, we calculated the Cronbach’s Alpha of that scale from that data for each lab that administered the scale. As a result, it was possible to include the multiple estimates of Cronbach’s Alpha for those measures into a meta-analysis of the reliability, also commonly referred to as a Reliability Generalization (RG) Meta-Analysis [@vacha-haase1998ReliabilityGeneralizationExploring; @botella2012ManagingHeterogeneityVariance; @lopez-ibanez2024ReliabilityGeneralizationMetaanalysis]. This enabled us to quantify the degree of true variation (or heterogeneity) in reliability coefficients across lab locations.

For Cronbach's alpha, we used formulas 2 & 3 from @duhachek2004AlphaStandardError to calculate the standard error in the meta-analysis. Heterogeneity was estimated using the tau value, which indicates the standard deviation of the distribution of true Cronbach's alpha coefficients for a measure, and tested using the Cochran's Q test for each measure.

The RG meta-analysis was performed using the *rma* function from the *metafor* R package [@R-metafor]. Defaults settings were used. We implemented no correction for bias, because the Many Labs replications were not at risk of publication bias.

The average calculated Cronbach's alpha coefficient across replication sets was `r apa_num(mean(data_h3_multiple$alpha), digits = 3)` with a standard deviation of `r apa_num(sd(data_h3_multiple$alpha, na.rm = TRUE), digits = 3)`. Figure \@ref(fig:Plot23AlphaCode) displays the distributions of the calculated Cronbach’s Alpha scores from each lab for each measure, separated by successful and unsuccessful replication.

```{r ReorderPlot23Data}
# data re-ordering (factor order was not saved in export)
plot_23_data_alpha_reordered <- data_prep_plot_23_alpha(plot_23_data_alpha)
```

```{r Plot23AlphaCode, warning = FALSE,  fig.cap = "Distributions of calculated Cronbach’s alpha coefficients (> 0) calculated for the responses on a measure at each lab location, across the eighteen distinct measures for which raw data was available from which Cronbach’s alpha coefficients could be calculated. The green lines indicate the meta-analytic prediction interval lower and upper bound. The blue triangles indicate the reported alpha coefficient for that measure from the original article, when reported. The Tau column besides the figure shows the tau heterogeneity estimate based on a meta-analysis of the calculated reliabilities for each measure. Meta-analyses for which the Q-test for heterogeneity was signicant at alpha < .05 are in black, while non-significant results are in grey. The Diff column shows the difference between reported reliability and the average reliability calculated from the Many Labs data for the applicable measures, the reported reliabilities that fell outside the 95% quantile of calculated reliability scores are shown in bold."}

### use ggarange for aranging things at the side of this graph

# plot for alpha
ggplot(plot_23_data_alpha_reordered, aes(x = alpha, y = g)) +
  geom_boxplot(outlier.shape = NA) +
  geom_hline(yintercept = 6.5, color = "red", size = 1) +
  geom_point(alpha = 0.1) +
  
  # adding in the tau values
  geom_text(label = format(plot_23_data_alpha_reordered$tau, digits = 1), x = 1.15, size = 2.8, alpha = ifelse(plot_23_data_alpha_reordered$QEp < .05, 1, 0)) +
  geom_text(label = format(plot_23_data_alpha_reordered$tau, digits = 1), x = 1.15, size = 2.8, alpha = ifelse(plot_23_data_alpha_reordered$QEp >= .05, 1, 0), colour = "grey") +
  
  # adding in the alpha annotations
   annotation_custom(grob = textGrob(label = format(round(plot_23_data_reported_alpha$coefficient_difference[1], 2), nsmall = 2), hjust = 0, gp = gpar(fontsize = 8, fontface = ifelse(plot_23_data_reported_alpha$significance[1], "bold", "plain"))), ymin = 1, ymax = 1, xmin = 1.24, xmax = 1.24) +
  annotation_custom(grob = textGrob(label = format(round(plot_23_data_reported_alpha$coefficient_difference[2], 2), nsmall = 2), hjust = 0, gp = gpar(fontsize = 8, fontface = ifelse(plot_23_data_reported_alpha$significance[2], "bold", "plain"))), ymin = 2, ymax = 2, xmin = 1.24, xmax = 1.24) +
  annotation_custom(grob = textGrob(label = format(round(plot_23_data_reported_alpha$coefficient_difference[3], 2), nsmall = 2), hjust = 0, gp = gpar(fontsize = 8, fontface = ifelse(plot_23_data_reported_alpha$significance[3], "bold", "plain"))), ymin = 3, ymax = 3, xmin = 1.24, xmax = 1.24) +
  annotation_custom(grob = textGrob(label = format(round(plot_23_data_reported_alpha$coefficient_difference[4], 2), nsmall = 2), hjust = 0, gp = gpar(fontsize = 8, fontface = ifelse(plot_23_data_reported_alpha$significance[4], "bold", "plain"))), ymin = 5, ymax = 5, xmin = 1.24, xmax = 1.24) +
  annotation_custom(grob = textGrob(label = format(round(plot_23_data_reported_alpha$coefficient_difference[5], 2), nsmall = 2), hjust = 0, gp = gpar(fontsize = 8, fontface = ifelse(plot_23_data_reported_alpha$significance[5], "bold", "plain"))), ymin = 8, ymax = 8, xmin = 1.25, xmax = 1.25) +
  annotation_custom(grob = textGrob(label = format(round(plot_23_data_reported_alpha$coefficient_difference[6], 2), nsmall = 2), hjust = 0, gp = gpar(fontsize = 8, fontface = ifelse(plot_23_data_reported_alpha$significance[6], "bold", "plain"))), ymin = 9, ymax = 9, xmin = 1.24, xmax = 1.24) +
  annotation_custom(grob = textGrob(label = format(round(plot_23_data_reported_alpha$coefficient_difference[7], 2), nsmall = 2), hjust = 0, gp = gpar(fontsize = 8, fontface = ifelse(plot_23_data_reported_alpha$significance[7], "bold", "plain"))), ymin = 10, ymax = 10, xmin = 1.24, xmax = 1.24) +
  annotation_custom(grob = textGrob(label = format(round(plot_23_data_reported_alpha$coefficient_difference[8], 2), nsmall = 2), hjust = 0, gp = gpar(fontsize = 8, fontface = ifelse(plot_23_data_reported_alpha$significance[8], "bold", "plain"))), ymin = 11, ymax = 11, xmin = 1.24, xmax = 1.24) +
  annotation_custom(grob = textGrob(label = format(round(plot_23_data_reported_alpha$coefficient_difference[9], 2), nsmall = 2), hjust = 0, gp = gpar(fontsize = 8, fontface = ifelse(plot_23_data_reported_alpha$significance[9], "bold", "plain"))), ymin = 12, ymax = 12, xmin = 1.25, xmax = 1.25) +
  
  theme_minimal() +
  theme(legend.position = "none", plot.margin = unit(c(1, 6.5, 1, 1), "lines")) +
  
  # adding the necessary indicative texts
  annotation_custom(grob = textGrob(label = "Not Replicated", hjust = 0, gp = gpar(fontsize = 10)), ymin = 7.25, ymax = 7.25, xmin = 0.01, xmax = 0.01) +
  annotation_custom(grob = textGrob(label = "Replicated", hjust = 0, gp = gpar(fontsize = 10)), ymin = 6, ymax = 6, xmin = 0.01, xmax = 0.01) +
  annotation_custom(grob = textGrob(label = "Tau", hjust = 0, gp = gpar(fontsize = 12)), ymin = 20.2, ymax = 20.2, xmin = 1.08, xmax = 1.08) +
  annotation_custom(grob = textGrob(label = "Diff", hjust = 0, gp = gpar(fontsize = 12)), ymin = 20.2, ymax = 20.2, xmin = 1.24, xmax = 1.24) +
  
  coord_cartesian(xlim = c(0, 1), clip = "off") +
  
  # adding the blue triangles for reported reliability and green prediction intervals
  geom_point(data = plot_23_data_reported_alpha, mapping = aes(x = coefficient_reported, y = article_order), color = "blue", shape = 17, size = 3) +
  geom_point(mapping = aes(x = pi.lb), color = "green", shape = 124, size = 2.5) + 
  geom_point(mapping = aes(x = pi.ub), color = "green", shape = 124, size = 2.5) + 
  
  ylab("") +
  xlab("Cronbach's alpha") 


```

The distribution of reliabilities varied across measures. Most of the measures near the bottom showed average reliability scores of at least .80, corresponding to adequate reliability for general research purposes [@nunnally1994assessment], with minimal variation between labs. However, other measures showed not only considerably lower average reliability scores, but also greater variation.

Finally, the blue triangles indicate the Cronbach’s Alpha reported in the original articles. The reliabilities were generally only reported for those measures with a large average calculated reliability.

### H1

```{r ReportedReliabiltiesRetrieval}
N_reported_reliabilities_original <- sum(coded_data_original_shortened$reliability_type != "Not Reported" & coded_data_original_shortened$reliability_type != "")

N_reported_reliabilities_replications <- apa_num(sum(coded_data_replications$reliability_type != "Not Reported" & coded_data_replications$reliability_type != ""), numerals = FALSE)

N_reported_reliabilities_replications_number_style <- apa_num(sum(coded_data_replications$reliability_type != "Not Reported" & coded_data_replications$reliability_type != ""))
```

The number of reported reliabilities were `r N_reported_reliabilities_original` in original articles and `r N_reported_reliabilities_replications` in replications, which was too low to ensure that the pre-registered Mann-Whitney U test can function as an informative test on the difference in the reported value of Cronbach's Alpha between original articles and replications. Instead, the difference between the reported reliability in the original article and the calculated average reliability in replications is displayed in the Diff column in Figure \@ref(fig:Plot23AlphaCode). Contrary to Hypothesis 1, this column shows that the reported reliability in original articles was generally lower than the average reliability in the replication sample. Still, more than half of the reported reliabilities did fall within the 95-percentile range around the replication average.

### H2

To get an indication of the true variability in the reliability scores, we used the RG meta-analyses to test for heterogeneity in the reliabilities in the replications. This analysis deviated from the multilevel analysis we preregistered, as the preregistered analysis was later deemed not suitable for testing Hypothesis 2 (results from the preregistered analysis are shown in [Appendix A](AppendixScripts/Appendix_pre-reg_analyses.Rmd)).

Figure \@ref(fig:Plot23AlphaCode) displays the tau estimates, which indicates the differences in true reliability scores between labs for a given measure. For `r sum(data_h3_avg$QEp > .05)` measures the tau estimate was not significantly different from 0, while for `r apa_num(sum(data_h3_avg$QEp < .05), numerals = FALSE)` measures, the true variation in reliability across labs was significant. The true reliability was quite variable for some measures. For instance, the estimate of the standard deviation of the true reliability of the top measure in the graph (a measure of conscientiousness from @gosling2003very used in the replication of @defruyt2000cloninger) equaled `r apa_num(plot_23_data_alpha_reordered[plot_23_data_alpha_reordered$g == "Albarracín et al. (2008), exp 5 math", "tau"][1], digits = 3, gt1 = FALSE)` points of Cronbach’s Alpha. 

Thus for around a quarter of measures, the reliability was heterogeneous. Although for the remaining measures, we found no significant heterogeneity in reliabilities, we note that power to detect small heterogeneity is often low (@ioannidis2007uncertainty; @olsson2020heterogeneity).

### H3

We used a logistic regression model to test whether replication success as reflected in a significant mean effect in the meta-analysis conducted across the replicating labs could be predicted by the average calculated reliabilities of the measures.

```{r Hypothesis3MainTest, include = FALSE}
## The main model to test hypothesis 3, through a logistic regression model.
H3_test_result_main_alpha <- apa_print(glm(formula = replication ~ 1 + alpha, 
                                      family = binomial(), data = data_h3_avg))

# coefficient is positive but not significant.

# adding OR to the result
H3_main_full_results_alpha_with_OR <- OR_to_apa_full_supplier(H3_test_result_main_alpha$full_result$alpha, negative_b = FALSE)
  
# well this OR is quite a ridiculous number now..., but then again a step from 0 alpha to 1, is quite a jump too...
H3_test_result_main_alpha$full_result$alpha

```

Cronbach's alpha did not significantly predict replication success in the main logistic regression model (`r H3_main_full_results_alpha_with_OR`). Results based on the Omega coefficient lead to a similar conclusion (see [Appendix C](AppendixScripts/Appendix_omega_analyses.Rmd). However, it should be noted that the reliability coefficient could be calculated for only `r nrow(data_h3_avg)` measures. As a result, the estimates of the relation between reliability and replication success obtained using these models each come with large uncertainty.

## Measurement Reporting

### Reliability & Validity Reporting

Figure \@ref(fig:ReliabilityReportingFlowDiagram) depicts the flow of measures in relation to reliability reporting. First, it shows that almost half of the measures in both replication (n = `r apa_num(sum(coded_data_replications$N_items == "1 item measure"))`) and original research (n = `r apa_num(sum(coded_data_original_shortened$N_items == "1 item measure"))`) were single-item measures. When looking at the multiple item measures, the graph illustrates that `r N_reported_reliabilities_replications` measures in the replication protocols and `r N_reported_reliabilities_original` in the original articles reported a reliability coefficient. The most commonly reported reliability coefficient was Cronbach's Alpha, which accounted for `r apa_num(sum(coded_data_replications$reliability_type == "alpha"), numerals = FALSE)` reported reliabilities in the replication protocols, and `r apa_num(sum(coded_data_original_shortened$reliability_type == "alpha"))` in the original articles.

```{r ReliabilityReportingFlowDiagram, fig.cap = "Reliability reporting flow diagram. Figure shows the number of measures as reported in both the replication protocols and original article, which meet the criterion in the box within the diagram and those criteria before it.", out.height = "60%"}
knitr::include_graphics(path = "../../SupplementaryMaterials/reliability_reporting_flow_diagram.png")
```

```{r Hypothesis1RecycledModel, include = FALSE}
# first some descriptives of reliability reporting
# Measurement type per original and replication:
reliability_reporting_descriptive_table_1 <- table(data_h1$rep_org, data_h1$N_items)

# amount of reliability reported per original and replication 
# (given that measure is a multiple item measure):
reliability_reporting_descriptive_table_2 <- table(data_h1$rep_org[data_h1$N_items == "multiple item measure"], data_h1$reliability_type[data_h1$N_items == "multiple item measure"])

# recycled H1 test
# A chi square test for difference in N reliability reported for multiple 
# item measures.
reliability_reporting_test_result <- chisq.test(data_h1$rep_org[data_h1$N_items == "multiple item measure"], data_h1$reliability_reported[data_h1$N_items == "multiple item measure"])

```

Reliability was more commonly reported in original (n = `r `N_reported_reliabilities_original`) as compared to replication research (n = `r N_reported_reliabilities_replications_number_style`), $\chi^{2}$(`r apa_num(reliability_reporting_test_result$parameter)`) = `r apa_num(reliability_reporting_test_result$statistic)`, *p* = `r apa_p(reliability_reporting_test_result$p.value)`. Validity evidence was similarly reported infrequently. `r apa_num(sum(coded_data_original_shortened$sel_psychometric_evidence_REV != "None" & coded_data_original_shortened$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)"), numerals = FALSE)` original articles reported validity evidence from a factor analysis, while only `r apa_num(sum(coded_data_replications$sel_psychometric_evidence_REV != "None" & coded_data_replications$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)"), numerals = FALSE)` replications did.

### H4

```{r numberNAsForQMPRatio}
ratio_of_NAs <- sum(is.na(data_h4$def1) + is.na(data_h4$op_1_REV) + is.na(data_h4$op_2_REV) + is.na(data_h4$op_3) + is.na(data_h4$op_4) + is.na(data_h4$op_5_REV) + is.na(data_h4$sel_1_REV) + is.na(data_h4$sel_1) + is.na(data_h4$sel_2) + is.na(data_h4$sel_3_REV) + is.na(data_h4$sel_4) + is.na(data_h4$quant_1_REV) + is.na(data_h4$quant_2_REV) + is.na(data_h4$quant_3_REV) + is.na(data_h4$quant_4) + is.na(data_h4$mod_1_REV) + is.na(data_h4$mod_2_REV) + is.na(data_h4$mod_3_REV) + is.na(data_h4$mod_4_REV) + is.na(data_h4$mod_5_REV) + is.na(data_h4$mod_6_REV)) / (20 * 154)

QMP_NA_percentage <- paste0(apa_num(ratio_of_NAs * 100), "%")

```

```{r MeanQMPCode, warning = FALSE, include = FALSE}
# calculating mean total QMP ratios
Mean_Rat_Rep <- mean(plot_456_data$QMP_ratio_REV[
  plot_456_data$QMP_type == "Total" & plot_456_data$RepOrg == "Replication"])
Mean_Rat_Org <- mean(plot_456_data$QMP_ratio_REV[
  plot_456_data$QMP_type == "Total" & plot_456_data$RepOrg == "Original"])
```

The top part of Figure \@ref(fig:Plot456Code) displays the distributions of the total QMP ratio for both original articles and replication protocols, showing that the average QMP ratio in replication protocols was smaller (`r apa_num(Mean_Rat_Rep)`) than in original articles (`r apa_num(Mean_Rat_Org)`). The bottom of Figure \@ref(fig:Plot456Code) shows that original articles and replication protocols had different distributions of QMPs per category. The number of measures for which modification items were applicable also differed. In total `r QMP_NA_percentage` of all responses across all QMP items were non-applicable.

```{r Plot456Code, warning = FALSE, fig.cap = "QMP ratio counts for each QMP Type and QMP total ratio distribution grouped by whether the QMP ratio was obtained from an original article or a replication protocol. The top graph shows the distributions of total QMP ratios for both replication protocols and original articles, with the line indicating the mean QMP ratio. The specific observed values are indicated along the bottom row with dots. The bottom row shows for each QMP type the proportions of each QMP ratio obtained, darker colors represent proportionally more QMPs, grey means modification did not occur for that measure."}

# re-ordering the QMP-types, because it didn't save
plot_456_data$QMP_type <- factor(plot_456_data$QMP_type, levels = c("Definition", "Selection", "Operationalization", "Quantification", "Modification", "Total"))

# shortening the replication and original labels for nicer plot 1
levels(plot_456_data$RepOrg) <- c("Org.", "Rep.")


### ratio (REV) Original and Rep
# QMP type specific QMP ratios
Plot_456_1 <- ggplot(plot_456_data[plot_456_data$QMP_type != "Total",], 
                      aes(x = RepOrg, fill = as.factor(QMP_ratio_1_REV))) + 
  geom_bar(position = "fill") +
  theme_minimal() +
  scale_fill_manual(values = c("#ff766c", "#ef6f65", "#ce5f57", "#be5851", "#aa4f48", "#994741", "#803b36", "#672f2b", "#562824", "#411e1b", "#341816", "#030101","#4366ff", "#4265fc", "#3551cb", "#324dc1", "#324cbe", "#2d44aa", "#2c44a9", "#283d99", "#223380", "#162255", "#10193e", "#010206", "#000102")) + # gradient for the colors was found by changing the brightness of the colour in a hsl color editor (in GIMP) to be equal to the inverse of the QMP ratio value.
  facet_grid(~ QMP_type) + 
  theme(legend.position = "none", strip.text = element_text(size = 7)) +
  ylab("Relative Prevalence of\nQMP Ratio in Sample") +
  xlab("QMP Type") +
  ggtitle("QMP Ratio's Per QMP Type Grouped by Original and Replication")

# changing the replication and original labels back for plot 2
levels(plot_456_data$RepOrg) <- c("Original", "Replication")



# total QMP ratios
Plot_456_2 <- ggplot(plot_456_data[plot_456_data$QMP_type == "Total",], 
       aes(x = QMP_ratio_REV, fill = RepOrg, colour = RepOrg)) +
  geom_density(alpha = 0.2) +
  geom_point(y = 0, alpha = 0.2, size = 3) +
  geom_segment(aes(x = Mean_Rat_Rep, xend = Mean_Rat_Rep, y = 0, yend = 2.66)
               , color = "blue4") +
  geom_segment(aes(x = Mean_Rat_Org, xend = Mean_Rat_Org, y = 0, yend = 2.183)
               , color = "brown") +
  coord_cartesian(xlim = c(0, 1)) +
  theme_ridges() + 
  theme(axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        axis.text.x = element_text(size = 8)) +
  scale_fill_manual(values=c("brown", "blue4")) +
  scale_color_manual(values=c("brown2", "blue")) +
  scale_x_continuous(breaks = c(0, Mean_Rat_Rep, 0.25, Mean_Rat_Org, 
                                0.5, 0.75, 1), 
                     labels = c("0", "0.18", "0.25", "0.31", "0.5", 
                                "0.75", "1")) + 
  ggtitle("Distribution of Total QMP Ratio's by Original and Replication") +
  guides(color = "none") +
  labs(fill = "Coded from") +
  ylab("") +
  xlab("") 


## the following two plots are constructed solely for the purpose of extracting
# a legend from them.
for_legend_plot_456_org <- ggplot(plot_456_data[plot_456_data$QMP_type != "Total",], 
                                  aes(x = RepOrg, y = abs(QMP_ratio_REV - 1), fill = abs(QMP_ratio_REV - 1))) + 
  geom_bar(stat = "identity") +
  theme_minimal() +
  scale_fill_gradient(name = "Original", low = "#030101", high = "#ff766c", limits = c(0, 1), 
                      breaks = c(0.001, 0.5, 0.999), labels = c(1, 0.5, 0)) +
  theme(legend.title=element_text(size = 9)) + 
  facet_grid(~ QMP_type) 

for_legend_plot_456_rep <- ggplot(plot_456_data[plot_456_data$QMP_type != "Total",], 
                                  aes(x = RepOrg, y = abs(QMP_ratio_REV - 1), fill = abs(QMP_ratio_REV - 1))) + 
  geom_bar(stat = "identity") +
  theme_minimal() +
  scale_fill_gradient(name = "Replication", low = "#000102", high = "#4366ff", limits = c(0, 1), 
                      breaks = c(0.001, 0.5, 0.999), labels = c(1, 0.5, 0)) +
  theme(legend.title=element_text(size = 9)) + 
  facet_grid(~ QMP_type) 

# extracting the legends from the plot
legend_456_org <- get_legend(for_legend_plot_456_org) 
legend_456_rep <- get_legend(for_legend_plot_456_rep) 


# combining both plots and the legends into one figure
grid.arrange(Plot_456_2, 
             Plot_456_1, 
             legend_456_org,
             legend_456_rep,
             layout_matrix = rbind(c(1, 1, 1),
                                    c(2, 3, 4)),
              widths = c(8, 1, 1))

```

We used a beta-regression model to test the difference in QMP ratio between original and replication was significant using the *betareg* R package [@R-betareg_a; @R-betareg_b]. Beta regression models are similar to other generalized linear regression models, but they are better suited to model dependent variables with values in the interval of $(0, 1)$, including ratios. Furthermore, they are robust for heteroskedastic and asymmetrically distributed dependent variables [@cribari2010beta].

```{r Hypothesis4Model, include = FALSE}
## Hypothesis 4 beta regression model
# running the models with revised QMP ratio's
H4_test_results_REV <- betareg(QMP_REV_ratio ~ rep_org, data = data_h4)
```

Using a beta-regression model, the total QMP ratio was regressed on a dummy variable indicating whether the coded report was an original article or a replication protocol. The results supported Hypothesis 4 and indicated that this difference was significant (`r betareg_output_to_apa_full(H4_test_results_REV)`).

### H5

```{r Hypothesis5Model, include = FALSE}
## Hypothesis 5 logistic regression model
H5_test_results_REV <- apa_print(glm(hypothesis_support ~ QMP_REV_ratio, data = data_h5, family = binomial))

H5_test_results_REV_with_OR <- OR_to_apa_full_supplier(H5_test_results_REV$full_result$QMP_REV_ratio, negative_b = TRUE)
H5_test_results_REV_with_OR
  
```

We used logistic regression to test the association between the QMP ratio in replication protocols and replication success. In line with Hypothesis 5, results showed that successful replications were associated with fewer QMPs compared to unsuccessful replications (`r H5_test_results_REV_with_OR`).

### H6

```{r Hypothesis6Model, include = FALSE}
## Hypothesis 6 beta regression model
H6_test_results_REV <- betareg(Rep_QMP_REV_ratio ~ QMP_REV_ratio, data = data_h6)

```

Using a beta regression model, we found total QMP ratio in the original article to be significantly related to the total QMP ratio in the subsequent replication (`r betareg_output_to_apa_full(H6_test_results_REV)`). This result provides evidence in favor of Hypothesis 6. 


# Discussion

In this article, we analyzed the data, protocols, and related original articles from four Many Labs replication projects to assess reliability and measurement (reporting) practices. We additionally looked at how these features might be related to replication success, as a proxy of the credibility of findings. Overall, even though the average reliability was relatively high, we found that not all measures were sufficiently reliable, nor where measures always equally reliable in each setting in which they were used. Although, we did not observe reliability to significantly predict replication success, we did observe a negative relation between replication success and presence of QMPs in replication research. Finally, we found QMPs in the replication to be positively associated with QMPs in the original studies

## Reliability

### Reliability varied within and between measures

While most measures showed sufficient average reliability for most purposes in psychological research (around .80), about a quarter of the measures had a reliability generally considered insufficient (around .60 or less) [@nunnally1994assessment]. Furthermore, the reliability in original research was generally lower than in the replication samples, contrary to our hypothesis that reported reliability estimates would not differ significantly between replications and original articles. These differences may be due to features that were specific to the Many Labs projects. Large teams of researchers with diverse expertise were involved in the Many labs projects. It is possible that the pooling of expertise and use of structured procedures led to more reliable measurement compared to the original research.

Our results additionally show that the reliability of a measure differs across samples, in some cases even showing signs of true variation, or heterogeneity, in reliability. Reliability heterogeneity was particularly common for measures with a lower average reliability. However, the number of investigated measures was small, and some measures were only used in a small number of independent samples. This is particularly relevant, since the Q-test to test heterogeneity is sensitive to the number of studies included in each test [@li2015DilemmaHeterogeneityTests]. Furthermore, most measures did not show evidence of significant reliability heterogeneity, a similar lack of widespread heterogeneity was also observed for effect sizes in replications [@klein2018many; @olsson2020heterogeneity]. Regardless, our results do demonstrate an often-overlooked aspect of reliability: reliability is a sample-specific, not a measurement-specific feature.

### No observed relation between reliability and replicability
@stanley2014expectations illustrated that measurement error, when not attenuated for, can impact replication assessments. However, we observed no relation between reliability and replicability, even though our chosen replication index did not attenuate for measurement error. This result is surprising not only because it runs counter to the findings of @stanley2014expectations and Hypothesis 3. It also runs counter to the fact that according to statistical theory, the increased noise in the data as the result of lower reliability decreases the effect size estimates based on those data.

However, the relationship between reliability and replicability is complex [@willson1980research; @lebreton2014corrections; @oswald2015Imperfect; @zhang2024Metaanalysis]. For example, reliability and replicability are only related when the true effect is not null. In our sample more than half of the effects were not replicated, indicating that many of these effects may be null effects. For these effects no relation is expected. Furthermore, while lower reliability decreases effect sizes it can also reduce the observed true variance in the effects [@olsson-collentine2023UnreliableHeterogeneityHow]. As a result, effect estimates from various studies will appear more similar than they truly are, seemingly replicating the original. These and other relations are further complicated by the fact that reliability is not a fixed aspect of a scale, but a feature of that scale within a sample [e.g., @vacha1999practices; @cho2015cronbach].

The complexity of the relationship combined with the fact that we could only calculate reliability for a small number of measures meant we had low statistical power to detect the overall relation between reliability and replicability, which provides a likely explanation for our non-significant finding. 

The small number of relevant measures and reported reliabilities also indicates an important issue: we observed a lack of reliability evidence in both original and replication psychological research.

## Measurement Reporting

### Measurement Reporting is often incomplete

The reliability of measures was rarely reported in both original and replication research, which is in line with earlier investigations in the literature [@shaw2020measurement; @flake2017construct; @flake2022construct]. Reported reliabilities were so few, that the preregistered test for Hypothesis 1 was no longer informative. We found that replication research reported reliability coefficients less often than original research, which is in line with @flake2022construct. This pattern continued for validity evidence reporting. Of the `r nrow(coded_data_replications)` measures, validity evidence in the form of factor analysis or similar analyses was reported for `r apa_num(sum(coded_data_original_shortened$sel_psychometric_evidence_REV != "None" & coded_data_original_shortened$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)"), numerals = FALSE)` measures in original articles, and `r apa_num(sum(coded_data_replications$sel_psychometric_evidence_REV != "None" & coded_data_replications$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)"), numerals = FALSE)` measures in replication protocols.

The underreporting of reliability and validity evidence has long been discussed [@willson1980research;@vacha1999practices; @green2011recent]. Our study illustrates three potential reasons for why this problem has persisted. Firstly, the studies in our sample commonly used single-item measures, an observation that was also made by @shaw2020measurement. Calculating reliability for single-item measures is not as straightforward and less common as for multiple item measures [@wanous1996estimating].

Secondly, our results indicate that there may be bias in reporting reliability coefficients. Reliabilities were reported in original research for those measures that obtained higher calculated reliabilities in the replication samples. If we take the calculated reliabilities in the replication sample as an accurate representation of the distribution of the true reliability for that measure, then this would imply that reliabilities are more often reported when the measure is truly reliable, and less when it is unreliable; a common issue of selective reporting. Researchers may be reluctant to report reliability for unreliable measures, thus lowering the prevalence of reported reliabilities. In turn, researchers may become overly optimistic about the reliability of measures in their field, similarly to how bias in publication causes issues for establishing estimates of true effect sizes in meta-analyses [@sutton2000empirical].

Thirdly, the replication protocols were written before data were collected, which means no measurement based on these protocols had taken place yet. As a result, there was no data available yet from which the replicating researchers could have calculated reliability and validity. This would explain why there was little reliability and validity evidence reported in replication protocols. It may still represent a problem however, because original articles similarly rarely reported reliability and validity evidence. This implies that the replications commonly used measures that were not yet validated.

On the other hand, we found that more general issues in the transparency of measurement reporting practices, as assessed by QMPs, were less common in replication protocols compared to original articles. This is in line with Hypothesis 4, but runs counter to the findings of @flake2022construct, who found less construct validity evidence in the replications from the RPP [@osc2015estimating] than in original articles. We believe this discrepancy was the result of our sample being the Many Labs projects. These projects made use of structured replication protocols to document the way measurement was going to be conducted. For example, several Many Labs replication protocols contained a section specifically listing the deviations from the original measurement.

### Incomplete measurement reporting hinders replicability

Existing literature has already warned about the potential detrimental effects of QMPs on replications [@shaw2020measurement; @flake2022construct]. It is challenging for replication researchers to mimic the measurement of an original article, when the original article does not document the measurement in sufficient detail. Consequently, the measurement in the replication may assess constructs substantially differently, weakening the relation between the test in original and replication studies [@flake2022construct].

In line with Hypotheses 5 & 6, we found an indication of such a spillover effect in our sample. The total QMPs of an original study and the total QMPs in the protocol of the replication for the same study were positively associated. Furthermore, we found that QMPs may have negatively impacted replicability. An increase in total QMP ratio was associated with a decrease in replicability. These associations together indicate that poor measurement reporting in an original study could be a risk factor for subsequent replication attempts.

However, it is worth noting that these associations were only found for QMPs coded with the revised coding protocol. Neither hypothesis was supported by the data on QMPs obtained with the initial coding protocol (see [Appendix D](AppendixScripts/Appendix_initial_QMP_analyses.Rmd) for results based on the initial protocol).

## Limitations & Future Research

Our study had several limitations. We encountered numerous challenges in operationalizing QMPs. Whereas previous research has focused on descriptives of specific QMPs detrimental to construct validity [@flake2022construct; @shaw2020measurement], we operationalized all coded QMPs together as a variable to assess measurement practices and evaluate their impact on replicability. However, this also means that it remains uncertain if our operationalization accurately captured the concept. We already deemed it necessary to revise our coding protocol to be more lenient regarding more context dependent QMPs compared to the initial protocol. This revision was not trivial, as it altered the interpretation of our results. Regardless, it reflects the challenges we encountered in operationalizing QMP in this novel way. Future research is needed to identify which way is the most rigorous and fair.

One challenge in operationalizing was the difficulty in determining when a practice is questionable, and the impact of the context on this decision. For example, in our sample the constructs that were measured were less commonly defined in replication protocols than in original articles. However, one may argue that it is not the responsibility of the replication to define the construct, as it is meant to be equivalent to the construct assessed in the original article. In that case, not defining the construct may not be a questionable practice at all.

Another challenge in constructing a QMP variable was the way to treat practices that were not applicable to a particular measurement. We chose to make use of ratios to cancel out the effects of non-applicable items as much as possible. However, this has the consequence that both a measure for which all relevant measurement-related information was reported, and a measure for which no item was applicable would have a QMP ratio of zero. Yet, we would not consider the latter case to indicate the same level of completeness as the first. A little more than one third of all responses were not applicable. As a result, the relation between QMP ratios and measurement reporting completeness may have been obscured to some extent.

The relation is further complicated, as it is not clear how great a ratio of QMPs represents clear violations of good research practices. One might argue that any QMP is a sign of questionable research and thus that any ratio greater than zero is cause for alarm. However, this may be too harsh. As mentioned before, not all violations may have detrimental consequences or be relevant in all contexts. Future research will be needed to determine the best way(s) to operationalize QMP ratios. Researchers can take inspiration from this study and earlier studies by @shaw2020measurement and @flake2022construct, as well as the criteria on measurement reporting as set out in the @aera2014standards.

A second limitation is that our tests on reliability were underpowered. This was the result of the small number of reported reliability scores, and multiple item scales with available data. However, this limitation also illustrates two key findings. One finding is that reliability was rarely reported for measures in both original articles and replication protocols.

The other finding is that around half of the measures in our sample were single item-measures. The use of single item measures comes with psychometric risks [@diamantopoulos2012guidelines; @nunnally1978overview], and is more limited with respect to the type of reliability and validity evidence that can be determined [@sarstedt2016selecting; @shaw2020measurement]. Because of these issues, it could even be argued that the use of single-item measures is a QMP in certain contexts.

As a third limitation, it is important to consider whether replication protocols and research articles can be fairly compared in terms of QMPs. The space for describing methodology within a replication protocol is generally shorter than an article. Furthermore, for direct replications the methodology may be seen as largely equivalent to the original article, thus measurement descriptions in the replication protocol may be considered superfluous. There are three main reasons why we believe protocols and articles remain comparable. Firstly, the majority of the measurements in our replication sample were modified in some way, it is therefore important that replication protocols still report on their measurement as it may deviate from the original. Secondly, articles are often also restricted in the amount of space they have available to devote to measurement [@gardiner2019editorial]. Thirdly, supplementary materials were also included as a valid information source. Still, future research may wish to analyze supplementary materials in greater detail, to investigate to what extent these materials cover measurement details in both replication protocols and original articles.

A fourth limitation is that we of only used one definition of replication success. Replication success can be estimated and framed in multiple ways [for an example in replication research see, @osc2015estimating], including methods that do not make use of significance testing, or that attenuate for (measurement) error. The relationship between reliability and other indices of replication success may differ.

Finally, we note that we used an observational design which means that we cannot attach definitive causal directions to the relations observed in this study.  To directly assess if there is a negative impact of reliability and QMPs on replicability, we would need experimental research. Future research could for example randomly assign original studies in a large scale replication project to either needing to report all relevant measurement information as noted down in Table 1 in @flake2020measurement, while other studies receive no such reporting requirements. Similarly, the requirement to use validated measures based on the criteria set out in section 1, 2, and 3 in the @aera2014standards standards, or lack thereof could be randomly assigned conditions.  Although such experimental designs would be the most direct way to assess causality, it would take tremendous coordination and funding to integrate original studies into a large scale replication project from the start. While such efforts are not unprecedented [@protzko2020high], alternative methods also exist to assess causal relations on observational data, at least to some extent. @rohrer2018thinking has suggested researchers make use of Directed Acyclic Graphs to investigate causality in (non-)experimental settings by systematically accounting for confounding influences on an effect. 

## Recommendations

Taking all the findings together, the following statements will be relevant to consider for future research. Firstly, reliable and valid measures are a prerequisite for credible findings. The observation that many measures are not reported with such evidence is worrying, because it obscures the credibility of findings, and hampers scientific progress. Therefore, we join earlier recommendations [@hogan2004empirical; @barry2014validity; @flake2020measurement; @flake2022construct] that establishing validity evidence for multiple item scales should become a community standard for credible research. Furthermore, reliability coefficient estimates for multiple item scales should by default always be reported, since these estimates can easily be obtained with most statistical software, and provide important information on a measure’s performance. Only researchers that can provide reasoning to the contrary for their research should be exempt from this default.

Secondly, researchers should report their procedure and measurement in detail, as specified in for example Table 1 in @flake2020measurement, or section 3.6 in @apa2020publication on measures and measurement. This is important so that it can be mimicked in replications if needed. The remaining information that could not fit in the article can be made available on a public repository, such as the Open Science Framework (OSF; @soderberg2018using) or Zenodo [@zenodo2024open].

Furthermore, we argue that researchers seeking to replicate a study should first evaluate the measurement of that study. It is crucial for an informative replication that the measurement is reliable or valid. Otherwise it may be futile to attempt a direct replication [see also @nuijten2022assessing]. If the measurement is insufficiently reliable or valid, we suggest that researchers instead use their resources to conduct a replication of a study with reliable, valid and well-documented measurement. When replicating another study is not an option, we advise the replicating researcher to first attempt a conceptual replication using reliable and valid measurement, or use both the original and their own preferred measure [as in @stoevenbelt2018rrr]. Afterwards, a direct replication can be performed based on the conceptual replication to further assess the effect’s credibility.

# Conclusion

Through our investigations into the reliability, and reporting of measurement in the Many Labs replications and associated original studies, we found that reliability and validity evidence was reported infrequently. Furthermore, QMPs that obscured important information needed to evaluate and reconstruct the measurement were common, especially in original research. QMPs were in turn related to the replicability of a finding. Results were less clear for reliability differences between original and replication, due to lack of power. However, around a quarter of measures showed signs of significant variation in reliability across labs. Combined these findings illustrate the need for more concrete standards in measurement reporting, and that researchers should consider the validity and reporting completeness of a study's measurement before attempting to replicate it.


## ORCID iDs
Cas Goos https://orcid.org/0009-0005-3792-4148
Marjan Bakker https://orcid.org/0000-0001-9024-337X
Jelte M. Wicherts https://orcid.org/0000-0003-2415-2933
Michèle B. Nuijten https://orcid.org/0000-0002-1468-8585

## Funding
The preparation of this article was supported by the veni grant ??? awarded to Michèle Nuijten, and the vici grant ???  awarded to Jelte M. Wicherts from the Netherlands Organization for Scientific Research (NWO).

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::

\newpage

# (APPENDIX) Appendix {.unnumbered}

```{r child = "AppendixScripts/Appendix_pre-reg_analyses.Rmd"}
```

```{r child = "AppendixScripts/Appendix_multilevel_analyses.Rmd"}
```

```{r child = "AppendixScripts/Appendix_omega_analyses.Rmd"}
```

```{r child = "AppendixScripts/Appendix_initial_QMP_analyses.Rmd"}
```
