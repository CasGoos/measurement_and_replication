---
title             : "The Impact of Measurement Practices and Measurement Error on Construct Validity and Replication Outcomes in Psychological Science"
shorttitle        : "Measurement Quality and Replicability"

author: 
  - name          : "Goos, C."
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Professor Cobbenhagenlaan 125, 5037 DB, Tilburg, The Netherlands"
    email         : "casgoos99@gmail.com"
    role: # Contributorship roles (e.g., CRediT, https://credit.niso.org/)
      - "Conceptualization"
      - "Data curation"
      - "Formal Analysis"
      - "Investigation"
      - "Methodology"
      - "Visualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
  - name          : "Nuijten, M.B."
    affiliation   : "1"
    role:
      - "Project Administration"
      - "Writing - Review & Editing"
      - "Supervision"

affiliation:
  - id            : "1"
    institution   : "Tilburg University"

authornote: |
  1 Department of Methodology and Statistics, Tilburg School of Social and Behavioral Sciences, Tilburg University, Tilburg, NL. 

  Enter author note here. Caspar?

abstract: |
  Recently, the influence of measurement related issues on replicability has received more investigations. 
  In particular, studies have indicated that low reliability, and poor measurement reporting can negatively influence the replicability of an effect. 
  Building on this new line of research, this thesis assessed these influences in large scale replication projects. 
  Protocols from the Many Labs replication projects, and the original articles that the replications were based on were used to assess the  reporting of the measures, while publicly available data from the Many Labs replications was used to assess the reliability of the measures. Reliability did not relate to replicability, and was generally consistent between labs for each measure in the Many Labs projects. 
  The quality of measurement reporting in replication protocols did however relate to replicability.
  Additionally, tentative evidence was found for a relation between quality of measurement reporting in the protocol of a replication and the quality of measurement reporting in its related original study.
  Recommendations are given for researchers and replicators in order to improve measurement practices in replication, and ward its negative impact on replicability. 
  Additionally, directions are given for future research investigating the relationship between measurement issues and replicability.


keywords          : "reliability, measurement, measurement reporting, replicability, construct validity"
wordcount         : "X"

bibliography      : ["r-references.bib", "references.bib"]

floatsintext      : yes
linenumbers       : no
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : "man"
output            : papaja::apa6_pdf
knit              : worcs::cite_all
---

```{r setup, include = FALSE}
# loading R libraries
library("papaja")
library("worcs")

# loading source script
source(file = "../source_script.R")

# Code below loads the processed data. The raw data was prepared for analysis in 'prepare_data.R.
# load_data(worcs_directory = "../../Data/AnalysisData") 

# creates a reference list for all used R packages and the installed R version (does not include Rstudio)
r_refs("r-references.bib")
```

This manuscript uses the Workflow for Open Reproducible Code in Science [WORCS version 0.1.1, @vanlissaWORCS2021] to ensure reproducibility and transparency. All code and XXX data are available at <https://github.com/CasGoos/WORCSPersonalTemplate.git>.

This is an example of a non-essential citation [\@ @vanlissaWORCS2021]. If you change the rendering function to `worcs::cite_essential`, it will be removed.

```{r analysis-preferences}
# Seed for random number generation
set.seed(31102023)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Introduction

For many years, methodologists and statisticians have raised concerns about the way that research practices [@cumming2014new; @wicherts2016degrees], reporting practices [@bakker2011mis; @nuijten2016prevalence], and publication decisions [@sterling1959publication; @bakker2012rules; @giner2012science] may undermine the credibility of psychology's scientific claims. Replicability of a finding is seen as one crucial step in establishing its credibility [@nosek2022replicability]. In order to investigate the replicability of psychological science, the large scale Reproducibility Project: Psychology (RPP) was set-up [@osc2015estimating]. Replications of 100 published studies were performed using similar materials, procedures, and analyses, while using a new sample of participants for each study. They found however, that in many instances the results did no match those of the original studies. This lack of replicability of psychological findings illustrated that psychology was facing a so-called 'replication crisis' [@hughes2018psychology; @giner2019crisis].

As a response, the demand to verify the credibility of psychological findings through replications rose. In order to attempt replications of various psychological findings, numerous large scale projects were set-up [@klein2014data; @camerer2016evaluating; @camerer2018evaluating; @ebersole2016many; @ebersole2020many; @klein2018many; @klein2022many}. Among these were the Many Labs projects, which operated similarly to the RPP, with the additional feature that each original article was replicated multiple times across different labs in various locations around the world. The results from these replication projects generally showed smaller effects than those reported in the original studies. Similarly, effects that had reached statistical significance in the original no longer did across the aggregate results of its replications.

Several explanations for this discrepancy between original and replication findings have been suggested and investigated, with many being centred around the concept of Questionable Research Practices [QRPs, @john2012measuring]. QRPs are practices, which either misrepresent or omit essential methodological information that is necessary for the evaluation of the research. Without this information, a reader will not know whether the result was in line with the author's expectation or simply the result of repeated trial and error until a suitable result was found [@simmons2011false]. Consequently, much of the discourse and the subsequent proposed solutions have been centred around preventing these QRPs through increased transparency in scientific reporting. Preregistrations are one method for preventing QRPs. Preregistrations provide a clear description of the originally planned methodology of a study, which remains publicly available alongside the final report of the study. As a result, the author's original expectations and intentions can be accessed by the reader [@nosek2022replicability].

However, many researchers have also raised concerns that other factors, such as low power [@stanley2018what], lack of strong theoretical foundations [@eronen2021theory], and validity [@finkel2017replicability], which have not been given as much attention, are just as if not more responsible for the lack of successful replications found in psychology. An idea which has gained more traction recently is that measurement related issues have a large problematic influence on replicability. In particular, studies have focused on the effects of issues such as measurement error and questionable measurement practices on replicability [@stanley2014expectations; @shaw2020measurement; @flake2017construct; @flake2020measurement].

Measurement in psychological science is defined in large part by one overarching issue: psychological constructs, such as affective state and intelligence, cannot be measured directly. As a result, psychologists face two primary challenges in the use of measurement: First of all, the measure has to assess the construct it intends to measure, even though it can only do so indirectly. Secondly, because the measurement is indirect, it also comes with some degree of error surrounding its assessment of the construct. The first issue relates to the concept of validity, and specifically construct validity. Validity refers to the overall extent to which a measure measures what it is supposed to [@borsboom2004concept], while construct validity specifically refers to the substantial relation between the item scores on a psychological measure and the psychological construct it intends to measure. In order for indirect assessment of psychological constructs to be a viable approach, the measure needs to have construct validity [@cook2002experimental]. However, both original and replication research are often not reported with sufficient details on construct validity [@shaw2020measurement; @flake2022construct]. The second issue relates to the concept of measurement error and reliability. Measurement error indicates the amount of variation in item scores that is the result of the used measures inaccuracies, and not due to differences in the underlying psychological construct of interest. While what should be considered acceptable levels of measurement error is a subject of debate, and dependent upon context [@cho2015cronbach], it is generally agreed upon that too much measurement error is problematic. If measurement error is too large, the item scores relate to a construct no more than they do to random noise. As a result, the interpretability of these scores as indicators of a psychological construct is limited.

@stanley2014expectations demonstrated, using a simulation study, the drastic effect measurement error can have on attempts to verify or falsify original findings. In practice, measurement error is commonly indicated using score-reliability. Score-reliability is an index of consistency of a measure, showing how much the items converge to the same point, with the point being the measured variable. Thus, @stanley2014expectations generated scalar item data with levels of score-reliability that mimicked what is standard in psychological research, to ensure that the resulting degree of measurement error was representative. Using this data, they found that the degree of measurement error can explain a large amount of the variation in observed effects found in replication research. It could therefore be that a failed replication was simply the result of measurement of the true effect varying from the original, rather than the true effect itself being different from what was originally reported. As a result, interpretations of replications might not be based on tests of the true effect, if the impact of measurement error is not attenuated for.

Another key aspect in evaluating the use of measures is how they are reported. Questionable Measurement Practices (QMPs) have been coined as a conjugate term to QRPs. QMPs are practices that decrease the information necessary to evaluate the measurement of a study. They range from lack of transparency and unclear motivation in choice of measure, to poor justification for modifications of measure and procedure of any sourced measures [@flake2020measurement]. @flake2022construct coded the presence of a series of QMPs in the replication report in both the replication protocols of each replication in the RPP, as well as the original articles the replications were based on. They found that QMPs linked to issues for replicators in recreating the measurement to closely match the measurement in the original. As a result, there was a lack of measurement related information in both replication reports, and original articles. Without this information, it becomes difficult to establish that original and replication are measuring the same construct(s), which has detrimental consequences for direct replication attempts.

This thesis aims to investigate the influence of measurement error and QMPs on replicability, using data and reports from the large scale Many Labs replication projects. One goal is to expand the research by [@stanley2014expectations] with a practical example. Whereas they obtained their findings using a simulation study, this thesis intended to investigate the scope of their findings in practice. QMPs in the context of replication research were already investigated by [@flake2022construct]. However, the investigations remained mostly descriptive. This thesis aims to expand upon the findings of [@flake2022construct] through the use of a new set of replication data, and by investigating the relation between QMP and replicability.

## Measurement Error

As a first step it is worth investigating what the state of measurement error is in original and psychological research, in order to assess the scope of @stanley2014expectations's findings in practice. Thus, this thesis will investigate the following:

-   RQ1a. What is the degree of score reliability in replications of psychological research?

-   RQ1b. What is the degree of score reliability in original psychological research?

It is difficult to suggest an expected or informed cut-off value for what could be considered a sufficient score reliability coefficient for both original and replication studies. While suggested cut-off points for reliability do exist [@nunnaly1994assessment], they are not without their criticism [@cho2015cronbach; @crutzen2017scale]. Instead a comparison of scores between original and their associated replication studies should be of more substantive interest. Since direct replications intend to investigate effects in nearly identical scenarios with nearly identical measures as the original study, it is expected that measurement errors are as present in replication research as in original psychological research.

-   H1. No difference is present in the degree of score reliability between replication research and original psychological research.

While reliability estimates are generally considered a measure specific feature, this is false. They are context dependent, and as a result the estimates of an effect can differ across contexts [@cho2015cronbach; @pauly2018resampling]. With large enough differences across contexts, the effect estimates between different replication attempts could deviate from each other beyond direct comparison. For instance, suppose that we have a measure of multiple items assessing agreeableness in relation to both parents as well as peers. In a non-traditional society we might find high reliability for this measure, because participants that score high on one item, score high on another item since they are generally agreeable, while participants that score low one item, score low for all the other items. On the other hand, in a more traditional society we might find that reliability is low, because agreeableness towards parents is normative in that society, and as a result items relating to agreeableness towards parents may not be indicative of general agreeableness as a personality trait. In this example, a high score on the measure in the first context relates more strongly to a single agreeableness construct compared to the second context. The findings across these two populations would then no longer be directly comparable. Consequently, the value of any direct replication would become limited. To investigate this potential problem, the variation in score reliability is assessed for each measure when used in different contexts.

- RQ2. To what degree do reliability estimates differ within a replication set of the same original study?

Even though reliability estimates are context dependent, sufficient protocol structure can counteract the variance in reliability estimates. An example of the standardising impact a structured protocol can have on reducing variability within an effect can be found in investigations into the closely related issue of heterogeneity [@patacchini2007unobserved]. Heterogeneity refers to the true variation in the true effect size due to minor but key differences in context, environment, and procedure. Heterogeneity is yet another proposed explanation for failed replications. This concern even prompted a Many Labs centred around the testing of heterogeneity in replications [@klein2018many]. However analyses from Many Labs 2, and @olsson2020heterogeneity showed little empirical evidence for widespread heterogeneity in effects amongst replications. These findings demonstrate that it is possible to reduce variability as the result of context using structured protocols, as was done in the Many Labs projects this thesis analyzed.

- H2. There is no significant variation in the reliability estimates of replications of the same original study.

This thesis investigated whether there is something in the measures used for replicated findings that sets them apart from those used for non-replicated findings. In particular, this thesis extends research by @stanley2014expectations on the relationship between reliability and replicability, by using data from real life replication projects.

- RQ3. What is the association between replication study score reliability and replication outcome?

Greater score-reliability means greater consistency in estimation as variance around the estimate of the true effect is decreased. Consequently measures across different occasions, including between original and replication, should also become more consistent [@nunnaly1994assessment]. When original and replication research share consistent result, and as long as the studied effect is true, then on average their statistical conclusions should converge.

- H3. Score reliability in replication research is positively associated with successful replication of an original finding. 

Reliability is only one aspect by which a measure can be judged, with validity being another important way to determine the scientific rigour of a measure [@roberts2006reliability]. Therefore as part of the exploratory analyses, validity was assessed for the same data source used to test the aforementioned hypotheses. Specifically, the unidimensionality of the measure was determined, meaning whether or not the items of a measure all related to a single variable. 

Validity of measures can be assessed not only through features found in the data. The measure must also be chosen and implemented in a way that enables proper measurement of the variables [@flake2020measurement]. Any scientific report should therefore clearly motivate the measure and procedure that was used. Without clear reporting on these aspects, the validity of measurement will remain uncertain. Therefore, the other tests of this paper focused on the reporting aspects of measurement, assessed through QMPs.

## Questionable Measurement Practices
@flake2022construct descriptively investigated QMPs within the reproducibility project psychology [@osc2015estimating]. This thesis seeks to conceptually replicate some of their findings within the Many Labs replication projects.

- RQ4a. To what degree are QMPs present in replications of psychological research?
- RQ4b. To what degree are QMPs present in original psychological research?

The investigations by @flake2022construct have shown not only that  QMPs are common in psychological research, they can be even more common in replications reports. However, the reporting of the Many Labs projects differ from the RPP data used in @flake2022construct. The structured and preregistered protocols used for the Many Labs projects were hypothesized to lead to consistent reporting of important measurement details, and as a result contain less QMPs.

- H4. QMPs are expected to be more frequent in original psychological research than in replication research.

QMPs put the validity of a study into question. If there is a lack of transparency and the used measures are invalid, replication results become difficult to interpret [@flake2020measurement]. With a lack of transparency, closely mimicking the original's measurement becomes challenging. Furthermore, if a replication does not properly measure the variables of the original effect, the effect analysed within the replication may not be the same as the original. For these reasons, establishing a link between QMPs and non-replicability should confirm suspicions of their detrimental effects on replications [@flake2017construct; @flake2020measurement; @flake2022construct].

- RQ5. What is the association between QMPs in replication studies and replication outcome?

Fewer QMPs means that measurement decisions in a report are transparent with the number arbitrary methodological decisions being limited as well. Fewer arbitrary decisions means less ways that a replicator can deviate from the original's intended effect. One of the Many Labs projects even touched on this concept [@ebersole2020many]. In this project, peer-review of replication protocols by original authors was used to ensure that approved methods were used. Additionally, it has been observed that authors who promote greater transparency in their work, have on average stronger evidence for their hypotheses [@wicherts2011willingness]. Finally, the results shown in @protzko2020high illustrate that when original studies are transparent, preregistered, and have adequate sample size, subsequent replication attempts find nearly the same effect size as the original. All of these findings point towards a positive relation between successful replication and various types of good research practices. A similar relation is expected for good measurement practices.

- H5. QMPs in replication research are negatively associated with replication of an original finding. 

@flake2022construct have already demonstrated the issues replicators face in transparently reporting the appropriate evidence for the measures they used. That study identified among other issues, a lack of available information in original articles. If essential information on a measure is not available in an original article, it may be difficult to know the specifics of that measure when used in a replication. In that case, the lack of proper reporting on measurement would impact the transparency and validity of future replications. Consequently, the article's findings become difficult to verify. To provide insight into this issue, an investigation of the relationship between original and replication QMPs was included.

- RQ6. What is the association between QMPs in original research and QMPs in replications of psychological research?

If the relationship exists, it is expected that questionable measurement practices in an original paper might cause a spill-over effect of QMPs into the replication. If the original paper does not specify how a measure was used, the replication may implement it differently. In that case, the replication can no longer be considered a direct replication. Additionally, investigations by @shaw2020measurement have shown that more validity evidence in an original article is associated with better psychometric properties in the replication sample. Based on these arguments, hypotheses are expected to be associated across original and replication studies.

- H6. Total number of QMPs in original psychological research is positively related to total number of QMPs in replication research. 


# Method

## Sample

### Replication Datasets

### Preregistered Replication Protocols

### Original Study

## Data Collection

## Measures

### Measurement Error Measures

### Replicability Measure

### QMP measures

### Validity Measure

## Analyses

### Analyses Measurement Error

### Analyses QMPs

### Exploratory Analyses

# Results

## Descriptives

## Reliability

## Questionable Measurement Practices

## Exploratory Results

# Discussion

## Quality of Measurement

## Impact on replicability

## Limitations & Future Research

## Implications for Research and Replications in Psychological Science

# Conclusion

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
