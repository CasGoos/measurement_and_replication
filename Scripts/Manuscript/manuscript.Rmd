---
title             : "The Impact of Measurement Practices and Measurement Error on Construct Validity and Replication Outcomes in Psychological Science"
shorttitle        : "Measurement Quality and Replicability"

author: 
  - name          : "Goos, C."
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Professor Cobbenhagenlaan 125, 5037 DB, Tilburg, The Netherlands"
    email         : "casgoos99@gmail.com"
    role: # Contributorship roles (e.g., CRediT, https://credit.niso.org/)
      - "Conceptualization"
      - "Data curation"
      - "Formal Analysis"
      - "Investigation"
      - "Methodology"
      - "Visualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
  - name          : "Nuijten, M.B."
    affiliation   : "1"
    role:
      - "Project Administration"
      - "Writing - Review & Editing"
      - "Supervision"

affiliation:
  - id            : "1"
    institution   : "Tilburg University"

authornote: |
  1 Department of Methodology and Statistics, Tilburg School of Social and Behavioral Sciences, Tilburg University, Tilburg, NL. 

  Enter author note here. Caspar?

abstract: |
  Recently, the influence of measurement related issues on replicability has received more investigations. 
  In particular, studies have indicated that low reliability, and poor measurement reporting can negatively influence the replicability of an effect. 
  Building on this new line of research, this thesis assessed these influences in large scale replication projects. 
  Protocols from the Many Labs replication projects, and the original articles that the replications were based on were used to assess the  reporting of the measures, while publicly available data from the Many Labs replications was used to assess the reliability of the measures. Reliability did not relate to replicability, and was generally consistent between labs for each measure in the Many Labs projects. 
  The quality of measurement reporting in replication protocols did however relate to replicability.
  Additionally, tentative evidence was found for a relation between quality of measurement reporting in the protocol of a replication and the quality of measurement reporting in its related original study.
  Recommendations are given for researchers and replicators in order to improve measurement practices in replication, and ward its negative impact on replicability. 
  Additionally, directions are given for future research investigating the relationship between measurement issues and replicability.


keywords          : "reliability, measurement, measurement reporting, replicability, construct validity"
wordcount         : "X"

bibliography      : ["r-references.bib", "references.bib"]

floatsintext      : yes
linenumbers       : no
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : "man"
output            : papaja::apa6_pdf
knit              : worcs::cite_all
---

```{r setup, include = FALSE}
# loading R libraries
library("papaja")
library("worcs")

# loading source script
source(file = "../source_script.R")

# Code below loads the processed data. The raw data was prepared for analysis in 'prepare_data.R.
# load_data(worcs_directory = "../../Data/AnalysisData") 

# creates a reference list for all used R packages and the installed R version (does not include Rstudio)
r_refs("r-references.bib")
```

This manuscript uses the Workflow for Open Reproducible Code in Science [WORCS version 0.1.1, @vanlissaWORCS2021] to ensure reproducibility and transparency. All code and XXX data are available at <https://github.com/CasGoos/WORCSPersonalTemplate.git>.

This is an example of a non-essential citation [\@ @vanlissaWORCS2021]. If you change the rendering function to `worcs::cite_essential`, it will be removed.

```{r analysis-preferences}
# Seed for random number generation
set.seed(31102023)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Introduction

For many years, methodologists and statisticians have raised concerns about the way that research practices [@cumming2014new; @wicherts2016degrees], reporting practices [@bakker2011mis; @nuijten2016prevalence], and publication decisions [@sterling1959publication; @bakker2012rules; @giner2012science] may undermine the credibility of psychology's scientific claims. Replicability of a finding is seen as one crucial step in establishing its credibility [@nosek2022replicability]. In order to investigate the replicability of psychological science, the large scale Reproducibility Project: Psychology (RPP) was set-up [@osc2015estimating]. Replications of 100 published studies were performed using similar materials, procedures, and analyses, while using a new sample of participants for each study. They found however, that in many instances the results did no match those of the original studies. This lack of replicability of psychological findings illustrated that psychology was facing a so-called 'replication crisis' [@hughes2018psychology; @giner2019crisis].

As a response, the demand to verify the credibility of psychological findings through replications rose. In order to attempt replications of various psychological findings, numerous large scale projects were set-up [@klein2014data; @camerer2016evaluating; @camerer2018evaluating; @ebersole2016many; @ebersole2020many; @klein2018many; @klein2022many}. Among these were the Many Labs projects, which operated similarly to the RPP, with the additional feature that each original article was replicated multiple times across different labs in various locations around the world. The results from these replication projects generally showed smaller effects than those reported in the original studies. Similarly, effects that had reached statistical significance in the original no longer did across the aggregate results of its replications.

Several explanations for this discrepancy between original and replication findings have been suggested and investigated, with many being centred around the concept of Questionable Research Practices [QRPs, @john2012measuring]. QRPs are practices, which either misrepresent or omit essential methodological information that is necessary for the evaluation of the research. Without this information, a reader will not know whether the result was in line with the author's expectation or simply the result of repeated trial and error until a suitable result was found [@simmons2011false]. Consequently, much of the discourse and the subsequent proposed solutions have been centred around preventing these QRPs through increased transparency in scientific reporting. Preregistrations are one method for preventing QRPs. Preregistrations provide a clear description of the originally planned methodology of a study, which remains publicly available alongside the final report of the study. As a result, the author's original expectations and intentions can be accessed by the reader [@nosek2022replicability].

However, many researchers have also raised concerns that other factors, such as low power [@stanley2018what], lack of strong theoretical foundations [@eronen2021theory], and validity [@finkel2017replicability], which have not been given as much attention, are just as if not more responsible for the lack of successful replications found in psychology. An idea which has gained more traction recently is that measurement related issues have a large problematic influence on replicability. In particular, studies have focused on the effects of issues such as measurement error and questionable measurement practices on replicability [@stanley2014expectations; @shaw2020measurement; @flake2017construct; @flake2020measurement].

Measurement in psychological science is defined in large part by one overarching issue: psychological constructs, such as affective state and intelligence, cannot be measured directly. As a result, psychologists face two primary challenges in the use of measurement: First of all, the measure has to assess the construct it intends to measure, even though it can only do so indirectly. Secondly, because the measurement is indirect, it also comes with some degree of error surrounding its assessment of the construct. The first issue relates to the concept of validity, and specifically construct validity. Validity refers to the overall extent to which a measure measures what it is supposed to [@borsboom2004concept], while construct validity specifically refers to the substantial relation between the item scores on a psychological measure and the psychological construct it intends to measure. In order for indirect assessment of psychological constructs to be a viable approach, the measure needs to have construct validity [@cook2002experimental]. However, both original and replication research are often not reported with sufficient details on construct validity [@shaw2020measurement; @flake2022construct]. The second issue relates to the concept of measurement error and reliability. Measurement error indicates the amount of variation in item scores that is the result of the used measures inaccuracies, and not due to differences in the underlying psychological construct of interest. While what should be considered acceptable levels of measurement error is a subject of debate, and dependent upon context [@cho2015cronbach], it is generally agreed upon that too much measurement error is problematic. If measurement error is too large, the item scores relate to a construct no more than they do to random noise. As a result, the interpretability of these scores as indicators of a psychological construct is limited.

@stanley2014expectations demonstrated, using a simulation study, the drastic effect measurement error can have on attempts to verify or falsify original findings. In practice, measurement error is commonly indicated using score-reliability. Score-reliability is an index of consistency of a measure, showing how much the items converge to the same point, with the point being the measured variable. Thus, [@stanley2014expectations] generated scalar item data with levels of score-reliability that mimicked what is standard in psychological research, to ensure that the resulting degree of measurement error was representative. Using this data, they found that the degree of measurement error can explain a large amount of the variation in observed effects found in replication research. It could therefore be that a failed replication was simply the result of measurement of the true effect varying from the original, rather than the true effect itself being different from what was originally reported. As a result, interpretations of replications might not be based on tests of the true effect, if the impact of measurement error is not attenuated for.

Another key aspect in evaluating the use of measures is how they are reported. Questionable Measurement Practices (QMPs) have been coined as a conjugate term to QRPs. QMPs are practices that decrease the information necessary to evaluate the measurement of a study. They range from lack of transparency and unclear motivation in choice of measure, to poor justification for modifications of measure and procedure of any sourced measures [@flake2020measurement]. [@flake2022construct] coded the presence of a series of QMPs in the replication report in both the replication protocols of each replication in the RPP, as well as the original articles the replications were based on. They found that QMPs linked to issues for replicators in recreating the measurement to closely match the measurement in the original. As a result, there was a lack of measurement related information in both replication reports, and original articles. Without this information, it becomes difficult to establish that original and replication are measuring the same construct(s), which has detrimental consequences for direct replication attempts.

This thesis aims to investigate the influence of measurement error and QMPs on replicability, using data and reports from the large scale Many Labs replication projects. One goal is to expand the research by [@stanley2014expectations] with a practical example. Whereas they obtained their findings using a simulation study, this thesis intended to investigate the scope of their findings in practice. QMPs in the context of replication research were already investigated by [@flake2022construct]. However, the investigations remained mostly descriptive. This thesis aims to expand upon the findings of [@flake2022construct] through the use of a new set of replication data, and by investigating the relation between QMP and replicability.


## Measurement Error

## Questionable Measurement Practices

# Method

## Sample

### Replication Datasets

### Preregistered Replication Protocols

### Original Study

## Data Collection

## Measures

### Measurement Error Measures

### Replicability Measure

### QMP measures

### Validity Measure

## Analyses

### Analyses Measurement Error

### Analyses QMPs

### Exploratory Analyses

# Results

## Descriptives

## Reliability

## Questionable Measurement Practices

## Exploratory Results

# Discussion

## Quality of Measurement

## Impact on replicability

## Limitations & Future Research

## Implications for Research and Replications in Psychological Science

# Conclusion

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
