---
title             : "Assessing Reliable and Valid Measurement as a Prerequisite for Informative Replications in Psychology"
shorttitle        : "Measurement and Informative Replications"

author: 
  - name          : Cas Goos
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Professor Cobbenhagenlaan 125, 5037 DB, Tilburg, The Netherlands"
    email         : "c.goos@tilburguniversity.edu"

  - name          : Marjan Bakker
    affiliation   : "1"

  - name          : Jelte M. Wicherts
    affiliation   : "1"

  - name          : Michèle B. Nuijten
    affiliation   : "1"


authornote: |
  The authors made the following contributions. CG: Conceptualization, Data curation, Formal Analysis, Investigation, Methodology, Project Administration, Software, Visualization, Writing - Original Draft Preparation, Writing - Review & Editing; MB: Conceptualization, Supervision, Writing - Review & Editing; JW: Conceptualization, Supervision, Writing - Review & Editing; MN: Conceptualization, Project Administration, Supervision, Validation, Writing - Review & Editing.


abstract: |
  For a replication to be informative, measurement should be reliable and valid in both original and replication studies. 

keywords          : "reliability, construct validity, measurement, reporting, questionable measurement practices, replications, informative replications, psychology"
wordcount         : "1"

bibliography      : ["r-references.bib", "references.bib"]

floatsintext      : yes
linenumbers       : no
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

header-includes:
  - | 
    \makeatletter
    \renewcommand{\paragraph}{\@startsection{paragraph}{4}{\parindent}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-1em}%
      {\normalfont\normalsize\bfseries\typesectitle}}
    
    \renewcommand{\subparagraph}[1]{\@startsection{subparagraph}{5}{1em}%
      {0\baselineskip \@plus 0.2ex \@minus 0.2ex}%
      {-\z@\relax}%
      {\normalfont\normalsize\bfseries\itshape\hspace{\parindent}{#1}\textit{\addperi}}{\relax}}
    \makeatother
  - \renewcommand\author[1]{}
  - \renewcommand\affiliation[1]{}
  - \authorsnames[1, 1, 1, 1]{Cas Goos, Marjan Bakker, Jelte M. Wicherts, Michèle B. Nuijten}
  - \authorsaffiliations{{Department of Methodology and Statistics, Tilburg School of Social and Behavioral Sciences, Tilburg University, Tilburg, NL.}}

csl               : "`r system.file('rmd', 'apa7.csl', package = 'papaja')`"
documentclass     : "apa7"

classoption       : man
output            : papaja::apa6_pdf
knit              : worcs::cite_all
---

```{r setup, include = FALSE}
# loading R libraries
library(papaja)
library(worcs)
library(lavaan)
library(psych)
library(metafor)
library(forcats)
library(ggplot2)
library(grid)

# Code below loads all data. The raw data was loaded within the 'prepare_data.R script.
load_data()

# the code below removes the raw data to preserve disk space.
# If you want to rerun the data cleaning and preparations steps, DO NOT run
# this code.
# If you want to only rerun the analyses, YOU CAN run the code below.
rm(coded_data_initial_raw, coded_data_revised_raw, coded_data_vignette_raw,
   data_2.10.1, data_2.12, data_2.15, data_2.19.1, data_2.2, data_2.20, 
   data_2.23, data_2.3, data_2.4.1, data_2.4.2, data_2.8.2, data_3.5, data_5.1,
   data_5.4, data_5.5, data_5.7, data_5.9.1, data_ml1, data_ml3)

# creates a reference list for all used R packages and the installed R version 
# (does not include Rstudio)
r_refs("r-references.bib")
```

<!-- altering latex defaults to get better figure and table placement -->

\renewcommand{\arraystretch}{0.7} <!-- reducing the line spacing within tables -->

\renewcommand{\topfraction}{.8} <!-- max fraction of page for floats at top -->

\renewcommand{\bottomfraction}{.8} <!-- max fraction of page for floats at bottom -->

\renewcommand{\textfraction}{.15} <!-- min fraction of page for text -->

\renewcommand{\floatpagefraction}{.8} <!-- min fraction of page that should have floats .66 -->

\setcounter{topnumber}{3} <!-- max number of floats at top of page -->

\setcounter{bottomnumber}{3} <!-- max number of floats at bottom of page -->

\setcounter{totalnumber}{4} <!-- max number of floats on a page -->

<!-- remember to use [htp] or [htpb] for placement -->

```{r analysis-preferences}
# Seed for random number generation
set.seed(26052025)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

```{r helper_functions}
# Function for storing data openly in the intermediate data folder
saving_to_intermediate_data <- function(data){
  open_data(data = data, filename = paste0(paste0(
    "Data/IntermediateData/", deparse(substitute(data))), ".csv"),
    codebook = NULL, value_labels = NULL) 
}

# Function for storing data openly in the analysis data folder
saving_to_analysis_data <- function(data){
  open_data(data = data, filename = paste0(paste0(
    "Data/AnalysisData/", deparse(substitute(data))), ".csv"),
    codebook = NULL, value_labels = NULL) 
}
```

<!-- Introduction -->

The challenge with measurement in quantitative psychological research is that psychological constructs cannot be directly measured like height or weight. Instead, we use scores from a measurement as a proxy indicator for that psychological construct. However, we cannot simply assume that any measure we create will form a good indicator of the construct. We have to substantiate this. Psychometrics can be used to statistically estimate properties of the measure that are indicative of its validity. The effort of validating a measure represents a research effort all on its own. Regardless, the burden of validating a measure – besides some well-established psychiatric and personality measures – is most often placed on the researcher using a measure as part of their broader aim to study a psychological phenomena. When the effort involved in validating a measure is relegated to being only a component of a study that is more focused on the testing of a phenomena, we have to wonder, how well does this measurement validation process go?

## Measurement Validation

When we validate a measure using psychometrics, we evaluate it mainly on two aspects: construct validity & reliability. Construct validity refers to whether the variance in the measured item scores between participants can be attributed to variance in the unobserved constructs of interest between participants [@cronbachConstructValidityPsychological1955; @vazireCredibilityReplicabilityImproving2022]. Whereas reliability refers to how consistent a participant's responses on a measurement are [@nunnallyOverviewPsychologicalMeasurement1978; @mellenberghConceptualIntroductionPsychometrics2011]. Reliability is part of the validation of a measure because an unreliable measure cannot consistently capture a construct through the items, and therefore cannot be assumed to properly capture the construct of interest [@crutzenScaleQualityAlpha2017b]. Suppose we have a measure and we want it to measure general intelligence, and we want to validate this as a measure of general intelligence. We would see our measurement as construct valid if being more intelligent resulted in generally scoring higher on our test, and our measurement would be reliable if the score of people on our measure consistently reflected their actual intelligence.

In practice construct validity is often evaluated through factor analysis. In factor analysis, a model is fitted to explain as much of the variance of multiple items through a smaller number of factors, which are each weighted averages of the item scores. The number of factors are first specified, and then the fit of a model involving those number of factors is determined. Preferably, a model with the number of factors corresponding to the number of constructs of interest should show adequate fit. Additionally, the items aimed at the same construct should be statistically related to the same factor. For example, suppose items 1 through 5 from our general intelligence measure are meant to capture verbal intelligence, and items 6 through 10 are distinctly about mathematical intelligence. To establish construct validity, we want to find that a two factor model shows adequate fit, with one factor to which items 1 through 5 relate, and one to which items 6 through 10 relate. However, even in this scenario we cannot be certain that the second factor relates to mathematical intelligence and not for example abstract thinking, other types of validity checks as well as theoretical underpinnings will be necessary. Additionally, the factor relations may not be stable for this measure in other contexts and implementations, a concept known as measurement invariance [@hornPracticalTheoreticalGuide1992; @cheungDirectComparisonApproach2012].

Reliability gives us an indication of how much our measure captures the signal of our measured variables over noise. Psychometrics offers various ways to assess reliability empirically [@nunnallyOverviewPsychologicalMeasurement1978; @mellenberghConceptualIntroductionPsychometrics2011]. For example, we can compare scores on multiple equivalent forms of the test, between multiple different raters, or between multiple different timepoints of the same test. Still, the most common way to assess reliability is through an indication of consistency within the measure itself, and the most common among these indicators is the Cronbach's Alpha coefficient. The Cronbach's Alpha coefficient is defined as “the average of all the possible split-half coefficients for a given test” [@cronbachCoefficientAlphaInternal1951]. Here split-half coefficients being the correlation between two halves of the same test. It can be calculated by taking the ratio of interitem covariance to total variance and multiplying it by n / (n - 1), where n is the number of items in the scale. However, Cronbach's Alpha comes with strong assumptions on the underlying factor structure, including that the measure is unidimensional, so a single factor. Furthermore, because Cronbach's alpha is proportional to the total variance in the target variable in the sample, it is an indicator of reliability of a measurement, and not the measure itself. In spite of this many researchers report and interpret Cronbach's Alpha as such [@cortinaWhatCoefficientAlpha1993; @schmittUsesAbusesCoefficient1996].

Thus, psychometric tests are useful but they are not always easy to perform in practice. Even simple factor analysis requires sample sizes of at least around 100, unless the amount of variance explained by the factors is large and the factor structure is clear (Gorsuch, 1983; MacCallum et al., 1999). However, even in the latter case factor analysis does not converge with low sample sizes. Furthermore, if we want to know not just the reliability and validity of one measurement but for the measure across multiple contexts, we need to conduct multiple studies with generally even larger sample sizes per group to detect meaningful effects [@meadePowerPrecisionConfirmatory2007; @frenchMeasurementInvarianceTechniques2016; @koziolImpactModelParameterization2018]. However, what we observe in the literature is that most measures are reused once or never [@elsonPsychologicalMeasuresArent2023; @anvariFragmentedFieldConstruct2025; @anvariDefragmentingPsychology2025]. Finally, @maireadshawMeasurementPracticesLargescale2020 checked the factor structure and reliability of measures used in the Many Labs 2 replication project [@kleinManyLabs22018b] on the available data, the results. They observed that for none of the scales they assessed, a factor structure congruent with the use of the scales met all of their evaluation criteria, and reliability was rarely consistently above an acceptable threshold.

## Measurement Reporting

As illustrated in the previous section, validating measurement is a complex but essential part of doing psychological research. Yet the scientific community has not fully stepped up to this challenge [@flakeMeasurementSchmeasurementQuestionable2020]. A large number of measures used in the psychological literature lack information on the validity and reliability of the measure [@hoganEmpiricalStudyReporting2004; @flakeConstructValidationSocial2017; @flakeConstructValidityValidity2022].

Questionable Measurement Practices (QMPs) are practices that raise doubts about a measurement’s validity [@flakeMeasurementSchmeasurementQuestionable2020] and have been coined as a term analogous to Questionable Research Practices [QRPs; @simmonsFalsePositivePsychologyUndisclosed2011a; @johnMeasuringPrevalenceQuestionable2012a; @wichertsDegreesFreedomPlanning2016a]. QMPs range from lack of transparency in the reporting of the measure and unclear motivation in choice of measure to poor justification for modifications of a measure and procedure of an existing measure [@flakeMeasurementSchmeasurementQuestionable2020]. @flakeConstructValidityValidity2022 documented QMPs among 100 replications and their respective original articles from the Reproduciblity Project Psychology [@opensciencecollaborationEstimatingReproducibilityPsychological2015]. They coded the number of measures, the number of items in the measure, and the information that was reported describing the measure and justifying its use. They observed limited reporting of evidence demonstrating the reliability and validity of the measurement. Additionally, only eight of the 40 translated scales contained validity evidence for the translated version. But reporting of reliability and validity evidence is not only low, but may also be biased as well. @husseyAberrantAbundanceCronbachs2023 observed a disproportional amount of Cronbach's Alpha values at the common acceptably reliable threshold value of .70, and low reporting just below .70. Thus the lack of reported measurement evidence may on occasion be hiding psychometric skeletons in the closet.

Further findings by @flakeConstructValidityValidity2022 and others [@flakeConstructValidationSocial2017; @maireadshawMeasurementPracticesLargescale2020] also show that QMPs not only limit the reported validity of a measurement in a study, the lack of information also creates challenges for replicating researchers. Basic content descriptions such as the number of items, the response format, the scoring of the scale were irregular (!!!exact numbers!!!). If it is not clear what items were used, how they were scored, and how they were administered, then it is challenging to reconstruct the measurement for a replication study. In turn, the replication may assess the constructs in a substantially similar way, or they even assess different constructs. If different constructs are assessed, the replication cannot be seen as a test of the same phenomenon as the original. If constructs are assessed differently, the estimated effects in the replication cannot easily be substantially compared to the effects in the original study. In either case, substantial comparisons between original and replication are hindered.

## Research Aim

Our aim is to provide a descriptive account of the reliability, validity, and reporting practices of measurement in replications and related original research. We coded several aspects of the measurement reporting practices in replications and original articles. Additionally, we estimated the reliability and validity evidence using openly available replication data of the measurement item responses from across multiple labs. Our unique contribution is by evaluating reported and the recalculated measurement information for each measure together. This allows us to capture a "double whammy" scenario where there is uncertainty in the reliability and validity of these measures when we calculate these ourselves, and where QMPs obscure the ability to evaluate the measure as well as inhibiting future researchers to recreate the measurement to validate it in a new study. Besides this, our study also functions as a conceptual replication of earlier research looking at reporting practices and measurement validation using different questions and analyses on a different set of replications and original research. While the theory underlying the measurement is important to evaluate how the content of the measure fits to the constructs of interest, because evaluating the theory of a measurement requires domain specific knowledge for each measure we do not address it in this study, we refer interested readers to articles discussing theory in measurement elsewhere [@gehlbachMeasureTwiceCut2011; @humphryPsychologicalMeasurementTheory2017; @borgstedeMeaningfulMeasurementRequires2023; @hasselmanGoingSquaresTheorybased2023].

# Disclosures

## Preregistration
We preregistered data collection, coding protocol, and planned analyses: <https://osf.io/jgxyu>. We deviated from our coding protocol as is explained further in the text. Our pre-registered analyses have been move to the [Supplementary Analyses A](../../SupplementaryMaterials/SupplementaryAnalysesScripts/Supplementary_exploratory_version_pre-reg_analyses.Rmd) in favor of focusing on the descriptive results.

## Data, Materials, and Online Resources
TThis manuscript was created in RStudio [*v`r rstudioapi::versionInfo()$version`*\; @R-Rstudio] with R Version `r paste0(R.Version()$major, ".", R.Version()$minor)` [@R-base], and generated using the Workflow for Open Reproducible Code in Science [*v`r getNamespaceVersion("worcs")[[1]]`*\; @R-worcs] to ensure reproducibility and transparency. All code and data used to generate this manuscript and its results are available at: <https://github.com/CasGoos/measurement_and_replication> and <https://osf.io/9r8yt/> (DOI: 10.17605/OSF.IO/9R8YT).

## Reporting
We report how we determined all data exclusions, all manipulations, and all measures in the study. Our sample size was predetermined by the number of studies in the Many Labs projects.

## Ethical Approval
This research was approved by the Tilburg University School of Social and Behavioral Sciences Ethical Review Board (nr. TSB_TP_REMA06).


# Method

## Data Source

The data consists of three main sources: replication datasets, replication protocols, and original study articles. We retrieved the data of Many Labs 1, 2, 3, & 5 [@kleinInvestigatingVariationReplicability2014a; @ebersoleManyLabs32016; @kleinManyLabs22018b; @ebersoleManyLabs52020d] from their respective OSF pages. Many Labs 4 [@klein2022many] was excluded, as there was no publicly available replication protocol. Additionally, the replication of [@crosbyWhereWeLook2008a] in Many Labs 5 made use of videos and eye-tracking measures, which did not match this study’s focus on item-based measures.

## Unit of Analysis

Our unit of analysis is a measure of a single variable within a replication protocol or original article that was used in the main analysis that was replicated. We allowed for multiple variables to be measured per study. We used the replication protocols to identify the measure of each variable. We did not include acquiescence bias checks, manipulation checks, pilot test measures, and measures added for exploratory analyses.

## Data Collection

The data on the preregistered replication protocols, and replication datasets from Many Labs 1, 2, 3, & 5 were all retrieved from their respective OSF pages: <https://osf.io/wx7ck/>, <https://osf.io/8cd4r/>, <https://osf.io/ct89g/>, & <https://osf.io/7a6rd/>. Both the replication protocols and replication datasets were scanned through to ensure that the planned analyses were feasible. However, no coding or analysis of either of them had taken place before the analyses were preregistered. Further details on the search strategy can be found in the [coding protocol information file](../../SupplementaryMaterials/CodingProtocols/coding_protocol_information.Rmd) in the supplementary materials.



### Replication Datasets

The replication datasets refer to the publicly available datasets containing the data obtained from all labs of a Many Labs replication. For the analyses, we extracted the scores on the items of each previously identified measure that also met our inclusion criteria specified in the paragraph below. When scores could not be clearly identified, any available codebooks, analysis scripts, or study materials were used to identify the relevant scores.

To be included in our reliability recalculations and factor analyses, the measure had to be a scale of multiple items. If cleaned data were available, we chose these over raw data, to ensure that variables were coded as intended (e.g., no reverse-coded items). We omitted pilot data from the analyses. These criteria combined with difficulties in retrieving suitable data from some datasets resulted in a set of item score data from `r length(unique(calculated_reliability_lab_data\$g))` replication sets spread across on average approximately `r round(mean(table(calculated_reliability_lab_data\$g)), 0)` lab locations for our analyses.

```{r CleaningReplicationDatasetsData, include = FALSE, eval = FALSE}
##### This code can be used to rerun the data preparation to convert the raw
##### (input) data of the Many Labs replications into intermediate data.
##### However, because the intermediate data is also available in this project,
##### the manuscript can also be reproduced without running this code block.

### Below we extract the relevant data from the Many Labs' datasets. The first
### number in each dataset refers to the Many Labs project it is related to.
### The second number to which study the data is from in order of appearance in
### the Many Labs pre-registered protocols, or in the case of Many Labs 5, 
### within the OSF folder structure. If there is a third number, the study had 
### multiple relevant measures this refers to the order of appearance of the 
### measure where the data is from within the study's description in the Many 
### Labs protocol or OSF folder structure. A fourth number indicates which part
### of the data of this measure was taken, in case the measure assessed multiple
### constructs, as these were treated separately for some analyses.

## ML 1
# 1.3
data_1.3_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[22:29])
colnames(data_1.3_clean)[1] <- "g"
# 1.10
data_1.10_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[108:115])
colnames(data_1.10_clean)[1] <- "g"
# 1.11
data_1.11_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[73:76])
colnames(data_1.11_clean)[1] <- "g"
# 1.12.1
# not found
# 1.12.3
data_1.12.3.1_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[54:59])
colnames(data_1.12.3.1_clean)[1] <- "g"
data_1.12.3.2_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[60:65])
colnames(data_1.12.3.2_clean)[1] <- "g"

## ML 2
# 2.2
data_2.2_clean <- cbind(as.factor(data_2.2[[5]]), data_2.2[6:11])
colnames(data_2.2_clean)[1] <- "g"
# 2.3
# data does not appear suitable
# 2.4.1
data_2.4.1_clean <- cbind(as.factor(data_2.4.1[[5]]), data_2.4.1[6:11])
colnames(data_2.4.1_clean)[1] <- "g"
# 2.4.2
data_2.4.2_clean <- cbind(as.factor(data_2.4.2[[5]]), data_2.4.2[6:14])
colnames(data_2.4.2_clean)[1] <- "g"
# 2.8.2
data_2.8.2_clean <- cbind(as.factor(data_2.8.2[[6]]), data_2.8.2[9:13])
colnames(data_2.8.2_clean)[1] <- "g"
# 2.10.1
data_2.10.1_clean <- cbind(as.factor(data_2.10.1[[5]]), data_2.10.1[6:11])
colnames(data_2.10.1_clean)[1] <- "g"
# 2.12.1
data_2.12.1_clean <- cbind(as.factor(data_2.12[[5]]), data_2.12[c(6,7,8,9,10,31,32,33,34,35)]) 
data_2.12.1_clean[3465:6905,2:6] <- data_2.12.1_clean[3465:6905,7:11]
data_2.12.1_clean <- data_2.12.1_clean[1:6]
colnames(data_2.12.1_clean)[1] <- "g"
# 2.12.2
data_2.12.2_clean <- cbind(as.factor(data_2.12[[5]]), data_2.12[c(11,14,15,18,19,22,24,27,28,29,36,39,40,43,44,47,49,52,53,54)]) 
data_2.12.2_clean[3465:6905,2:11] <- data_2.12.2_clean[3465:6905,12:21]
data_2.12.2_clean <- data_2.12.2_clean[1:11]
colnames(data_2.12.2_clean)[1] <- "g"
# 2.12.3
data_2.12.3_clean <- cbind(as.factor(data_2.12[[5]]), data_2.12[c(12,13,16,17,20,21,23,25,26,30,37,38,41,42,45,46,48,50,51,55)]) 
data_2.12.3_clean[3465:6905,2:11] <- data_2.12.3_clean[3465:6905,12:21]
data_2.12.3_clean <- data_2.12.3_clean[1:11]
colnames(data_2.12.3_clean)[1] <- "g"
# 2.15
data_2.15_clean <- cbind(as.factor(data_2.15[[5]]), data_2.15[8:12])
colnames(data_2.15_clean)[1] <- "g"
# 2.19.1
# difficult to extract
# 2.19.2
# difficult to extract
# 2.20
data_2.20_clean <- cbind(as.factor(data_2.20[[5]]), data_2.20[6:45]) 
data_2.20_clean[3729:7396,2:21] <- data_2.20_clean[3729:7396,22:41]
data_2.20_clean <- data_2.20_clean[1:21] 
# coding so all 1's means somebody used rule-based grouping strategy
data_2.20_clean[,c(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)] <- ifelse(data_2.20_clean[,c(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)] == 1, 1, 0)
data_2.20_clean[,c(3, 5, 7, 9, 11, 13, 15, 17, 19, 21)] <- ifelse(data_2.20_clean[,c(3, 5, 7, 9, 11, 13, 15, 17, 19, 21)] == 2, 1, 0)
colnames(data_2.20_clean)[1] <- "g"
# 2.23
data_2.23_clean <- cbind(as.factor(data_2.23[[5]]), data_2.23[c(7,8,12,13,15)])
colnames(data_2.23_clean)[1] <- "g"


## ML 3
# 3.2.1
data_3.2.1 <- cbind(as.factor(data_ml3[[1]]), data_ml3[77:86] - 1)
data_3.2.1.1_clean <- na.omit(data_3.2.1[1:6])
colnames(data_3.2.1.1_clean)[1] <- "g"
data_3.2.1.2_clean <- na.omit(data_3.2.1[c(1, 7:11)])
colnames(data_3.2.1.2_clean)[1] <- "g"
# 3.5
# data appears unusable
# 3.7.1
data_3.7.1_clean <- na.omit(cbind(as.factor(data_ml3[[1]]), data_ml3[38:42]))
colnames(data_3.7.1_clean)[1] <- "g"
# 3.7.2
data_3.7.2_clean <- na.omit(cbind(as.factor(data_ml3[[1]]), data_ml3[89:94]))
colnames(data_3.7.2_clean)[1] <- "g"
# 3.8.1
# a single measure was reported
# 3.8.2
data_3.8.2_clean <- na.omit(cbind(as.factor(data_ml3[[1]]), data_ml3[29:30])) 
colnames(data_3.8.2_clean)[1] <- "g"


## ML 5
# 5.1.1
data_5.1.1_clean <- cbind(as.factor(data_5.1[[2]]), data_5.1[13:27])
colnames(data_5.1.1_clean)[1] <- "g"
# 5.1.2
data_5.1.2_clean <- cbind(as.factor(data_5.1[[2]]), data_5.1[28:33])
colnames(data_5.1.2_clean)[1] <- "g"
# 5.4
data_5.4_clean <- cbind(as.factor(data_5.4[[1]]), data_5.4[18:41])
colnames(data_5.4_clean)[1] <- "g"
# 5.5.1 & 5.5.2
# from this dataset it appears that this data will be difficult to use.
# 5.5.2
# also difficult to use
# 5.7 
data_5.7_clean <- cbind(as.factor(data_5.7[[3]]), data_5.7[c(25, 34, 35, 36, 37, 38, 39, 40, 41, 42)])
colnames(data_5.7_clean)[1] <- "g"
# 5.9.1
data_5.9.1_clean <- na.omit(cbind(as.factor(data_5.9.1[[4]]), data_5.9.1[c(79, 83, 87, 91, 95, 98, 101)]))
colnames(data_5.9.1_clean)[1] <- "g"


### Saving to Intermediate Data Folder
saving_to_intermediate_data(data_1.3_clean)
saving_to_intermediate_data(data_1.10_clean)
saving_to_intermediate_data(data_1.11_clean)
saving_to_intermediate_data(data_1.12.3.1_clean)
saving_to_intermediate_data(data_1.12.3.2_clean)
saving_to_intermediate_data(data_2.10.1_clean)
saving_to_intermediate_data(data_2.12.1_clean)
saving_to_intermediate_data(data_2.12.2_clean)
saving_to_intermediate_data(data_2.12.3_clean)
saving_to_intermediate_data(data_2.15_clean)
saving_to_intermediate_data(data_2.20_clean)
saving_to_intermediate_data(data_2.23_clean)
saving_to_intermediate_data(data_3.2.1.1_clean)
saving_to_intermediate_data(data_3.2.1.2_clean)
saving_to_intermediate_data(data_3.7.1_clean)
saving_to_intermediate_data(data_3.7.2_clean)
saving_to_intermediate_data(data_3.8.2_clean)
saving_to_intermediate_data(data_5.1.1_clean)
saving_to_intermediate_data(data_5.1.2_clean)
saving_to_intermediate_data(data_5.4_clean)
saving_to_intermediate_data(data_5.7_clean)
saving_to_intermediate_data(data_5.9.1_clean)

```

### Replication Protocols

The replication protocols refer to the publicly available protocols describing the background, methodology, and analysis of the replication of an original study. These were retrieved from the OSF pages of the Many Labs projects (the search strategy and OSF file locations can be found in the [data retrieval information](../../SupplementaryMaterials/data_retrieval_information.Rmd) supplementary document; URL <https://github.com/CasGoos/measurement_and_replication/blob/master/SupplementaryMaterials/data_retrieval_information.Rmd>).


### Original Articles

We identified and retrieved all original study articles using the citations for these articles in each replication protocol.

```{r CleaningCodedData, include = FALSE, eval = FALSE}
##### This code can be used to rerun the data preparation to convert the raw 
##### (input) data of the coded data into intermediate data. However, because 
##### the intermediate data is also available in this project, the manuscript  
##### can also be reproduced without running this code block.

# Selecting the relevant rows and columns for the data
coded_data_initial_sel <- coded_data_initial_raw[3:160, 18:57]
coded_data_revised_sel <- coded_data_revised_raw[3:160, 18:38]
coded_data_vignette_sel <- coded_data_vignette_raw[3:160, 2]

# Combining the datasets
coded_data_full <- cbind(coded_data_initial_sel, 
                         cbind(coded_data_revised_sel, coded_data_vignette_sel))

# filtering out unnecessary double columns
coded_data_full <- cbind(coded_data_full[, 1:40], coded_data_full[, 45:62])


### data preparation
# renaming columns
colnames(coded_data_full) <- c("many_labs_version", "rep_org", "title", "measure_name", 
      "variable_name", "multi", "variable_order", "N", "N_items", 
      "hypothesis_support", "reliability_type", "reliability_type_text", 
      "reliability_coeff", "def_1", "op_version", "op_1", "op_2", "op_3", "op_4", 
      "op_5", "sel_existing", "sel_existing_text", "sel_1", "sel_2", "sel_3", 
      "sel_4", "sel_psychometric_evidence", "sel_psychometric_evidence_text", 
      "quant_1", "quant_2", "quant_3", "quant_4", "mod_check", "mod_1", "mod_2", 
      "mod_3", "mod_4", "mod_5", "mod_6", "mod_time", "op_1_REV", "op_2_REV",
      "op_5_REV", "sel_1_REV", "sel_3_REV", "sel_psychometric_evidence_REV", 
      "sel_psychometric_evidence_text_REV", "quant_1_REV", "quant_2_REV", 
      "quant_3_REV", "mod_check_REV", "mod_1_REV", "mod_2_REV", "mod_3_REV", 
      "mod_4_REV", "mod_5_REV", "mod_6_REV", "inseperable_material")
  
# renaming rows
rownames(coded_data_full) <- 1:nrow(coded_data_full)

# fixing some coding mistakes
coded_data_full$variable_name[79] <- "quote attribution effect"
coded_data_full$N[3] <- "5284"
coded_data_full$N[158] <- "1202"
coded_data_full$reliability_type[50] <- "Not Reported"
coded_data_full$reliability_type[127] <- "Not Reported"
coded_data_full$op_1[145] <- "False"
coded_data_full$op_3[121] <- "True"
coded_data_full$op_5[157] <- "True"
coded_data_full$sel_1[59] <- "True"
coded_data_full$quant_2[113] <- "True"
coded_data_full$mod_time[1] <- "Before"
coded_data_full$psychometric_evidence_text_REV[coded_data_full$sel_psychometric_evidence_text_REV == "convergent validitiy"] <- "convergent validity"


# removing missing entry 77
coded_data_full <- data.frame(coded_data_full)[-77,]

# Many labs 2.25 and 2.26, as well as 3.4 and 3.5 (for replications) were coded 
# in reverse order thus need to be swapped in right order. Additionally, some
# of the entries were included later than following their order, due to some
# minor coding oversights.
Coded_Data_Full_Restructured <- coded_data_full[c(1:18, 148, 19:23, 147, 24:28, 
                                                  153, 29:40, 154, 42, 41, 155, 
                                                  43:49, 156, 51, 50, 52:76, 
                                                  157, 77:88, 149:152, 89:146),]


# rename the rownames to match the new order
rownames(Coded_Data_Full_Restructured) <- 1:nrow(Coded_Data_Full_Restructured)

# changing the variable types for each column to better represent their 
# intended variable type
class(Coded_Data_Full_Restructured$many_labs_version) <- "numeric"
Coded_Data_Full_Restructured$many_labs_version <- as.factor(Coded_Data_Full_Restructured$many_labs_version)
Coded_Data_Full_Restructured$rep_org <- droplevels(as.factor(Coded_Data_Full_Restructured$rep_org))
Coded_Data_Full_Restructured$multi <- droplevels(as.factor(Coded_Data_Full_Restructured$multi))
Coded_Data_Full_Restructured$variable_order <- droplevels(as.factor(Coded_Data_Full_Restructured$variable_order))
class(Coded_Data_Full_Restructured$N) <- "numeric"
Coded_Data_Full_Restructured$N_items <- droplevels(as.factor(Coded_Data_Full_Restructured$N_items))
Coded_Data_Full_Restructured$hypothesis_support <- droplevels(as.factor(Coded_Data_Full_Restructured$hypothesis_support))
levels(Coded_Data_Full_Restructured$hypothesis_support) <- c("No", "Unclear", "Yes")
Coded_Data_Full_Restructured$reliability_type <- droplevels(as.factor(Coded_Data_Full_Restructured$reliability_type))
class(Coded_Data_Full_Restructured$reliability_coeff) <- "numeric"
Coded_Data_Full_Restructured$def_1 <- as.logical(Coded_Data_Full_Restructured$def_1)
Coded_Data_Full_Restructured$op_1 <- as.logical(Coded_Data_Full_Restructured$op_1)
Coded_Data_Full_Restructured$op_2 <- as.logical(Coded_Data_Full_Restructured$op_2)
Coded_Data_Full_Restructured$op_3 <- as.logical(Coded_Data_Full_Restructured$op_3)
Coded_Data_Full_Restructured$op_4 <- as.logical(Coded_Data_Full_Restructured$op_4)
Coded_Data_Full_Restructured$op_5 <- as.logical(Coded_Data_Full_Restructured$op_5)
Coded_Data_Full_Restructured$sel_existing <- droplevels(as.factor(Coded_Data_Full_Restructured$sel_existing))
Coded_Data_Full_Restructured$sel_1 <- as.logical(Coded_Data_Full_Restructured$sel_1)
Coded_Data_Full_Restructured$sel_2 <- as.logical(Coded_Data_Full_Restructured$sel_2)
Coded_Data_Full_Restructured$sel_3 <- as.logical(Coded_Data_Full_Restructured$sel_3)
Coded_Data_Full_Restructured$sel_4 <- as.logical(Coded_Data_Full_Restructured$sel_4)
Coded_Data_Full_Restructured$sel_psychometric_evidence <- droplevels(as.factor(Coded_Data_Full_Restructured$sel_psychometric_evidence))
Coded_Data_Full_Restructured$quant_1 <- as.logical(Coded_Data_Full_Restructured$quant_1)
Coded_Data_Full_Restructured$quant_2 <- as.logical(Coded_Data_Full_Restructured$quant_2)
Coded_Data_Full_Restructured$quant_3 <- as.logical(Coded_Data_Full_Restructured$quant_3)
Coded_Data_Full_Restructured$quant_4 <- as.logical(Coded_Data_Full_Restructured$quant_4)
Coded_Data_Full_Restructured$mod_check <- droplevels(as.factor(Coded_Data_Full_Restructured$mod_check))
Coded_Data_Full_Restructured$mod_1 <- as.logical(Coded_Data_Full_Restructured$mod_1)
Coded_Data_Full_Restructured$mod_2 <- as.logical(Coded_Data_Full_Restructured$mod_2)
Coded_Data_Full_Restructured$mod_3 <- as.logical(Coded_Data_Full_Restructured$mod_3)
Coded_Data_Full_Restructured$mod_4 <- as.logical(Coded_Data_Full_Restructured$mod_4)
Coded_Data_Full_Restructured$mod_5 <- as.logical(Coded_Data_Full_Restructured$mod_5)
Coded_Data_Full_Restructured$mod_6 <- as.logical(Coded_Data_Full_Restructured$mod_6)
Coded_Data_Full_Restructured$mod_time <- droplevels(as.factor(Coded_Data_Full_Restructured$mod_time))
Coded_Data_Full_Restructured$op_1_REV <- as.logical(Coded_Data_Full_Restructured$op_1_REV)
Coded_Data_Full_Restructured$op_2_REV <- as.logical(Coded_Data_Full_Restructured$op_2_REV)
Coded_Data_Full_Restructured$op_5_REV <- as.logical(Coded_Data_Full_Restructured$op_5_REV)
Coded_Data_Full_Restructured$sel_1_REV <- as.logical(Coded_Data_Full_Restructured$sel_1_REV)
Coded_Data_Full_Restructured$sel_3_REV <- as.logical(Coded_Data_Full_Restructured$sel_3_REV)
Coded_Data_Full_Restructured$sel_psychometric_evidence_REV <- droplevels(as.factor(Coded_Data_Full_Restructured$sel_psychometric_evidence_REV))
Coded_Data_Full_Restructured$quant_1_REV <- as.logical(Coded_Data_Full_Restructured$quant_1_REV)
Coded_Data_Full_Restructured$quant_2_REV <- as.logical(Coded_Data_Full_Restructured$quant_2_REV)
Coded_Data_Full_Restructured$quant_3_REV <- as.logical(Coded_Data_Full_Restructured$quant_3_REV)
Coded_Data_Full_Restructured$mod_check_REV <- droplevels(as.factor(Coded_Data_Full_Restructured$mod_check_REV))
Coded_Data_Full_Restructured$mod_1_REV <- as.logical(Coded_Data_Full_Restructured$mod_1_REV)
Coded_Data_Full_Restructured$mod_2_REV <- as.logical(Coded_Data_Full_Restructured$mod_2_REV)
Coded_Data_Full_Restructured$mod_3_REV <- as.logical(Coded_Data_Full_Restructured$mod_3_REV)
Coded_Data_Full_Restructured$mod_4_REV <- as.logical(Coded_Data_Full_Restructured$mod_4_REV)
Coded_Data_Full_Restructured$mod_5_REV <- as.logical(Coded_Data_Full_Restructured$mod_5_REV)
Coded_Data_Full_Restructured$mod_6_REV <- as.logical(Coded_Data_Full_Restructured$mod_6_REV)
Coded_Data_Full_Restructured$inseperable_material <- droplevels(as.factor(Coded_Data_Full_Restructured$inseperable_material)

                                                                
                                                                

# the moral foundations questionnaire in original 2.4 is reported using all 5 of 
# its factors, whereas in replication 2.4 only the two overarching groups of 
# binding and individualizing foundations are described. For that reason a 
# shortened original dataset will be used for any direct comparisons between 
# original and replication coding.

Coded_Data_Full_Shortened <- Coded_Data_Full_Restructured[c(1:94, 96, 99:157),] 
Coded_Data_Full_Shortened[c(95,96),5] <- c("individualizing moral foundations", "binding moral foundations")
Coded_Data_Full_Shortened[95,13] <- NA
Coded_Data_Full_Shortened[96,13] <- NA


# separating the replication and original data from each other.
coded_data_replications <- Coded_Data_Full_Shortened[1:77,]
coded_data_original <- Coded_Data_Full_Shortened[78:154,]

# exporting cleaned data
saving_to_intermediate_data(coded_data_replications)
saving_to_intermediate_data(coded_data_original)

```

## Measures

### Coding Protocol

We extracted reported measurement information from the original articles and replication protocols using our preregistered coding protocol. We extracted the reported reliability coefficient and type of index (Cronbach’s Alpha, retest, interrater, etc.) when present. Similarly, we coded if any psychometric construct validity evidence, such as a factor analysis, was presented for the measure. We additionally included twenty items that were based on the QMP table presented in @flakeMeasurementSchmeasurementQuestionable2020. For each of these items we a piece information we would want to see reported with regards to the measurement was described. If the information was clearly reported we coded the item as true; false if not or not clear enough; or not applicable if not relevant for that measure (e.g., reporting results from a factor analysis for single-item measures). Example items can be seen in Table \@ref(tab:QMPCodingInfoTable).

```{r QMPCodingInfoTable, warning = FALSE}
QMP_info_dataframe <- data.frame(Category = c("Definition", "", "", "Operationalisation", "", "", "", "Selection/Creation", "", "", "Quantification", "Modification", "", "", "", "", ""),
           'N Questions' = c("1", "", "", "5", "", "", "", "4", "", "", "4", "6", "", "", "", "", ""),
           'Example Question' = c("A psychological/sociological definition",
            "is given to the name of the measured",
            "variable within the paper.", 
            "The administration format (pen-and-",
            "paper/computer) and environment (in",
            "public/in a lab) are described (Note:",
            "both should be present for a true rating).", 
            "The source of the scale is provided",
            "(in case the scale was newly developed",
            "this should be clearly stated).", 
            "The number of items are described.", 
            "Any format changes are mentioned",
            "(paper-and-pencil <–> computer), if no",
            "changes were made to the format, and",
            "this was mentioned then code as No",
            "modification. If it is not clear, then code",
            "as False."))

# making the column names look less robot speak-y.
colnames(QMP_info_dataframe) <- c("Category", "N Questions", "Example Question")


# transfer the data to an APA table for printing
apa_table(
  QMP_info_dataframe, align = c("l", "r", "l")
  , caption = "Information of QMP coding variables per category."
  , note = "N Questions refers only to the questions used for calculating QMP ratios. Selection and creation share a category as the justifications and requirements in selecting a measure are similar to those for creating a new measure."
  , escape = FALSE, placement = "htp", booktabs = TRUE)

```

After the initial coding, we made minor revisions to 14 of the 20 QMP items from the preregistered coding protocol. These items were changed after familiarizing ourselves with the way measurement was reported, and it became clear that our criteria were too stringent in the exact way and degree of detail in which to present things. For example, in the initial protocol, an example item had to be present within the article or protocol itself, or else this was counted as a QMP. In the revised protocol, references to online appendices with example items were also considered sufficient for this item. The analyses, tables, and figures presented in this article are all based on the revised coding protocol. The equivalent QMP descriptives obtained with the initial protocol can be found in [Supplementary Analyses B](../../SupplementaryMaterials/SupplementaryAnalysesScripts/Supplementary_initial_QMP_ratio_table.rmd).

We initially intended to form a QMP index using these QMP items and perform regression analyses using this index, but this index could not be validated properly and its results would be misleading to present here. A more detailed explanation of what was omitted and why, as well as the results from the preregistered analyses themselves can be found in [Supplementary Analyses A](../../SupplementaryMaterials/SupplementaryAnalysesScripts/Supplementary_exploratory_version_pre-reg_analyses.Rmd). The items are currently used as individual descriptives on the reporting practices in our sample.

```{r Calculating_QMP_Data, include = FALSE, eval = FALSE}
# function for changing QMP data to their ratio equivalent
Create_QMP_descriptive_case <- function(practice_name, original_coded_var, replication_coded_var){
  # add practice name to first row
  Practice <- practice_name
  
  # Sum all QMP/GMP (Good Measurement Practice), keeping NA items out of
  # the equation
  original_n_QMPs <- sum(original_coded_var == "FALSE", na.rm = TRUE)
  replication_n_QMPs <- sum(replication_coded_var == "FALSE", na.rm = TRUE)
  
  original_n_GMPs <- sum(original_coded_var == "TRUE", na.rm = TRUE)
  replication_n_GMPs <- sum(replication_coded_var == "TRUE", na.rm = TRUE)
  
  # calculating the N of applicable items for originals
  N_applicable_original <- original_n_QMPs + original_n_GMPs
  
  # checking that we are not dividing by 0, and then calculating the ratio of 
  # QMPs for applicable items for originals
  if(N_applicable_original != 0){
    QMP_percentage_original <- round(original_n_QMPs / (N_applicable_original), 2)
  } else{
    QMP_percentage_original <- 0
  }                               
  
  # calculating the N of applicable items for replications
  N_applicable_replication <- replication_n_QMPs + replication_n_GMPs
  
  # checking that we are not dividing by 0, and then calculating the ratio of 
  # QMPs for applicable items for replications
  if(N_applicable_replication != 0){
    QMP_percentage_replication <- round(replication_n_QMPs / N_applicable_replication, 2)
  } else{
    QMP_percentage_replication <- 0
  }     
  
  # calculating the Phi coefficient to estimate the relation between variables
  Phi <- phi(matrix(c(original_n_QMPs, original_n_GMPs, replication_n_QMPs, 
                      replication_n_GMPs), nrow = 2, byrow = TRUE))
  
  return(c(Practice, QMP_percentage_original, N_applicable_original, QMP_percentage_replication, N_applicable_replication, Phi))
}



# loading the relevant items
original_QMP_data <- coded_data_original[c("def_1", "sel_2", "op_4", "reliability_type", "sel_existing", "op_version", "op_1_REV", "sel_1_REV", "sel_3_REV", "sel_4", "sel_psychometric_evidence", "op_2_REV", "op_3", "quant_1_REV", "quant_2_REV", "quant_3_REV", "quant_4", "op_5_REV", "mod_1_REV", "mod_2_REV", "mod_3_REV", "mod_4_REV", "mod_5_REV", "mod_6_REV")]

replication_QMP_data <- coded_data_replications[c("def_1", "sel_2", "op_4", "reliability_type", "sel_existing", "op_version", "op_1_REV", "sel_1_REV", "sel_3_REV", "sel_4", "sel_psychometric_evidence", "op_2_REV", "op_3", "quant_1_REV", "quant_2_REV", "quant_3_REV", "quant_4", "op_5_REV", "mod_1_REV", "mod_2_REV", "mod_3_REV", "mod_4_REV", "mod_5_REV", "mod_6_REV")]



# create an empty dataset to add a recoded TRUE and FALSE response for QMPs to 
QMP_data <- data.frame(delete_this = rep(NA, 77))


QMP_data$reliability_reported_org <- ifelse(original_QMP_data$reliability_type != "Not Reported" & original_QMP_data$reliability_type != "" & !is.na(original_QMP_data$reliability_type), TRUE, ifelse(original_QMP_data$reliability_type == "Not Reported", FALSE, NA))

QMP_data$reliability_reported_rep <- ifelse(replication_QMP_data$reliability_type != "Not Reported" & replication_QMP_data$reliability_type != "" & !is.na(replication_QMP_data$reliability_type), TRUE, ifelse(replication_QMP_data$reliability_type == "Not Reported", FALSE, NA))


# add if clearly specified if measure existed or not
QMP_data$select_or_create_clarity_org <- original_QMP_data$sel_existing == "Not Clearly Stated"

QMP_data$select_or_create_clarity_rep <- replication_QMP_data$sel_existing == "Not Clearly Stated"


# add if version was clearly specified
QMP_data$version_clarity_org <- original_QMP_data$op_1_REV == FALSE | original_QMP_data$op_version == "" & original_QMP_data$sel_existing == "True, namely:"

QMP_data$version_clarity_rep <- replication_QMP_data$op_1_REV == FALSE | replication_QMP_data$op_version == "" & replication_QMP_data$sel_existing == "True, namely:"


# add if factor structure was analysed
QMP_data$factor_analysis_org <- ifelse(original_QMP_data$sel_psychometric_evidence == "None", FALSE, ifelse(original_QMP_data$sel_psychometric_evidence != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)", TRUE, NA))

QMP_data$factor_analysis_rep <- ifelse(replication_QMP_data$sel_psychometric_evidence == "None", FALSE, ifelse(replication_QMP_data$sel_psychometric_evidence != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)", TRUE, NA))

# direct copies
QMP_data$justified_definition_org <- original_QMP_data$def_1
QMP_data$justified_definition_rep <- replication_QMP_data$def_1
QMP_data$justified_selection_org <- original_QMP_data$sel_2
QMP_data$justified_selection_rep <- replication_QMP_data$sel_2
QMP_data$justified_operationalisation_org <- original_QMP_data$op_4
QMP_data$justified_operationalisation_rep <- replication_QMP_data$op_4
QMP_data$source_org <- original_QMP_data$sel_1_REV
QMP_data$source_rep <- replication_QMP_data$sel_1_REV
QMP_data$in_psychometric_org <- original_QMP_data$sel_3_REV
QMP_data$in_psychometric_rep <- replication_QMP_data$sel_3_REV
QMP_data$out_psychometric_org <- original_QMP_data$sel_4
QMP_data$out_psychometric_rep <- replication_QMP_data$sel_4
QMP_data$admin_format_org <- original_QMP_data$op_2_REV
QMP_data$admin_format_rep <- replication_QMP_data$op_2_REV
QMP_data$admin_procedure_org <- original_QMP_data$op_3
QMP_data$admin_procedure_rep <- replication_QMP_data$op_3
QMP_data$N_items_org <- original_QMP_data$quant_1_REV
QMP_data$N_items_rep <- replication_QMP_data$quant_1_REV
QMP_data$N_response_org <- original_QMP_data$quant_2_REV
QMP_data$N_response_rep <- replication_QMP_data$quant_2_REV
QMP_data$recoding_org <- original_QMP_data$quant_3_REV
QMP_data$recoding_rep <- replication_QMP_data$quant_3_REV
QMP_data$aggregation_org <- original_QMP_data$quant_4
QMP_data$aggregation_rep <- replication_QMP_data$quant_4
QMP_data$example_items_org <- original_QMP_data$op_5_REV
QMP_data$example_items_rep <- replication_QMP_data$op_5_REV
QMP_data$mod_admin_format_org <- original_QMP_data$mod_1_REV
QMP_data$mod_admin_format_rep <- replication_QMP_data$mod_1_REV
QMP_data$mod_admin_format_support_org <- original_QMP_data$mod_2_REV
QMP_data$mod_admin_format_support_rep <- replication_QMP_data$mod_2_REV
QMP_data$mod_language_org <- original_QMP_data$mod_3_REV
QMP_data$mod_language_rep <- replication_QMP_data$mod_3_REV
QMP_data$mod_language_support_org <- original_QMP_data$mod_4_REV
QMP_data$mod_language_support_rep <- replication_QMP_data$mod_4_REV
QMP_data$mod_N_items_or_response_org <- original_QMP_data$mod_5_REV
QMP_data$mod_N_items_or_response_rep <- replication_QMP_data$mod_5_REV
QMP_data$mod_N_items_or_response_support_org <- original_QMP_data$mod_6_REV
QMP_data$mod_N_items_or_response_support_rep <- replication_QMP_data$mod_6_REV


QMP_data <- QMP_data[,!(names(QMP_data) %in% "delete_this")]



### QMP ratio data
# create the empty QMP ratio dataset
QMP_ratio_data <- data.frame(Practice = rep("", ncol(QMP_data)/2),
                   QMP_percentage_original = rep(0, ncol(QMP_data)/2),
                   N_applicable_original = rep(0, ncol(QMP_data)/2),
                   QMP_percentage_replication = rep(0, ncol(QMP_data)/2),
                   N_applicable_replication = rep(0, ncol(QMP_data)/2),
                   Phi = rep(0, ncol(QMP_data)/2))


# we loop through the length of the number of Measurement Practices (MPs) 
# ignoring doubles due to having both original and replication
for(i in 1:(ncol(QMP_data)/2)){
  # we create the name of the variable by taking the name and removing the 
  # _org suffix. Then we for each column except the NA initial column
  # we take the original MP and the related replication MP, and calculate
  # their QMP
  QMP_ratio_data[i,] <- Create_QMP_descriptive_case(substr(names(QMP_data)[i*2], 1, nchar(names(QMP_data)[i*2]) - 4), QMP_data[[(i*2)-1]], QMP_data[[i*2]])
  }

# store QMP ratio data in the analysis data folder
saving_to_analysis_data(QMP_ratio_data)
```

### Recalculating Data

We calculated reliability for each measure from the replication datasets with suitable data for each lab the measure was used in. We calculated both Cronbach’s alpha, as well as its standard error using formulas 2 & 3 from @duhachekAlphasStandardError2004a to calculate the standard error in the meta-analysis. Then we entered these two values into a meta-analysis of the reliability, also commonly referred to as a Reliability Generalization (RG) Meta-Analysis [@botellaManagingHeterogeneityVariance2012; @lopez-ibanezReliabilityGeneralizationMetaanalysis2024; @vacha-haaseReliabilityGeneralizationExploring1998]. Using RG Meta-Analysis we quantified the degree of true variation (or heterogeneity) in reliability coefficients across lab locations. We performed the RG Meta-Analysis using the rma function from the metafor R package [*v`r getNamespaceVersion("metafor")[[1]]`*; @R-metafor] and default settings. We evaluated the heterogeneity using the tau statistic and the Cochran's Q-test [@cochranCombinationEstimatesDifferent1954]. These indicators have received criticism as indicators of heterogeneity, especially when within study sample sizes are small and power to detect heterogeneity is low [@hoaglinMisunderstandingsCochransTest2016; @pereiraCriticalInterpretationCochrans2010]. Therefore, we also present the prediction interval and suggest that the heterogeneity results should be viewed with a critical eye [@borensteinAvoidingCommonMistakes2024]. We implemented no correction for bias, because the Many Labs replications were not at risk of publication bias.

Our analyses will focus on Cronbach’s Alpha, because its common use in research allows us to make comparisons between calculated and reported reliabilities. Furthermore, we were able to calculate the standard error of alpha to be used in the RG meta-analysis. We additionally estimated McDonald’s Omega for each lab from the same set of replications as for Cronbach's Alpha, since it has been argued to be a more informative measure of reliability than Cronbach's Alpha, while also simultaneously providing some validity evidence [@crutzenScaleQualityAlpha2017b; @dengTestingDifferenceReliability2017]. McDonald’s Omega was calculated using the omega function in the *psych* R package [*v`r getNamespaceVersion("psych")[[1]]`*; @R-psych]. Default arguments were used in the function except the nfactors argument, which was set to 1. No RG Meta-Analysis was performed on McDonald's Omega, as we could not find a formula for calculating its standard error.

```{r calculated_reliability_across_labs, include = FALSE, eval = FALSE}
# creating an empty data frame to insert all the responses into
calculated_reliability_lab_data <- data.frame(alpha = 0, omega.tot = 0, 
                                          omega.hier = 0, ASE = 0, g = 0)

# Combining the data together 1.3, & 5.4 were omitted, because data was not
# recorded in usable numeric format
extracted_score_data <- list(data_1.10_clean, data_1.11_clean, 
    data_1.12.3.1_clean, data_1.12.3.2_clean, data_2.12.1_clean, 
    data_2.12.2_clean, data_2.12.3_clean, data_2.15_clean, data_2.20_clean, 
    data_2.23_clean, data_3.2.1.1_clean, data_3.2.1.2_clean, data_3.7.1_clean,
    data_3.7.2_clean, data_3.8.2_clean, data_5.1.1_clean, data_5.1.2_clean, 
    data_5.7_clean, data_5.9.1_clean)


# obtaining the omega and alpha values for a measure in one lab.
get_omega_and_alpha_values <- function(Data){
  # first we calculate alpha and ase separately in case the omega function goes haywire
  alpha <- psych::alpha(Data)$total[["std.alpha"]]
  ase <- psych::alpha(Data)$total[["ase"]]
  
  # try to calculate the omega
  result <- c(NA, NA)
  tryCatch({
      result <- omega(Data)
    }, error = function(e) {
      result <- c(NA, NA)
    }, warning = function(w) {
      result <- c(NA, NA)
    })
  
  # combine the information in one vector
  omega_and_alpha_vec <- as.numeric(c(alpha, result[c(4, 1)], ase))
  
  return(omega_and_alpha_vec)
}


# calculate the alpha, omega.tot, omega.hier, & ASE for all relevant datasets
# for each lab.
for (i in 1:length(extracted_score_data)){
  calculated_reliability_instance <- tapply(extracted_score_data[[i]][-1], 
                                            extracted_score_data[[i]]$g, 
                                            get_omega_and_alpha_values)
  
  calculated_reliability_instance <- data.frame(matrix(unlist(
    calculated_reliability_instance), ncol = 4, byrow = TRUE))
  
  # make sure the var names match the complete dataframe
  colnames(calculated_reliability_instance) <- c("alpha", "omega.tot", 
                                                 "omega.hier", "ASE")
  # adding the measure as a group (g) indicator
  calculated_reliability_instance$g <- i
  
  # adding this measure's data to the total
  calculated_reliability_lab_data <- rbind(calculated_reliability_lab_data, 
                                       calculated_reliability_instance)
}


# removing the empty first row
calculated_reliability_lab_data <- calculated_reliability_lab_data[-1,]

# making sure group (g) is a factor
calculated_reliability_lab_data$g <- as.factor(calculated_reliability_lab_data$g)

# indexing the meta-analysis results with a specific index relating to a 
# row (measure) in coded_data_replications
calculated_reliability_lab_data$coded_data_index <- c(rep(10, 36), rep(11, 36), 
  rep(14, 36), rep(14, 36), rep(29, 74), rep(30, 74), rep(31, 74), rep(34, 61), 
  rep(39, 60), rep(42, 58), rep(51, 21), rep(51, 21), rep(59, 20), rep(60, 21), 
  rep(61, 21), rep(65, 4), rep(66, 4), rep(74, 8), rep(76, 5))

# changing the group variable to reflect measure descriptions from the text.
# checked using:
# coded_data_replications[unique(calculated_reliability_data$reporting_index), 3], and
# coded_data_replications[unique(calculated_reliability_data$reporting_index), 5]
levels(calculated_reliability_lab_data$g) <- c("Caruso et al. (2012)", 
    "Husnu & Crisp (2010)", "Nosek et al. (2002), Math", "Nosek et al. (2002), Art", 
    "Anderson et al. (2012), SWL", "Anderson et al. (2012), PA", 
    "Anderson et al. (2012), NA", "Giessner & Schubert, (2007)", 
    "Norenzayan et al. (2002)", "Zhong & Lijenquist (2006)", 
    "Monin & Miller (2001), most", "Monin & Miller (2001), some", 
    "Cacioppo et al. (1983), arg",  "Cacioppo et al. (1983), nfc", 
    "De Fruyt et al. (2000)", "Albarracín et al. (2008), exp 5 verb", 
    "Albarracín et al. (2008), exp 5 math", "Shnabel & Nadler (2008)",
    "Vohs & Schooler (2008)")
  
calculated_reliability_lab_data$g <- factor(calculated_reliability_lab_data$g, 
    labels = c("Caruso et al. (2012)", 
    "Husnu & Crisp (2010)", "Nosek et al. (2002), Math", 
    "Nosek et al. (2002), Art", "Anderson et al. (2012), SWL", 
    "Anderson et al. (2012), PA", "Anderson et al. (2012), NA", 
    "Giessner & Schubert, (2007)", "Norenzayan et al. (2002)", 
    "Zhong & Lijenquist (2006)", "Monin & Miller (2001), most", 
    "Monin & Miller (2001), some", "Cacioppo et al. (1983), arg",  
    "Cacioppo et al. (1983), nfc", "De Fruyt et al. (2000)", 
    "Albarracín et al. (2008), exp 5 verb", 
    "Albarracín et al. (2008), exp 5 math", "Shnabel & Nadler (2008)", 
    "Vohs & Schooler (2008)"))

# adding whether or not an effect replicated based on what was coded from the 
# replication report.
calculated_reliability_lab_data$replication_success <- c(coded_data_replications[
    calculated_reliability_lab_data$coded_data_index, "hypothesis_support"])



# store per lab calculated reliability data in the analysis data folder
saving_to_analysis_data(calculated_reliability_lab_data)
```

```{r averaged_reliability, include = FALSE, eval = FALSE}
# function to assess the heterogeneity in the calculated Cronbach's Alpha values
# and get Cronbach's Alpha prediction intervals
assess_heterogeneity<- function(data_on_alpha){
  # run a random effects meta-analysis
  rma_model <- rma(yi = data_on_alpha$alpha, sei = data_on_alpha$ASE, 
                   method = "REML", control = list(stepadj = 0.5, maxiter = 1000))
  
  # extract the relevant heterogeneity information
  temp_tau <- sqrt(rma_model$tau2)
  temp_QEp <- rma_model$QEp
  
  # get prediction intervals for alpha
  rma_prediction <- predict(rma_model)
  temp_pi.lb <- rma_prediction$pi.lb
  temp_pi.ub <- rma_prediction$pi.ub
  
  return(c(temp_tau, temp_QEp, temp_pi.lb, temp_pi.ub))
}

# function to convert the lab specifc reliability to averaged
convert_reliability_data_to_avg <- function(reliability_data){
  # conducting the reliability-generalization meta-analysis for each measure
  heterogeneity_results <- assess_heterogeneity(reliability_data[c(1, 4)])
  
  # getting the avearage reliability scores for each measure
  avg_reliabilities <- colMeans(reliability_data[1:4], na.rm = TRUE)
  
  # indicates the measure
  g <- reliability_data$g[[1]]
  
  # we need one of the indices per measure as they are all the same across labs
  coded_data_index <- reliability_data$coded_data_index[[1]]
  
  # we need one of the indices per measure as they are all the same across labs
  replication_success <- reliability_data$replication_success[[1]]
  
  # extracting the reported reliability coefficient 
  coefficient_reported <- coded_data_original$reliability_coeff[coded_data_index]
  
  # calculating the difference between reported and calculated average 
  # reliability coefficient
  coeficient_difference <- coefficient_reported - avg_reliabilities[[1]]
  
  # testing whether or not (for those studies that had a reported alpha) if
  # it was out of the 95% bounds around the mean calculated alpha
  population_95_bounds <- quantile(reliability_data$alpha, probs = c(0.025, 0.975))
  
  significance_reported_coefficient <- coefficient_reported < population_95_bounds[1] | 
                                          coefficient_reported > population_95_bounds[2]
  
  # return all the data as a single row in the dataframe
  return(data.frame(alpha = avg_reliabilities[[1]], omega.tot = avg_reliabilities[[2]], 
                    omega.hier = avg_reliabilities[[3]], ASE = avg_reliabilities[[4]], 
                    tau = heterogeneity_results[[1]], QEp = heterogeneity_results[[2]],
                    pi.lb = heterogeneity_results[[3]], pi.ub = heterogeneity_results[[4]], 
                    g = g, coded_data_index = coded_data_index, 
                    replication_success, reported_coefficient = coefficient_reported, 
                    coefficient_difference = coeficient_difference, 
                    significance_reported_coefficient = significance_reported_coefficient))
}

# calculate the average reliability data + heterogeneity test + reported and calculated
# reliability coefficient comparison
avg_reliability_list <- tapply(calculated_reliability_lab_data, calculated_reliability_lab_data$g, convert_reliability_data_to_avg)

# convert data output to a dataframe
measure_reliability_data <- do.call(rbind.data.frame, avg_reliability_list)


# store per measure  reliability data in the analysis data folder
saving_to_analysis_data(measure_reliability_data)
```

Finally, we also tested these measures for unidimensionality. Our unidimensionality check serves a double purpose. First, unidimensionality is one of the assumptions of Cronbach's Alpha. If this assumption does not hold for a measure, Cronbach's Alpha gives a biased and uninformative estimate of the reliability. Second, all measurement items for which unidimensionality was checked, were used to form a singular index of one latent vasriable. Certainly, not all singular indexes require the underlying latent factor model to be unidimensional (e.g., general intelligence can be used as a variable on its own or sub-divided in multiple components such as numerical and verbal intelligence). Still, we believe it is not unreasonable to assume that for most of our measures we should observe a unidimensional factor model to have at least adequate fit.

We tested for unidimensionality using two tests on the data from each lab for each measure. If at least one of these tests was passed, the measure in that lab was marked as unidimensional. For the first test we fit a Confirmatory Factor Analysis (CFA) model with one factor to the data, and we then evaluate the RMSEA value. If this value is below .08, the test is passed. For the second test we run a parallel analysis. This runs multiple Exploratory Factor Analyses (EFA) where the number of factors in the model is increased by 1 until the number of factors is one less than the number of items. It then compares the eigenvalues (an indication of how much variance is explained by that factor) of each factor to the eigenvalues for that factor if the data matrix was effectively random. If only the single factor has an eigenvalue that is significantly higher than the eigenvalue when the data matrix is random, the test is passed. We chose a combination of these two tests, as the RMSEA is known to be biased for models with small degrees of freedom [@kennyPerformanceRMSEAModels2015], while the parallel analysis may be less suited if a two or more factor solution would also fit the sample item data.

```{r validity_and_alpha_assumption_tests, include = FALSE, eval = FALSE}
# function that runs all our checks for Cronbach's alpha. Also functions as some basic validity checks
validity_and_alpha_assumption_tests <- function(data){
  ### Tests for Unidimensionality
  # Test 1a: obtaining single factor model cfa RMSEA value for evaluation
  RMSEA_values <- c(NA, NA)
  tryCatch({
    # creating the base function for the model (it is unidimensional)
    model_free <- "Factor =~ " 

    for (item_name in names(data)[-1]){
      # adding all of the items from the scale to the model formula
      model_free <- paste0(model_free, item_name, " + ")
    }
      
    # trimming the excess " + "
    model_free <- substring(model_free, 1, nchar(model_free) - 3)

    # fitting the free coefficient estimated single dimensional model
    fit.modelfree <- cfa(model_free, 
                              data = data, 
                              std.lv = TRUE, 
                              estimator = "MLM")

    rmsea <- fitMeasures(fit.modelfree)["rmsea"] 
    rmsea.ci.upper <- fitMeasures(fit.modelfree)["rmsea.ci.upper"]
    rmsea.se <- (rmsea.ci.upper - rmsea) / 1.645
    RMSEA_values <- c(rmsea, rmsea.se)
    
  }, error = function(e) {
    RMSEA_values <- c(NA, NA)
  })
  
  
  # Test 1b: conducting parallel test to check if one factor solution is best.
  parallel_N_factors <- NA
  tryCatch({
      parallel_N_factors <- suppressWarnings(fa.parallel(data[-1], fa = "fa", plot = FALSE))$nfact

    }, error = function(e) {
      parallel_N_factors <- NA
  })
  
  # adding a single factor check indicator value
  if(is.null(RMSEA_values[[1]]) | is.null(parallel_N_factors)){
    single_factor <- NA
  } else{
    single_factor <- ifelse(RMSEA_values[[1]] < 0.08 | parallel_N_factors == 1, "Yes", "No")
  }
  
  
  ### Test 2: Test for Tau equivalence
  Tau_results <- c(NA, NA, NA)
  tryCatch({
    # creating the base function for the fixed model (it is unidimensional)
    model_tau_restrict <- "Factor =~ " 
  
    for (item_name in names(data)[-1]){
      # adding all of the items from the scale to the model formula
      # estimated with the same coefficient a to emulate a Tau equivalent model
      model_tau_restrict <- paste0(model_tau_restrict, "a*", item_name, " + ")
    }
    
    # trimming the excess " + "
    model_tau_restrict <- substring(model_tau_restrict, 1, nchar(model_tau_restrict) - 3)
    
    # fitting the tau restricted model
    fit.modeltaurestrict <- cfa(model_tau_restrict, 
                                     data = data, 
                                     std.lv = TRUE, 
                                     estimator = "MLM")
    
    #summary(fit.modelfree, fit.measures = TRUE)
    #summary(fit.modeltaurestrict, fit.measures = TRUE)
    
    # conducting the fit test between the free coefficient model and the tau
    # restricted model
    fit.test <- anova(fit.modelfree, fit.modeltaurestrict)
    
    # extract the relevant results
    Tau_results <- fit.test[2, c("Chisq diff", "Df diff", "Pr(>Chisq)")]
    }, error = function(e) {
      Tau_results <- c(NA, NA, NA)
  })
  
  ### Test 3: Assessment of uncorrelated errors
  error_cor_results <- rep(NA, 4)
  
  
  tryCatch({
    # copying the tau restricted model as the baseline.
    model_freed_errors <- model_tau_restrict
    fit.modelfreed <- fit.modeltaurestrict
    
    # create empty vector to put data into
    spec.all.vec <- rep(NA, 5)
    
    # looping through five instances of freeing the highest mod index error 
    # covariance parameter estimation. We loop 5 times rather than adding the top 5
    # highest mod index, because when one parameter is freed it will affect the extent 
    # to which other error covariances impact the fit since (part of) their additional 
    # explained variance may have already been captured in the parameter just freed.
    for (i in 1:5) {
      modindex_highest <- suppressWarnings(modindices(fit.modelfreed, sort = TRUE))[1,]
      
      # store the fully standardized estimated coefficient change for the highest
      # mod index freed parameter
      spec.all.vec[i] <- modindex_highest[[7]]
      
      # add the freeing of the parameter into the model code
      model_freed_errors <- paste(model_freed_errors, "\n", modindex_highest[[1]], modindex_highest[[2]], modindex_highest[[3]])
      
      # rerun the model
      fit.modelfreed  <- cfa(model_freed_errors, 
                             data = data, 
                             std.lv = TRUE, 
                             estimator = "MLM")
    }
  
    # perform a model comparison between the model with the top 5 error covariances
    # freely estimated and the model with Tau equivalence.
    fit.test2 <- anova(fit.modeltaurestrict, fit.modelfreed)
    
    error_cor_test_results <- fit.test2[2, c("Chisq diff", "Df diff", "Pr(>Chisq)")]
    
    # calculate the mean of the fully standardized estimated coefficient change for
    # all the highest mod index freed parameters
    spec.all.mean <- mean(spec.all.vec)
    
    # store all error related results
    error_cor_results <- unlist(c(spec.all.mean, error_cor_test_results))  
    
    }, error = function(e) {
      error_cor_results <- NA
  })
  
  
  ### storing all of the data
  FA_data <- c(RMSEA_values, parallel_N_factors, single_factor, 
               Tau_results, error_cor_results)
  
  # returning the FA data
  return(FA_data)
}


extracted_data_list <- list(data_1.10_clean, data_1.11_clean, data_1.12.3.1_clean, data_1.12.3.2_clean, data_2.12.1_clean, data_2.12.2_clean, data_2.12.3_clean, data_2.15_clean, data_2.20_clean, data_2.23_clean, data_3.2.1.1_clean, data_3.2.1.2_clean, data_3.7.1_clean, data_3.7.2_clean, data_3.8.2_clean, data_5.1.1_clean, data_5.1.2_clean, data_5.7_clean, data_5.9.1_clean)


# list to store the data in
FA_list <- list(NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
                NA, NA, NA, NA, NA, NA, NA, NA, NA)

# running the validity/alpha assumption checks for all of the extracted datasets
for (i in 1:length(extracted_data_list)){
  # applying the function across each individual lab
  FA_data <- tapply(extracted_data_list[[i]], extracted_data_list[[i]]$g,
                    validity_and_alpha_assumption_tests)

  # transforming the returned list to a dataframe
  FA_data_frame <- do.call(rbind.data.frame, FA_data)
  
  # Setting the column names
  names(FA_data_frame) <- c("RMSEA", "RMSEA_se", "N_factors", "unidimensional", "Tau_chisq_diff", 
                            "Tau_Df_diff", "Tau_p_diff", "mean_spec", "err_chisq_diff", 
                            "err_Df_diff", "err_p_diff")
  
  # append results to the overarching FA results list
  FA_list[[i]] <- FA_data_frame
}




# store all dataframes with the list of the assumption test data in 
# the analysis data folder
fa_Caruso_2012 <- FA_list[[1]]
saving_to_analysis_data(fa_Caruso_2012)
fa_Husnu_2010 <- FA_list[[2]]
saving_to_analysis_data(fa_Husnu_2010)
fa_Nosek_2002_Math <- FA_list[[3]]
saving_to_analysis_data(fa_Nosek_2002_Math)
fa_Nosek_2002_Art <- FA_list[[4]]
saving_to_analysis_data(fa_Nosek_2002_Art)
fa_Anderson_2012_SWL <- FA_list[[5]]
saving_to_analysis_data(fa_Anderson_2012_SWL)
fa_Anderson_2012_PA <- FA_list[[6]]
saving_to_analysis_data(fa_Anderson_2012_PA)
fa_Anderson_2012_NA <- FA_list[[7]]
saving_to_analysis_data(fa_Anderson_2012_NA)
fa_Giessner_2007 <- FA_list[[8]]
saving_to_analysis_data(fa_Giessner_2007)
fa_Norenzayan_2002 <- FA_list[[9]]
saving_to_analysis_data(fa_Norenzayan_2002)
fa_Zhong_2006 <- FA_list[[10]]
saving_to_analysis_data(fa_Zhong_2006)
fa_Monin_2001_most <- FA_list[[11]]
saving_to_analysis_data(fa_Monin_2001_most)
fa_Monin_2001_some <- FA_list[[12]]
saving_to_analysis_data(fa_Monin_2001_some)
fa_Cacioppo_1983_arg <- FA_list[[13]]
saving_to_analysis_data(fa_Cacioppo_1983_arg)
fa_Cacioppo_1983_nfc <- FA_list[[14]]
saving_to_analysis_data(fa_Cacioppo_1983_nfc)
fa_De_Fruyt_2000 <- FA_list[[15]]
saving_to_analysis_data(fa_De_Fruyt_2000)
fa_Albarracin_2008_verb <- FA_list[[16]]
saving_to_analysis_data(fa_Albarracin_2008_verb)
fa_Albarracin_2008_math <- FA_list[[17]]
saving_to_analysis_data(fa_Albarracin_2008_math)
fa_Shnabel_2008 <- FA_list[[18]]
saving_to_analysis_data(fa_Shnabel_2008)
fa_Vohs_2008 <- FA_list[[19]]
saving_to_analysis_data(fa_Vohs_2008)


rm(extracted_data_list)
```

# Results

## Reported Reliability & Validity Evidence

Figure \@ref(fig:ReliabilityReportingFlowDiagram) depicts the flow of measures in relation to reliability reporting. First, it shows that almost half of the measures in both replication (N = `r sum(coded_data_replications$N_items == "1 item measure")`) and original research (N = `r sum(coded_data_original$N_items == "1 item measure")`) were single-item measures. Second, only `r apa_num(sum(coded_data_original$reliability_type != "Not Reported" & coded_data_original$reliability_type != ""), numerals = FALSE)` measures in original and `r apa_num(sum(coded_data_replications$reliability_type != "Not Reported" & coded_data_replications$reliability_type != ""), numerals = FALSE)` in replications were reported alongside a reliability indicator. Cronbach’s Alpha was the most commonly reported reliability indicator.

```{r ReliabilityReportingFlowDiagram, fig.cap = "Reliability reporting flow diagram. Figure shows the number of measures as reported in both the replication protocols and original article, which meet the criterion in the box within the diagram and those criteria before it.", out.height = "60%"}
knitr::include_graphics(path = "../../SupplementaryMaterials/reliability_reporting_flow_diagram.png")
```

For validity evidence the pattern was similar. Only, `r apa_num(sum(coded_data_original$sel_psychometric_evidence_REV != "None" & coded_data_original$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)" | coded_data_original$sel_3_REV == TRUE, na.rm = TRUE), numerals = FALSE)` original studies and `r apa_num(sum(coded_data_replications$sel_psychometric_evidence_REV != "None" & coded_data_replications$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)" | coded_data_replications$sel_3_REV == TRUE, na.rm = TRUE), numerals = FALSE)` replications reported psychometric validity evidence on the measurement. Replications reported both less reliability and validity evidence. Exploratory factor analyses (original: `r apa_num(sum(coded_data_original$sel_psychometric_evidence_REV == "Exploratory Factor Analysis"), numerals = FALSE)`, replications: `r apa_num(sum(coded_data_replications$sel_psychometric_evidence_REV == "Exploratory Factor Analysis"))`) and convergent validity evidence (original: `r apa_num(sum(coded_data_original$sel_psychometric_evidence_text_REV == "convergent validity"), numerals = FALSE)`, replications: `r apa_num(sum(coded_data_replications$sel_psychometric_evidence_text_REV == "convergent validity"), numerals = FALSE)`) were the most common. Psychometric validity evidence from previous studies was reported in `r apa_num(sum(coded_data_original$sel_4 == TRUE, na.rm = TRUE), numerals = FALSE)` original studies and `r apa_num(sum(coded_data_replications$sel_4 == TRUE, na.rm = TRUE), numerals = FALSE)` replications. For single-item measures, `r apa_num(sum(coded_data_replications$sel_psychometric_evidence_text_REV[coded_data_replications$N_items == "1 item measure"] != ""), numerals = FALSE)` were reported with validity evidence – specifically, all were convergent validity – in the replication protocols. In the original studies, `r apa_num(sum(coded_data_original$sel_psychometric_evidence_text_REV[coded_data_original$N_items == "1 item measure"] != ""), numerals = FALSE)` single-item measures came with reported validity evidence.

```{r validity_evidence_reporting}
table(coded_data_original$sel_psychometric_evidence_REV)
table(coded_data_replications$sel_psychometric_evidence_REV)

# how much psychometric validity evidence was reported
sum(coded_data_original$sel_psychometric_evidence_REV != "None" & coded_data_original$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)" | coded_data_original$sel_3_REV == TRUE, na.rm = TRUE)
sum(coded_data_replications$sel_psychometric_evidence_REV != "None" & coded_data_replications$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)" | coded_data_replications$sel_3_REV == TRUE, na.rm = TRUE)

table(coded_data_original$sel_psychometric_evidence_text_REV)
table(coded_data_replications$sel_psychometric_evidence_text_REV)


# internal
table(coded_data_original$sel_3_REV)
table(coded_data_replications$sel_3_REV)
# external
table(coded_data_original$sel_4)
table(coded_data_replications$sel_4)

sum(coded_data_original$sel_4 == TRUE, na.rm = TRUE)
sum(coded_data_replications$sel_4 == TRUE, na.rm = TRUE)


# how many existing measures were used
sum(coded_data_original$sel_existing == "True, namely:")
sum(coded_data_replications$sel_existing == "True, namely:")

# for existing measures, how many had the specific version reported
sum(coded_data_original$op_1_REV[coded_data_original$sel_existing == "True, namely:"] == TRUE, na.rm = TRUE)
sum(coded_data_replications$op_1_REV[coded_data_replications$sel_existing == "True, namely:"] == TRUE , na.rm = TRUE)


# for measures how much exploratory factor analysis validity evidence was reported
sum(coded_data_original$sel_psychometric_evidence_REV == "Exploratory Factor Analysis")
sum(coded_data_replications$sel_psychometric_evidence_REV == "Exploratory Factor Analysis")


# for measures how much convergent validity evidence was reported
sum(coded_data_original$sel_psychometric_evidence_text_REV == "convergent validity")
sum(coded_data_replications$sel_psychometric_evidence_text_REV == "convergent validity")


# for single item measures was any validity evidence reported
apa_num(sum(coded_data_replications$sel_psychometric_evidence_text_REV[coded_data_replications$N_items == "1 item measure"] != ""), numerals = FALSE)

apa_num(sum(coded_data_original$sel_psychometric_evidence_text_REV[coded_data_original$N_items == "1 item measure"] != ""), numerals = FALSE)


# if an existing measure was used, how did that affect the reporting of reliability
# and validity evidence
table(coded_data_original$sel_existing, coded_data_original$reliability_type) # reliability does appear to indicate more reporting for existing reliabilities

table(coded_data_original$sel_existing, coded_data_original$sel_psychometric_evidence_REV) # not much to go on for psychometric validity evidence

# overall not substantive enough data too highlight in the article (would require a test to evaluate)
```

## Calculated Reliability & Validity Evidence

The average calculated Cronbach’s alpha coefficient across individual replications was `r apa_num(mean(calculated_reliability_lab_data$alpha))` with a standard deviation of `r apa_num(sd(calculated_reliability_lab_data$alpha))`. Figure \@ref(fig:PlotAlphaDistributions) displays the distributions of the calculated Cronbach’s Alpha scores from each lab for each measure, separated by successful and unsuccessful replication, based on the meta-analytic p-value (alpha \< .05) retrieved from the Many Labs reports.

```{r alpha_distributions_data_prep}
### Preparation code for the alpha distribution figure
# creating an editable copy of the calculated_reliability_lab_data
calculated_alpha_plot_data <- calculated_reliability_lab_data

# calculating the average alpha per group
calculated_alpha_plot_data$avg.alpha <- ave(calculated_reliability_lab_data$alpha, 
                                            calculated_reliability_lab_data$g)

# making replication success a Boolean, so we can order the dataset by it.
calculated_alpha_plot_data$replication_success <- ifelse(
  calculated_alpha_plot_data$replication_success == "Yes", TRUE, FALSE)

# reordering the plot from succesfully replicated to unsuccesfully replicated, and then from least to most reliable.
calculated_alpha_plot_data <- calculated_alpha_plot_data[order(-calculated_alpha_plot_data$replication_success, -calculated_alpha_plot_data$avg.alpha),]

# Removing calculated alpha's that fell below 0 to make everythign fit nicely in the plot.
calculated_alpha_plot_data <- calculated_alpha_plot_data[calculated_alpha_plot_data$alpha > 0,]

# reordering the grouping variable factor levels
calculated_alpha_plot_data$g <- fct_inorder(as.factor(calculated_alpha_plot_data$g), ordered = NA)


# adding an index to which row in the plot data the row in measure_reliability_data belongs to
measure_reliability_data$graph_index <- NA

for (i in 1:nrow(measure_reliability_data)){
  measure_reliability_data$graph_index[which(unique(calculated_alpha_plot_data$g)[i] == measure_reliability_data$g)] <- i
}

```

```{r PlotAlphaDistributions, warning = FALSE,  fig.cap = "Distributions of calculated Cronbach’s alpha coefficients calculated for the responses on a measure at each lab location, across the nineteen measures for which raw data was available from which Cronbach’s alpha coefficients could be calculated. Cronbach's alpha values that fell below 0 were excluded. The green lines indicate the meta-analytic prediction interval lower and upper bound. The blue triangles indicate the reported alpha coefficient for that measure from the original article, when reported. The Tau column besides the figure shows the tau heterogeneity estimate based on a meta-analysis of the calculated reliabilities for each measure. Meta-analyses for which the Q-test for heterogeneity was significant at alpha < .05 are in black, while non-significant results are in grey. The Diff column shows the difference between reported reliability and the average reliability calculated from the Many Labs data for the applicable measures. The reported reliabilities that fell outside the 95% quantile of calculated reliability scores are shown in black, otherwise in grey."}

# plot for distribution of alpha
ggplot(calculated_alpha_plot_data, aes(x = alpha, y = g)) +
  geom_boxplot(outlier.shape = NA) +
  geom_hline(yintercept = 6.5, color = "red", size = 1) +
  geom_point(alpha = 0.1) +
  
  # adding in the tau values
  geom_text(data = measure_reliability_data, label = format(measure_reliability_data$tau, digits = 1), x = 1.15, y = measure_reliability_data$graph_index, color = ifelse(measure_reliability_data$QEp < .05, "black", "grey"), size = 2.8) +
  
  # adding in the difference in alpha coefficients
  geom_text(data = measure_reliability_data[c(1:8, 18),], label = format(measure_reliability_data$coefficient_difference[c(1:8, 18)], digits = 2), x = 1.28, y = measure_reliability_data[!is.na(measure_reliability_data$coefficient_difference),]$graph_index, color = ifelse(measure_reliability_data$significance_reported_coefficient[c(1:8, 18)], "black", "grey"), size = 2.8) +
  
  # setting the theme
  theme_minimal() +
  theme(legend.position = "none", plot.margin = unit(c(1, 6.5, 1, 1), "lines")) +
  
  # adding the necessary indicative texts
  annotation_custom(grob = textGrob(label = "Not Replicated", hjust = 0, gp = gpar(fontsize = 10)), ymin = 7.25, ymax = 7.25, xmin = 0.01, xmax = 0.01) +
  annotation_custom(grob = textGrob(label = "Replicated", hjust = 0, gp = gpar(fontsize = 10)), ymin = 6, ymax = 6, xmin = 0.01, xmax = 0.01) +
  annotation_custom(grob = textGrob(label = "Tau", hjust = 0, gp = gpar(fontsize = 12)), ymin = 20.2, ymax = 20.2, xmin = 1.1, xmax = 1.1) +
  annotation_custom(grob = textGrob(label = "Diff", hjust = 0, gp = gpar(fontsize = 12)), ymin = 20.2, ymax = 20.2, xmin = 1.24, xmax = 1.24) +
  
  coord_cartesian(xlim = c(0, 1), clip = "off") +
  
  
  # adding the blue triangles for reported reliability and green prediction intervals
  geom_point(data = measure_reliability_data, mapping = aes(x = reported_coefficient, y = graph_index), color = "blue", shape = 17, size = 3) +
  # we remove 14 & 15 lower bound because they are below 0
  geom_point(data = measure_reliability_data[-c(14, 15),], mapping = aes(x = pi.lb, y = graph_index), color = "green", shape = 124, size = 2.5) + 
  geom_point(data = measure_reliability_data, mapping = aes(x = pi.ub, y = graph_index), color = "green", shape = 124, size = 2.5) + 
  
  ylab("") +
  xlab("Cronbach's alpha") 

```

We found statistically significant indication of heterogeneity in Cronbach's alpha for `r apa_num(sum(measure_reliability_data$QEp < .05), numerals = FALSE)` of the `r apa_num(nrow(measure_reliability_data), numerals = FALSE)` measures. However, as noted earlier a more accurate indication of the heterogeneity can be observed in the prediction intervals. The prediction intervals generally show larger indications of heterogeneity for some of the lower reliability measures and less for higher reliability measures when compared to the Tau test results. Number of labs also appears to be a factor for narrowing the prediction intervals. Additionally, we observe that for most measures the reported reliabilities were generally lower than the reliabilities calculated for the replication labs. Perhaps more interesting is that we observed that reported reliabilities in the original study were more common for those measures that had higher average reliabilities (\>.80) in the replications compared to on average less reliable measures.

Figure \@ref(fig:Plot1Factor) below show the result of our unidimensionality test. `r apa_num((sum(unidimensionality_graph_data$RMSEA < .08, na.rm = TRUE) / nrow(unidimensionality_graph_data)) * 100)` % of labs were below our RMSEA .08 threshold, while for `r apa_num((sum(unidimensionality_meta_data$beta < .08, na.rm = TRUE)), numerals = FALSE)` measures of the total `r apa_num(nrow(unidimensionality_meta_data), numerals = FALSE)` the mean across labs was below this threshold. Furthermore, the RMSEA shows considerable variation within measures, with the standard deviation ranging from `r apa_num(min(tapply(unidimensionality_graph_data$RMSEA, unidimensionality_graph_data$g, sd, na.rm = TRUE)[!is.na(tapply(unidimensionality_graph_data$RMSEA, unidimensionality_graph_data$g, sd, na.rm = TRUE))]))` as the lowest to all the way up to `r apa_num(max(tapply(unidimensionality_graph_data$RMSEA, unidimensionality_graph_data$g, sd, na.rm = TRUE)[!is.na(tapply(unidimensionality_graph_data$RMSEA, unidimensionality_graph_data$g, sd, na.rm = TRUE))]))`. The parallel analysis results similarly showed inconsistency both across and within measures. While `r apa_num((sum(unidimensionality_graph_data$N_factors_capped == "1", na.rm = TRUE) / nrow(unidimensionality_graph_data)) * 100)`% of labs returned a single factor solution, `r apa_num((sum(unidimensionality_graph_data$N_factors_capped == "2", na.rm = TRUE) / nrow(unidimensionality_graph_data)) * 100)`% returned a two-factor, and `r apa_num((sum(unidimensionality_graph_data$N_factors_capped == "2", na.rm = TRUE) / nrow(unidimensionality_graph_data)) * 100)`% returned three or more factors. The remainder did not converge. The lack of convergence was an also an issue for the CFA used for the RMSEA check. The CFA failed to converge for `r apa_num((sum(is.na(unidimensionality_graph_data$RMSEA)) / nrow(unidimensionality_graph_data)) * 100)`% of labs. In the case of the @defruytCloningersPsychobiologicalModel2000 measure all labs failed to converge. Convergence in the other cases was the result of either the severe misfit of our factor models or a lack of sample size to stably estimate these models. In the end, `r apa_num((sum(unidimensionality_graph_data$unidimensional == "Yes", na.rm = TRUE) / nrow(unidimensionality_graph_data)) * 100)`% of labs passed our unidimensionality check.

```{r PlotData1FactorRMSEA}
unidimensionality_graph_data <- data.frame(RMSEA = c(NA),
                                           RMSEA_se = c(NA),
                                           N_factors = NA,
                                           unidimensional = NA,
                                           g = NA)

measured_variables <- c("caruso_2012", "husnu_2010", "nosek_2002_math", "nosek_2002_art", "anderson_2012_swl", "anderson_2012_pa", "anderson_2012_na", "giessner_2007", "norenzayan_2002", "zhong_2006", "monin_2001_most", "monin_2001_some", "cacioppo_1983_arg", "cacioppo_1983_nfc", "de_fruyt_2000", "albarracin_2008_verb", "albarracin_2008_math", "shnabel_2008", "vohs_2008")

for (i in 1:19){

  g = measured_variables[i]
  
  unidimensionality_graph_data <- rbind(unidimensionality_graph_data, cbind(FA_list[[i]][1:4], g))

}

# removing placeholder first row
unidimensionality_graph_data <- unidimensionality_graph_data[-1, ]

# ordering the levels of g to match the order in the reliability graph
unidimensionality_graph_data$g <- factor(unidimensionality_graph_data$g, levels = c("nosek_2002_art", "nosek_2002_math", "shnabel_2008", "husnu_2010", "norenzayan_2002", "vohs_2008", "cacioppo_1983_arg", "anderson_2012_pa", "anderson_2012_na", "giessner_2007", "anderson_2012_swl", "caruso_2012", "zhong_2006", "monin_2001_some", "cacioppo_1983_nfc", "monin_2001_most", "albarracin_2008_verb", "albarracin_2008_math", "de_fruyt_2000"), labels = c("Nosek et al. (2002), Art", "Nosek et al. (2002), Math", "Shnabel & Nadler (2008)", "Husnu & Crisp (2010)", "Norenzayan et al. (2002)", "Vohs & Schooler (2008)", "Cacioppo et al. (1983), arg", "Anderson et al. (2012), PA", "Anderson et al. (2012), NA", "Giessner & Schubert, (2007)", "Anderson et al. (2012), SWL", "Caruso et al. (2012)", "Zhong & Lijenquist (2006)", "Monin & Miller (2001), some", "Cacioppo et al. (1983), nfc", "Monin & Miller (2001), most",  "Albarracín et al. (2008), exp 5 verb", "Albarracín et al. (2008), exp 5 math", "De Fruyt et al. (2000)"))

# adding a capped of version of the N_factors variable as a category variable
N_factors_capped <- rep(NA, nrow(unidimensionality_graph_data))

N_factors_capped[unidimensionality_graph_data$N_factors == 1] <- "1"
N_factors_capped[unidimensionality_graph_data$N_factors == 2] <- "2"
N_factors_capped[unidimensionality_graph_data$N_factors >= 3] <- "3+"

unidimensionality_graph_data$N_factors_capped <- as.factor(N_factors_capped)


### meta-analyzing
assess_undidimensionality_heterogeneity <- function(data_on_RMSEA){
  temp_beta <- NA
  temp_tau <- NA
  temp_QEp <- NA
  rma_prediction <- NA
  temp_pi.lb <- NA
  temp_pi.ub <- NA
  
  tryCatch({
    # run a random effects meta-analysis
    rma_model <- rma(yi = data_on_RMSEA$RMSEA, sei = data_on_RMSEA$RMSEA_se, 
                     method = "REML", control = list(stepadj = 0.5, maxiter = 1000))
    # estimated coefficient
    temp_beta <- rma_model$beta
    
    # extract the relevant heterogeneity information
    temp_tau <- sqrt(rma_model$tau2)
    temp_QEp <- rma_model$QEp
    
    # get prediction intervals for alpha
    rma_prediction <- predict(rma_model)
    temp_pi.lb <- rma_prediction$pi.lb
    temp_pi.ub <- rma_prediction$pi.ub

    
  }, error = function(e) {
    
  })
  
  
  return(c(temp_beta, temp_tau, temp_QEp, temp_pi.lb, temp_pi.ub, data_on_RMSEA$g[1]))
}


unidimensionality_meta_list <- tapply(unidimensionality_graph_data, unidimensionality_graph_data$g, assess_undidimensionality_heterogeneity)

# convert data output to a dataframe
unidimensionality_meta_data <- do.call(rbind.data.frame, unidimensionality_meta_list)

names(unidimensionality_meta_data) <- c("beta", "tau", "QEp", "pi.lb", "pi.ub")


# added the reporting or lack of reporting of psychometric validty evidence in the original to the meta data
unidimensionality_meta_data$reported_psych_val <- coded_data_original$sel_psychometric_evidence_REV[measure_reliability_data$coded_data_index] != "None" & coded_data_original$sel_psychometric_evidence_REV[measure_reliability_data$coded_data_index] != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)" | coded_data_original$sel_3_REV[measure_reliability_data$coded_data_index] == TRUE


# adding the number of non-coverged factor models
non_converged_list <- tapply(unidimensionality_graph_data$RMSEA, unidimensionality_graph_data$g, is.na)

non_converged_vec <- rep(0, 19)

for (i in 1:19){
  non_converged_vec[i] <- sum(non_converged_list[[i]])
}

```

```{r Plot1Factor, fig.cap = "Distributions of calculated RMSEA of a single-factor model fit calculated for the responses on a measure at each lab location, across the nineteen measures for which raw data was available on which a factor model could be fitted. The red horizontal line separate the replicated from the non-repicated measures. The red vertical line indicates our .08 RMSEA cutoff value. The N non-conv. column besides the graph shows the number of labs per measure for which the single-factor CFA did not converge. The color of each dot shows the number of factors that were selected for that measure for that lab location based on the parallel analyses. Psychometric validity evidence was reported in the original Husnu & Crips (2010) and Shnabel & Nadler (2008) studies."}

# plot for distribution of alpha
ggplot(unidimensionality_graph_data, aes(x = RMSEA, y = g, colour = as.factor(N_factors_capped))) +
  geom_boxplot(outlier.shape = NA, colour = "black") +
  geom_hline(yintercept = 6.5, color = "red", size = 1) +
  geom_vline(xintercept = 0.08, color = "red", size = 1) +
  geom_point(alpha = 0.3) +
  
  # adding in the tau values
  #geom_text(data = unidimensionality_meta_data, label = format(unidimensionality_meta_data$tau, digits = 1), x = 0.58, y = 1:19, color = ifelse(unidimensionality_meta_data$QEp < .05, "black", "grey"), size = 2.8) +
  
  # adding in the N non converted
  geom_text(data = unidimensionality_meta_data, label = format(non_converged_vec), x = 0.51, y = 1:19, size = 2.8, colour = "black") +
  
  # setting the theme
  theme_minimal() +
  theme(legend.position = "bottom", plot.margin = unit(c(1, 5, 1, 1), "lines")) +
  
  
  # adding the necessary indicative texts
  annotation_custom(grob = textGrob(label = "Not Replicated", hjust = 0, gp = gpar(fontsize = 10)), ymin = 7.25, ymax = 7.25, xmin = 0.33, xmax = 0.33) +
  annotation_custom(grob = textGrob(label = "Replicated", hjust = 0, gp = gpar(fontsize = 10)), ymin = 6, ymax = 6, xmin = 0.37, xmax = 0.37) +
  #annotation_custom(grob = textGrob(label = "Tau", hjust = 0, gp = gpar(fontsize = 12)), ymin = 20.2, ymax = 20.2, xmin = 0.55, xmax = 0.55) +
  annotation_custom(grob = textGrob(label = "N Non-Conv.", hjust = 0, gp = gpar(fontsize = 11)), ymin = 20.2, ymax = 20.2, xmin = 0.48, xmax = 0.48) +
  
  coord_cartesian(xlim = c(0, 0.45), clip = "off") +
  
  # we remove 7 & 15 lower bound because they are below 0
  # geom_point(data = unidimensionality_meta_data[c(1:6, 8:14, 16:17),], mapping = aes(x = pi.lb, y = c(1:6, 8:13, 15:17) ), color = "green", shape = 124, size = 2.5) + 
  # geom_point(data = unidimensionality_meta_data, mapping = aes(x = pi.ub, y = 1:19), color = "green", shape = 124, size = 2.5) + 
  
  scale_colour_manual(name = "N Factors (Parallel Analysis)", values = c("#0072B2", "#009E73", "#F0E442"), na.value = "#000000") + 
  guides(colour = guide_legend(override.aes = list(alpha = 1))) +
  ylab("") +
  xlab("RMSEA (unidimensional CFA model)") 
```

```{r check_on_Omega_convergence}
is_mostly_na <- function(x){
  return((sum(is.na(x)) / length(x)) > .5) 
}

Omega_convergence_vector <- tapply(calculated_alpha_plot_data$omega.tot, calculated_alpha_plot_data$g, is_mostly_na)

sum(Omega_convergence_vector)
```

Figure \@ref(fig:PlotOmegaDistributions) below shows the result of our Omega analyses. Note that while we show the distributions of omega values here, due to the lack of a formula to calculate the Omega standard error, we did not perform a meta-analysis on these results. Overall, our Omega coefficient analyses show a similar pattern between measures to that observed for the alpha coefficient albeit with generally higher coefficient scores across measures. However, not all improved. The two columns beside Figure \@ref(fig:PlotOmegaDistributions) show in how many labs each measure was used, as well as for how many labs an omega coefficient could be obtained, which was not always. In `r apa_num(sum(is.na(calculated_alpha_plot_data$omega.tot)) / nrow(calculated_alpha_plot_data) * 100)`% of labs, no omega coefficient could be obtained. For `r sum(Omega_convergence_vector)` measures the number of labs for which an Omega coefficient could not be obtained was even the majority. This relates to our findings from the unidimensionality tests, where some factor models did not converge.

```{r PlotOmegaDistributions, warning = false, fig.cap = "Distributions of calculated McDonald’s Omega t  calculated for the responses on a measure at each lab location, across the eighteen measures for which raw data was available from which McDonald’s Omega t coefficients could be calculated. The N.Omg column besides the figure shows the number of lab locations for which an Omega coefficient could be successfully calculated. Due to lack of underlying model convergence an Omega coefficient could not be calculated for all lab locations, or any lab locations for the Der Fruyt et al. (2000) measure. The N.tot column shows the total number of lab locations at which the measure was taken. Any difference between N.omg and N.tot can be seen as an indication of the instability to get a stable estimate of an underlying facto model for some lab locations for that measure."}
# plot for omega
ggplot(calculated_alpha_plot_data, aes(x = omega.tot, y = g)) +
  geom_boxplot(outlier.shape = NA) +
  geom_point(alpha = 0.1) + 
  
  # adding labels to show the number of labs per measure for which a total omega could be calculated
  geom_text(data = calculated_alpha_plot_data[1:19,], label = table(calculated_alpha_plot_data$g[!is.na(calculated_alpha_plot_data$omega.tot)]), x = 1.15, y = 1:19, size = 2.8) +
  # adding labels to show the number of labs per measure
  geom_text(data = calculated_alpha_plot_data[1:19,], label = table(calculated_alpha_plot_data$g), x = 1.35, y = 1:19, size = 2.8) +
  
  # theming
  theme_minimal() +
  theme(legend.position = "none", plot.margin = unit(c(1, 6.5, 1, 1), 
                                                     "lines")) +
  
  # adding the replication line
  geom_hline(yintercept = 6.5, color = "red", size = 1) +
  
  # adding the necessary indicative texts
  annotation_custom(grob = textGrob(label = "Not Replicated", hjust = 0, gp = gpar(fontsize = 10)), ymin = 7.25, ymax = 7.25, xmin = 0.01, xmax = 0.01) +
  annotation_custom(grob = textGrob(label = "Replicated", hjust = 0, gp = gpar(fontsize = 10)), ymin = 6, ymax = 6, xmin = 0.01, xmax = 0.01) +
  annotation_custom(grob = textGrob(label = "N.Omg", hjust = 0, gp = gpar(fontsize = 12)), ymin = 20.2, ymax = 20.2, xmin = 1.08, xmax = 1.08) +
  annotation_custom(grob = textGrob(label = "N.Tot", hjust = 0, gp = gpar(fontsize = 12)), ymin = 20.2, ymax = 20.2, xmin = 1.28, xmax = 1.28) +
  
  coord_cartesian(xlim = c(0, 1), clip = "off") +
  
  ylab("") +
  xlab("Omega")

```

## Questionable Measurement Practices

Reliability and validity indicators are only one aspect of measurement reporting. We coded additional potential questionable measurement practices in reporting in general too. Table \@ref(tab:QMPTable) below shows for both original and replication for all of the QMP items the ratio of QMPs and for how many of the `r nrow(coded_data_original)` measures the QMP was applicable. Meanwhile, the Phi coefficient indicates the “correlation” between the MP being questionable or good in the original and the same for the replication.

```{r QMPtable}
# create informative Measurement Practice Labels 
MPractice <- c("Reliability is reported", "It is clear if the measure existed or is newly created",    "Specifc version of the measure is specified", "Validity evidence from a factor analysis is presented",   "The measured variable is defined", "The choice of measure selection or creation is justified",    "The implemented operationalisation is justified", "The source of the measure is provided",    "Psychometric evidence from the study is given", "Psychometric evidence from an earlier study is given",    "The administration format and environment are described", "The administration procedure is described",   "The number of items are described", "The number of response options are described",    "The recoding of responses is described", "The creation of the index is described",    "Example items are presented in text or supplement", "Any administration format changes are mentioned",    "Administration format changes are justified/validated", "Any translations are mentioned",    "Translated measures are justified/validated", "Any changes in number of items or response options are mentioned",    "Changes in number of items or response options are justified/validated")   


# construct the QMP table to be printed 
QMP_table <- data.frame("Practice" = MPractice, 
                        "QMP original" = paste0(qmp_ratio_data$QMP_percentage_original, " (", qmp_ratio_data$N_applicable_original, ")"), 
                        "QMP Replication" = paste0(qmp_ratio_data$QMP_percentage_replication, " (", qmp_ratio_data$N_applicable_replication, ")"), 
                        "Phi" = qmp_ratio_data$Phi)   


# print the table in apa formatting 
apa_table(QMP_table, align = c("l", "r", "r", "r"), caption = "Ratio of QMPs and number of applicable measures, for original and replication", note = "The first number in QMP org and QMP rep is the ratio of measures for which the practice in that row was not conducted well by our standards. The number between brackets indicates the number of measures for which the practice was applicable.", escape = FALSE, placement = "htp")
```

The Phi coefficients reveal little in the way of a consistent pattern on the relation of QMPs in original to QMPs in replications. However, if we look more conceptually we can also look at the reporting of information most relevant for reconstructing the measure, as well as modifications to the measure to learn about the relation between original and replication study. It was clear that existing measures were used for `r apa_num(sum(coded_data_original$sel_existing == "True, namely:") / nrow(coded_data_replications) * 100)`% original and `r sum(coded_data_replications$sel_existing == "True, namely:") / nrow(coded_data_original) * 100`% replication measures. It was not always clear which measures were used (`r sum(coded_data_original$sel_existing == "Not Clearly Stated") / nrow(coded_data_original) * 100`% original; `r sum(coded_data_replications$sel_existing == "Not Clearly Stated") / nrow(coded_data_replications) * 100`% replication), meaning they could not clearly be identified to be the same between original and replication. While the number of items (`r qmp_ratio_data$QMP_percentage_original[13]`% original; `r qmp_ratio_data$QMP_percentage_replication[13]`% replications) and response (`r qmp_ratio_data$QMP_percentage_original[14]`% original; `r qmp_ratio_data$QMP_percentage_replication[14]`% replications) were usually clear, and modifications were atypical, but did occur (`r qmp_ratio_data$QMP_percentage_original[22]`% original; `r qmp_ratio_data$QMP_percentage_replication[22]`% replications). It was however uncommon for this modification to be validated (`r qmp_ratio_data$QMP_percentage_original[23]`% original; `r qmp_ratio_data$QMP_percentage_replication[23]`% replication). QMPs were less common for language (`r qmp_ratio_data$QMP_percentage_original[20]`% original; `r qmp_ratio_data$QMP_percentage_replication[20]`% replications) and administration format modifications (`r qmp_ratio_data$QMP_percentage_original[18]`% original; `r qmp_ratio_data$QMP_percentage_replication[18]`% replications), . Validation was however still rare (language: `r qmp_ratio_data$QMP_percentage_original[21]`% original, `r qmp_ratio_data$QMP_percentage_replication[21]`% replications; administration format: `r qmp_ratio_data$QMP_percentage_original[19]`% original; `r qmp_ratio_data$QMP_percentage_replication[19]`% replications). While generally QMPs were not found for admin format and procedure in the replications (format: `r qmp_ratio_data$QMP_percentage_replication[11]`%; procedure: `r qmp_ratio_data$QMP_percentage_replication[12]`%), they did occur for original measures (format: `r qmp_ratio_data$QMP_percentage_original[11]`%; procedure: `r qmp_ratio_data$QMP_percentage_original[12]`%). In general, we observed that measurement was modified in some way from original to replication for `r sum(coded_data_replications$mod_check == "True") / nrow(coded_data_replications)`% of measures, and it was unclear whether modification occured for `r sum(coded_data_replications$mod_check == "None Reported") / nrow(coded_data_replications)`%.

# Discussion

To assess psychological phenomena we need measurement that is valid and reliable for our constructs. However, we noted first is that little reliability and validity evidence was reported. Second, our reanalysis of the reliability and validity of the measures showed inconsistent reliability and validity support. Finally, we observed a lack of transparent reporting, which makes it difficult for future researchers to reuse and build upon the validation of existing measurements.

In line with existing research we found that for the majority of the measures in original and replication studies no reliability coefficient was reported, and psychometric validity evidence was even less common [@beckmanHowReliableAre2004; @barryValidityReliabilityReporting2014; @flakeConstructValidationSocial2017; @maireadshawMeasurementPracticesLargescale2020; @flakeConstructValidityValidity2022]. Some part of this is due to around half of the measures in original and replication studies being single item measures. While it is possible to check the reliability and validity of single item measures, these methods often require multiple measures or studies to be able to cross-validate the measure with [@leppinkWeNeedMore2017; @sarstedtSelectingSingleItems2016]. Another factor may be observed in the fact that we observed that replications generally reported less reliability and psychometric validity evidence. This could be because replications implicitly defer the reporting of validity and reliability evidence to the original study. However, it was not uncommon for the replication to use different measures than the original study, and if it was not set in stone that the same measure as the original was used, then it stands to reason that the choice of measure should be motivated based on reliability and validity evidence. Furthermore, this does not explain the lack of reliability and validity reporting in the original studies. It may unfortunately be more reasonable to assume that many of these measures are just not validated before (or even after) use.

This is not merely based on us guessing, when we recalculated the reliability coefficients and factor structure of the measures used in the replication projects, the observed reliability and validity of several measures did not convincingly show that these would be valid if presented alongside the measure in a study. While few measures were consistently not reliable and valid and some were even consistently reliable and valid, many were reliable and valid in a portion of the labs. We also noted that some measures had convergence issues, where for some labs we could not get the factor model nor Omega coefficient results. This may have been because of sample size issues, or the small number of items common in the replication measures, but gross misfit is also possible. Although, the psychometric indicators we used were applied rather broadly across a spectrum of different measures, and should not be used to make individual inferences for the measures. As a general overview, our results show that reliability and validity for these measures could be poor.

We also observed at when measures which came with a reported Cronbach's Alpha in the original study, their average Cronbach's Alpha observed among the replications were all relatively high. This aligns with existing research indicating bias in reported Cronbach's Alpha values, with an excess of reported values at the common acceptably reliable threshold value of .70, and low reporting just below .70 [@husseyAberrantAbundanceCronbachs2023]. Thus authors may be less inclined to report reliabilities that fall below an acceptable threshold. In combination with our inconsistent reliability and validity evidence this bias in reporting could create a false sense of security when looking only. It is possible that reliability and validity are only reported when they are sufficient, and not otherwise.

After all, there is little incentive for researchers to report this information if they don't want to. Reporting standards for measurement reporting are not broadly applied, and we observed this also when we looked at the information needed to reconstruct a measure for reuse. Similar to @flakeConstructValidityValidity2022, we observed that QMPs were not infrequent. In particular, like @flakeConstructValidityValidity2022, we observed that original research underreported information needed to reconstruct the measurement In original research the administration format and procedure for each was not properly described for around a third of the applicable measures, the number of items and response options for each around an eight, and how the index was calculated, how the items were recoded and example items were presented for each a little under half. We also found that about two-thirds of the measurements were modified between original and replication, and specific modifications were rarely validated or justified. This raises some questions around the label direct replication. For example, the conscientiousness measure used for the replication of @defruytCloningersPsychobiologicalModel2000 was reduced from original NEO-PI-R 30-item measure to a two-item measure. It is highly unlikely that a measure with such a significant change would have a comparable validity and reliability to the original, and we did unsurprisingly observe convergence issues with validity as well as low reliability for this measure.

One notable difference to @flakeConstructValidityValidity2022 is that replications did on average have less QMPs than original research. We believe that this is in part due to the structured way in which the Many Labs protocols were written up. Several Many Labs protocols even contained specific sections to declare any deviations from the original methodology. The other reason may be due to the more lenient coding of the QMPs in the revised protocol. We mostly checked that something relevant to a specific QMP item was reported, not whether that reporting thoroughly shared all the needed information. Regardless, any lack of information needed to recreate the measurement makes it harder for follow up research to validate the measure.

Our goal with these findings is not to specifically scold the Many Labs replications nor the original studies. We commend the tremendous effort involved in the creation of the Many Labs projects, and we understand that for feasibility some corners had to be cut. Similarly, we understand that proper validation of psychological measures is not a responsibility that should be placed on any one original study. However, we would argue that full transparent measurement reporting is a responsibility both replications and original studies do carry. Furthermore, regardless of difficulty in validating measures, its necessity cannot be ignored if substantive interpretations based on the measurement data are desired.

## Limitations & Future Research

We focused primarily on a unidimensionality test from a factor analysis as our indicator of validity as recalculated from the data. However, our factor analyses were not informed by theory specific to each measure, and were instead applied the same across the board. It is quite possible that a measure fails at our checks and is still a valid and reliable measure to use in its specific use case. We consider these checks to be indicators of the overall construct validity in our sample. The checks should not be interpreted as concrete tests of measurement fit, nor should they be interpreted at the individual measurement level. Future research may want to deep dive into this specific topic and focus on doing theory informed factor analysis tests on measurement data.

One of the most impactful limitations was that we were unable to construct tests to compare the measurement properties between original and replication study, and replicated and non-replicated studies. This was in large part because there were simply to few measures with the relevant information. we planned to test. We initially planned to test the significance of the difference in reported reliability between original and replication studies, and the relation between reliability and replication outcome. But due to the small number of reported reliabilities sand measures for which reliability could be calculated we were unable to perform these tests. However, this limitation is also one of our most important findings. While we were unable to perform our pre-registered tests, the fact that we couldn't, is telling regarding the lack of standards for reported measurement information in our sample.

As another limitation, it is important to reflect on whether or not replication protocols and research articles can be fairly compared in terms of Measurement Practices. A study’s description within a replication protocol is generally shorter than an article. The protocol may therefore lack the space needed to report on the measurement in full detail. There are three reasons why we believe protocols and articles remain mostly comparable. Firstly, articles are often also restricted in the amount of space they have available to devote to measurement [@gardinerEditorialMethodsPapers2019; @zogmaisterAssessingTransparencyMethods2024]. Second, in the revised protocol some QMP items, such as whether example items were reported or not, also allowed for reporting this information in supplementary materials to count as good practice. Third, there are little to no other files outside the protocol and supplements where the measurement details for the replication could be found, as such the protocol and supplements represent the total available information on the measurement just like the original article does. A difference that was not resolved between articles and protocols was that the protocols were written before the measurement was conducted. Therefore we can not fairly compare protocol and replication on measurement information that is derived from the data – such as reliability coefficients and psychometric validity information. Still, future research should look if possible at replications that report on measurement information derived from the replication data.

There were additional sources of information we could have included. Data from original studies could have been used to recalculate reliability and validity indicators from to compare with the replication results. However, it is unlikely that a substantial number would have shared their data (many were conducted before the OSF and other Open Science initiatives were launched). This is likely different for more recent research [@hardwickeEstimatingPrevalenceTransparency2022; @hamiltonPrevalencePredictorsData2023a]. Including replications of recent research could help resolve some of the sample size issues for our tests. Future research may wish to look at replications of more recent research to compare recalculated measurement information between original and replication. reliabilities and validity.

## Recommendations

The two main issues we believe that should be addressed are: the prevalence of using non-validated measurement, and the lack of transparency surrounding measurement reporting. Unfortunately, there are no easy solutions to fix these issues. They will require significant efforts involving multiple members of the scientific community to bring about significant change. However, the positives are that these issues are interrelated, in that solving transparency will benefit greater validation of measures.

Thus, first we recommend that reporting standards for measurement deserve a more central place. We found that reporting reliability and validity information was the exception rather than the rule, and even basic information such as how many items were in the measure was not always present. The @americaneducationalresearchassociationStandardsEducationalPsychological2014 guidelines exist to guide researchers in the creation and validation of scales, but are not intended as general measurement reporting guidelines. Similarly, the APA guidelines (???CITE???) only briefly address reliability and validity information but say little on other measurement reporting. While statistical reporting standards and guidelines have proliferated, even resulting in official recommendations from the ASA [@wassersteinASAStatementPValues2016], a high-profile push for measurement reporting standards is lacking. We believe that measurement reporting standards deserve to be highlighted as much as statistical result reporting,. Even well reported statistics lack substantial interpretability without valid measurement, while a well reported measurement can be informative to future research on its own.

Transparent measurement reporting also has the advantage that it enables future researchers to recreate the original measure. This point is critical to fueling our second recommendation: as a community we need to systematically evaluate and revise our existing measures. First, existing measures need to be reusable. Then researchers need to reuse existing measures. The obtained reliability and validity of the measure then need to be compared across multiple occasions, and finally a value judgement to determine in what contexts, if any, the measure is valid. Transparent reporting takes care of the first step. For the second step, @elsonPsychologicalMeasuresArent2023 have suggested journals to implement the Standardisation Of BEhavior Research (SOBER) guidelines to specifically address issues of flexibility and norming in measurement. Furthermore, they propose an open repository of measurement protocols to facilitate the discovery of measures and building an evidence base. For the third and fourth step, the scientific community needs to analyse the validity and reliability of all uses of the measure such that they can be fairly compared to each other. For this it would be best if the original data of the item responses and demographics is shared for each use of the measure. This way the measure's validity and reliability can be recomputed similarly to what was done in our analyses, but with a closer link to theory. Otherwise, the summary statistics such as the variance-covariance structure, means, and standard deviations should be shared as these are sufficient for several factor analysis models.

Furthermore, researchers should report and evaluate our measures using more informative indicators. Cronbach's Alpha on its own gives only limited information the quality of a scale, and comes with strong assumptions [@cortinaWhatCoefficientAlpha1993; @sijtsmaUseMisuseVery2009]. Factor analytical evidence and McDonald's Omega are more informative indicators that should be presented in favor of or in tandem with Cronbach's Alpha.

Finally, we argue that researchers seeking to replicate a study should first evaluate the measurement of that study. Not only is it crucial for an informative replication that the measurement is reliable or valid. The original study should also report the measurement details necessary to reconstruct the original measurement. Otherwise it will be challenging to know if measurement was different between original and replication. If so, we suggest that researchers instead use their resources to conduct a replication of a study with reliable, valid and well-documented measurement. When replicating another study is not an option, we advise the replicating researcher to first attempt a conceptual replication using a validated measurement. Afterwards, a direct replication can be performed based on the conceptual replication to further assess the robustness.

# Conclusion

Cumulative knowledge on psychological phenomena starts with our ability to accurately capture the relevant constructs of interest. Based on the results from our sample and the findings of existing research, the validity and reliability of psychological measurements is rarely reported and based on our data cannot be assumed to be generally correct. The lack of transparent reporting on the measurement further complicates evaluating and reusing the existing measures. A change will be necessary. Otherwise, the basic premise that the measurement measures the variable it is intended to measure, will remain unsubstantiated for many psychological studies. Fortunately, even small steps towards improved reporting practices and single instances in cumulative measurement validation can already help proliferate validated measurement.

## Conflicts of Interest

The author(s) declare that there were no conflicts of interest with respect to the authorship or the publication of this article.

## ORCID iDs

Cas Goos <https://orcid.org/0009-0005-3792-4148>

Marjan Bakker <https://orcid.org/0000-0001-9024-337X>

Jelte M. Wicherts <https://orcid.org/0000-0003-2415-2933>

Michèle B. Nuijten <https://orcid.org/0000-0002-1468-8585>

## Funding

The preparation of this article was supported by the Veni grant VI.Veni.201G.003 awarded to Michèle Nuijten, and the Vici grant VI.C.221.100 awarded to Jelte M. Wicherts from the Dutch Research Council (NWO).

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
