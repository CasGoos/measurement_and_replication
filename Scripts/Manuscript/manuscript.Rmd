---
title             : "The Impact of Measurement Practices and Measurement Error on Construct Validity and Replication Outcomes in Psychological Science"
shorttitle        : "Measurement Quality and Replicability"

author: 
  - name          : "Goos, C."
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Professor Cobbenhagenlaan 125, 5037 DB, Tilburg, The Netherlands"
    email         : "casgoos99@gmail.com"
    role: # Contributorship roles (e.g., CRediT, https://credit.niso.org/)
      - "Conceptualization"
      - "Data curation"
      - "Formal Analysis"
      - "Investigation"
      - "Methodology"
      - "Visualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"
  - name          : "Nuijten, M.B."
    affiliation   : "1"
    role:
      - "Project Administration"
      - "Writing - Review & Editing"
      - "Supervision"

affiliation:
  - id            : "1"
    institution   : "Tilburg University"

authornote: |
  1 Department of Methodology and Statistics, Tilburg School of Social and Behavioral Sciences, Tilburg University, Tilburg, NL. 

  Enter author note here. Caspar/Jelte/Marjan?

abstract: |
  Recently, the influence of measurement related issues on replicability has received more investigations. 
  In particular, studies have indicated that low reliability, and poor measurement reporting can negatively influence the replicability of an effect. 
  Building on this new line of research, this study assessed these influences in large scale replication projects. 
  Protocols from the Many Labs replication projects, and the original articles that the replications were based on were used to assess the  reporting of the measures, while publicly available data from the Many Labs replications was used to assess the reliability of the measures. Reliability did not relate to replicability, and was generally consistent between labs for each measure in the Many Labs projects. 
  The quality of measurement reporting in replication protocols did however relate to replicability.
  Additionally, tentative evidence was found for a relation between quality of measurement reporting in the protocol of a replication and the quality of measurement reporting in its related original study.
  Recommendations are given for researchers and replicators in order to improve measurement practices in replication, and ward its negative impact on replicability. 
  Additionally, directions are given for future research investigating the relationship between measurement issues and replicability.


keywords          : "reliability, measurement, measurement reporting, replicability, construct validity"
wordcount         : "194"

bibliography      : ["r-references.bib", "references.bib"]

floatsintext      : yes
linenumbers       : no
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : "man"
output            : papaja::apa6_pdf
knit              : worcs::cite_all
bibliography: references.bib
---

```{r setup, include = FALSE}
# loading R libraries
library(papaja)
library(worcs)
library(tidyr)
library(betareg)
library(lmerTest)
library(psych)
library(forcats)
library(GPArotation)
library(ggplot2)
library(ggridges)
library(GGally)
library(ggforce)
library(grid)
library(gridExtra)
library(metafor)

# loading source script
source(file = "C:/Users/u686785/OneDrive - Tilburg University/Documents/uvt/PHD/Master Thesis to Paper/Reproducible_R_Project/Scripts/source_script.R")

# Code below loads the processed data. The raw data was prepared for analysis in 'prepare_data.R.
load_data() 

# creates a reference list for all used R packages and the installed R version (does not automatically include Rstudio)
r_refs("r-references.bib")

# manually retrieving the Rstudio version
rstudioapi::versionInfo()
```

<!-- altering latex defaults to get better figure and table placement -->

\renewcommand{\arraystretch}{0.7}

<!-- reducing the line spacing within tables -->

\renewcommand{\topfraction}{.85}

<!-- max fraction of page for floats at top -->

\renewcommand{\bottomfraction}{.7}

<!-- max fraction of page for floats at bottom -->

\renewcommand{\textfraction}{.15}

<!-- min fraction of page for text -->

\renewcommand{\floatpagefraction}{.66}

<!-- min fraction of page that should have floats -->

\setcounter{topnumber}{3} <!-- max number of floats at top of page --> \setcounter{bottomnumber}{3} <!-- max number of floats at bottom of page --> \setcounter{totalnumber}{4} <!-- max number of floats on a page --> <!-- remember to use [htp] or [htpb] for placement -->

```{r analysis-preferences}
# Seed for random number generation
set.seed(17042023)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

# Introduction

For many years, methodologists and statisticians have raised concerns about the way that research practices [@cumming2014new; @wicherts2016degrees], reporting practices [@bakker2011mis; @nuijten2016prevalence], and publication decisions [@sterling1959publication; @bakker2012rules; @giner2012science] may undermine the credibility of psychology's scientific claims. Replicability of a finding is seen as one crucial step in establishing its credibility [@nosek2022replicability]. In order to investigate the replicability of psychological science, the large scale Reproducibility Project: Psychology (RPP) was set-up [@osc2015estimating]. Replications of 100 published studies were performed using similar materials, procedures, and analyses, while using a new sample of participants for each study. They found however, that in many instances the results did no match those of the original studies. This lack of replicability of psychological findings illustrated that psychology was facing a so-called 'replication crisis' [@hughes2018psychology; @giner2019crisis].

As a response, the demand to verify the credibility of psychological findings through replications rose. In order to attempt replications of various psychological findings, numerous large scale projects were set-up [@klein2014investigating; @camerer2016evaluating; @camerer2018evaluating; @ebersole2016many; @ebersole2020many; @klein2018many; @klein2022many]. Among these were the Many Labs projects, which operated similarly to the RPP, with the additional feature that each original article was replicated multiple times across different labs in various locations around the world. The results from these replication projects generally showed smaller effects than those reported in the original studies. Similarly, effects that had reached statistical significance in the original no longer did across the aggregate results of its replications.

Several explanations for this discrepancy between original and replication findings have been suggested and investigated, with many being centred around the concept of Questionable Research Practices [QRPs, @john2012measuring]. QRPs are practices, which either misrepresent or omit essential methodological information that is necessary for the evaluation of the research. Without this information, a reader will not know whether the result was in line with the author's expectation or simply the result of repeated trial and error until a suitable result was found [@simmons2011false]. Consequently, much of the discourse and the subsequent proposed solutions have been centred around preventing these QRPs through increased transparency in scientific reporting.

However, many researchers have also raised concerns that other factors, such as low power [@stanley2018what], lack of strong theoretical foundations [@eronen2021theory], and validity [@finkel2017replicability], which have not been given as much attention, are just as if not more responsible for the lack of successful replications found in psychology. Another factor which has gained more traction recently are measurement related issues' impact on replicability. In particular, studies have focused on the effects of issues such as measurement error and questionable measurement practices on replicability [@stanley2014expectations; @shaw2020measurement; @flake2017construct; @flake2020measurement].

Measurement in psychological science is defined in large part by one overarching issue: the psychological constructs of interest, such as affective state and intelligence, cannot be measured directly. As a result, psychologists face two primary challenges in the use of measurement: First of all, the measure has to assess the construct it intends to measure, even though it can only do so indirectly. Secondly, because the measurement is indirect, it also comes with some degree of error surrounding its assessment of the construct. The first issue relates to the concept of validity, and specifically construct validity. Validity refers to the overall extent to which a measure measures what it is supposed to [@borsboom2004concept], while construct validity specifically refers to the substantial relation between the item scores on a psychological measure and the psychological construct it intends to measure. In order for indirect assessment of psychological constructs to be a viable approach, the measure needs to have construct validity [@cook2002experimental]. The second issue relates to the concept of measurement error and reliability. Measurement error indicates the amount of variation in item scores that is the result of the used measures inaccuracies, and not due to differences in the underlying psychological construct of interest.

@stanley2014expectations demonstrated, using a simulation study, the impact measurement error has on attempts to verify or falsify findings. Scalar item data was generated based on levels of sampling error and score-reliability, as the inverse of measurement error, that mimicked what is standard in psychological research, to ensure that the resulting degree of error was representative. The results indicated that measurement error caused observed effects found in replication research to vary heavily. 

Even when ignoring reliability, a measurement's content and implementation still need to validly capture what it intends to measure. Questionable Measurement Practices (QMPs) are practices that decrease the information that is necessary to evaluate a measurement's validity, and have been coined as a conjugate term to QRPs. They range from lack of transparency and unclear motivation in choice of measure, to poor justification for modifications of measure and procedure of any sourced measures [@flake2020measurement]. When this information is missing, it becomes difficult to establish that original and replication are measuring the same construct(s), which has detrimental consequences for direct replication attempts.

This study aims to investigate the influence of both measurement error and QMPs on replicability, using data and reports from the large scale Many Labs replication projects. One goal is to expand the research by [@stanley2014expectations] with a practical example. Whereas they obtained their findings using a simulation study, this study investigated the scope of their findings in practice. QMPs in the context of replication research were already investigated by [@flake2022construct]. However, the investigations remained mostly descriptive. This study additionally aims to expand upon the findings of [@flake2022construct] through the use of a new set of replication data, and by exploratively investigating the relation between QMP and replicability.

## Measurement Error

???connection/throughline of the measurement error hypotheses and other (including measurement error) hypotheses –\> is there a way to rewrite this part of the article to fit better as support/precedence to the QMP stuff???? –\> risky use of measurement!?!

As a first step the state of measurement error in original and replication research in psychology is investigated, to assess the scope of @stanley2014expectations's findings in practice:

-   RQ1a. What is the degree of score reliability in replications of psychological research?

-   RQ1b. What is the degree of score reliability in original psychological research?

Since direct replications intend to investigate effects in nearly identical scenarios with nearly identical measures as the original study, it is expected that measurement errors are as present in the replication research looked at in this study as in the accompanying original psychological research.

-   H1. No difference is present in the degree of score reliability between replication research and original psychological research.

While reliability estimates are generally considered a measure specific feature, this is false. They are context dependent, and as a result the estimates of an effect can differ across contexts [@cho2015cronbach; @pauly2018resampling]. With large enough differences across contexts, the effect estimates between different replication attempts could deviate from each other beyond direct comparison. To investigate this potential problem, the variation in score reliability is assessed for each measure when used across a set of replications of the same effect in contexts.

-   RQ2. To what degree do reliability estimates differ within a replication set of the same original study?

Even though reliability estimates are context dependent, sufficient protocol structure can counteract the variance in reliability estimates. An example of the standardizing impact a structured protocol can have on reducing variability within an effect can be found in investigations into the closely related issue of heterogeneity [@patacchini2007unobserved]. Heterogeneity refers to the true variation in the true effect size due to minor but key differences in context, environment, and procedure. Heterogeneity is yet another proposed explanation for failed replications. This concern even prompted a Many Labs centered around the testing of heterogeneity in replications [@klein2018many]. However analyses from Many Labs 2, and @olsson2020heterogeneity showed little empirical evidence for widespread heterogeneity in effects among replications. These findings demonstrate that it is possible to reduce variability as the result of context using structured protocols, as was done in the Many Labs projects this study analyzed.

-   H2. There is no significant variation in the reliability estimates of replications of the same original study.

This study investigated whether there is something in the measures used for replicated findings that sets them apart from those used for non-replicated findings. One of the elements this study looked at is the reliability of replicated studies compared to the non-replicated studies, thus extending the research by @stanley2014expectations on the relationship between reliability and replicability with data from real life replication projects.

-   RQ3. What is the association between replication study score reliability and replication outcome?

Greater score-reliability means greater consistency in estimation as variance around the estimate of the true effect is decreased. Consequently measures across different occasions, including between original and replication, should also become more consistent [@nunnaly1994assessment]. When original and replication research share consistent result, and as long as the studied effect is true, then on average their statistical conclusions should converge.

-   H3. Score reliability in replication research is positively associated with successful replication of an original finding.

## Questionable Measurement Practices
A measure not only needs to be reliable, it should also in its design and implementation adequately capture what it intends to measure [@flake2020measurement]. Any scientific report should therefore clearly motivate the use of measure and procedure. Without clear reporting on these aspects, the validity of measurement will remain uncertain. Therefore, the other tests of this paper focused on the reporting aspects of measurement, assessed through QMPs.

This study sought first of all to conceptually replicate some of the findings of @flake2022construct about QMP reporting in the RPP within the Many Labs replication projects.

-   RQ4a. To what degree are QMPs present in replications of psychological research?
-   RQ4b. To what degree are QMPs present in original psychological research?

@flake2022construct coded the presence of a series of QMPs in the replication report in both the replication protocols of each replication in the RPP, as well as the original articles the replications were based on. They found that not only are QMPs common in psychological research, they can be even more common in replications reports. However, the reporting of the Many Labs projects differ from the RPP data used in @flake2022construct. The structured and preregistered protocols used for the Many Labs projects were hypothesized to lead to consistent reporting of important measurement details, and as a result contain less QMPs.

-   H4. QMPs are expected to be more frequent in original psychological research than in replication research.

QMPs may obscure relevant information needed to evaluate the use of measurement in a study. If the replication used the measure differently from the original, the replication may no longer be a direct replication. As a result, there is additional deviation in the estimate of the effect from the original, due to differences in measurement. Establishing a link between QMPs and non-replicability should confirm suspicions of their detrimental effects on replications [@flake2017construct; @flake2020measurement; @flake2022construct].

-   RQ5. What is the association between QMPs in replication studies and replication outcome?

Fewer QMPs means that measurement decisions in a report are transparent with the number arbitrary methodological decisions being limited as well. Fewer arbitrary decisions means less ways that a replicator can deviate from the original's intended effect. One of the Many Labs projects even touched on this concept [@ebersole2020many]. In this project, peer-review of replication protocols by original authors was used to ensure that approved methods were used. Additionally, it has been observed that authors who promote greater transparency in their work, have on average stronger evidence for their hypotheses [@wicherts2011willingness]. Finally, the results shown in @protzko2020high illustrate that when original studies are transparent, preregistered, and have adequate sample size, subsequent replication attempts find nearly the same effect size as the original. All of these findings point towards a positive relation between successful replication and various types of good research practices. A similar relation is expected for good measurement practices.

-   H5. QMPs in replication research are negatively associated with replication of an original finding.

This study additionally aims to see if the relation between QMPs and replicability can be traced back to original article QMPs. When essential information on a measure is not available in an original article, it may be difficult to know the specifics of that measure to report for a replication. Therefore this study also investigates the relation between original QMPs and replication QMPs.

-   RQ6. What is the association between QMPs in original research and QMPs in replications of psychological research?

If the relationship exists, it is expected that questionable measurement practices in an original paper might cause a spill-over effect of QMPs into the replication. @flake2022construct found that QMPs linked to issues for replicators in recreating the measurement to closely match the measurement in the original. As a result, there was a lack of measurement related information in both replication reports, and original articles. Additionally, investigations by @shaw2020measurement have shown that more validity evidence in an original article is associated with better psychometric properties in the replication sample. Based on these arguments, QMPs are expected to be associated across original and replication studies.

-   H6. Total number of QMPs in original psychological research is positively related to total number of QMPs in replication research.

# Method

Data collection, coding protocol, and planned analyses were all preregistered. The preregistration and supplementary materials can be found on this article's OSF page: <https://osf.io/9r8yt/>. Any deviations from the preregistration are explicitly mentioned in the text. This manuscript was created in Rstudio [@R-Rstudio] with R Version `r paste0(R.Version()$major, ".", R.Version()$minor)` [@R-base], and generated using the Workflow for Open Reproducible Code in Science [WORCS version 0.1.1, @vanlissaWORCS2021] to ensure reproducibility and transparency. All code and data used to generate this manuscript and its results are available at: <https://github.com/CasGoos/measurement_and_replication.git>.

## Sample

The data used for the analyses in this study consists of three main sources: replication datasets, preregistered replication protocols, and original study articles. The data came from the Many Labs series of studies. In particular, data from Many Labs 1, 2, 3, & 5 was used [@klein2014investigating; @klein2018many; @ebersole2016many; @ebersole2020many]. Data from Many Labs 4 [@klein2022many] was not part of the sample, as there was no publicly available protocol which contains the information needed to code QMPs. Additionally, the replication of [@crosby2008we] in Many Labs 5 made use of videos and eye-tracking measures, which did not match this study's focus on scale and item measures.

The total number of identified psychological findings for which replication was attempted across the four included Many Labs projects amounts to a total of `r sum(table(coded_data_replications$many_labs_version, coded_data_replications$appearance)[,1])`. Each finding was tested across several labs, and was related to a finding from an original study, the combination of all these studies and their associated information formed a replication set. For each research finding, any primary measures assessing a unique variable were taken to form the key unit of analysis. Here primary measures refers to measures assessing variables included in the primary test of the replication. This was done to ensure that the analyses could link directly to the replicability of a study. Acquiescence bias checks, manipulation checks, measures used for pilot testing, and measures added for exploratory analyses in replication studies were not included. The result was a total sample size of `r nrow(coded_data_replications)` measures used in both original and replication research [^1]. Table BELOW shows further descriptive information about the data for each Many Labs Project.

[^1]: Initially the original articles were coded to have 3 more measures than the replication protocols. However, these measures were all related to the moral foundations questionnaire, and could be altered from representing five different moral foundations to the two overarching categories of individualizing and binding foundations found described in the replication protocol without much issue or loss of information.

```{r ManyLabsDescriptivesTable, warning = FALSE}
# construct the Many Labs descriptives table to be printed
many_labs_descriptives_table <- data.frame("Many Labs Version" = c("1", "2", "3", "5", "Total"),
                                      "N ML Projects" = c(table(coded_data_replications$many_labs_version, coded_data_replications$appearance)[,1], sum(table(coded_data_replications$many_labs_version, coded_data_replications$appearance)[,1])),
                                      "N Primary Measures" = c(table(coded_data_replications$many_labs_version), nrow(coded_data_replications)), 
                                      "Replication Total N" = c(apa_num(mean(coded_data_replications$N[coded_data_replications$many_labs_version == "1"], na.rm = TRUE), digits = 0),
apa_num(mean(coded_data_replications$N[coded_data_replications$many_labs_version == "2"], na.rm = TRUE), digits = 0),
apa_num(mean(coded_data_replications$N[coded_data_replications$many_labs_version == "3"], na.rm = TRUE), digits = 0),
apa_num(mean(coded_data_replications$N[coded_data_replications$many_labs_version == "5"], na.rm = TRUE), digits = 0), apa_num(mean(coded_data_replications$N, na.rm = TRUE), digits = 0)), 
"Replication Total IQR" = c(apa_num(IQR(coded_data_replications$N[coded_data_replications$many_labs_version == "1"], na.rm = TRUE), digits = 0),
apa_num(IQR(coded_data_replications$N[coded_data_replications$many_labs_version == "2"], na.rm = TRUE), digits = 0),
apa_num(IQR(coded_data_replications$N[coded_data_replications$many_labs_version == "3"], na.rm = TRUE), digits = 0),
apa_num(IQR(coded_data_replications$N[coded_data_replications$many_labs_version == "5"], na.rm = TRUE), digits = 0), apa_num(IQR(coded_data_replications$N, na.rm = TRUE), digits = 0)),
"Original Total N" = c(apa_num(mean(coded_data_original_shortened$N[coded_data_original_shortened$many_labs_version == "1"], na.rm = TRUE), digits = 0),
apa_num(mean(coded_data_original_shortened$N[coded_data_original_shortened$many_labs_version == "2"], na.rm = TRUE), digits = 0),
apa_num(mean(coded_data_original_shortened$N[coded_data_original_shortened$many_labs_version == "3"], na.rm = TRUE), digits = 0),
apa_num(mean(coded_data_original_shortened$N[coded_data_original_shortened$many_labs_version == "5"], na.rm = TRUE), digits = 0), apa_num(mean(coded_data_original_shortened$N, na.rm = TRUE), digits = 0)),
"Original Total IQR" = c(apa_num(IQR(coded_data_original_shortened$N[coded_data_original_shortened$many_labs_version == "1"], na.rm = TRUE), digits = 0),
apa_num(IQR(coded_data_original_shortened$N[coded_data_original_shortened$many_labs_version == "2"], na.rm = TRUE), digits = 0),
apa_num(IQR(coded_data_original_shortened$N[coded_data_original_shortened$many_labs_version == "3"], na.rm = TRUE), digits = 0),
apa_num(IQR(coded_data_original_shortened$N[coded_data_original_shortened$many_labs_version == "5"], na.rm = TRUE), digits = 0), apa_num(IQR(coded_data_original_shortened$N, na.rm = TRUE), digits = 0)))



# Changing column names to be less computer speak-y looking
colnames(many_labs_descriptives_table) <- c("Many Labs Version", "N ML Projects", "N Primary Measures",
                                            "Replication Total N", "Replication Total IQR", "Original Total N", 
                                            "Original Total IQR")

# print the table in apa formatting
apa_table(
  many_labs_descriptives_table, align = c("l", "r", "r", "r", "r", "r", "r")
  , caption = "Descriptive information of number of included projects measures, and their sample size, across many labs projects for both original articles and replication protocols"
  , escape = FALSE, placement = "htp", midrules = 4)

```


## Data Collection

The data on the preregistered replication protocols, and replication datasets from Many Labs 1, 2, 3, & 5 were all retrieved from their respective OSF pages: <https://osf.io/wx7ck/>, <https://osf.io/8cd4r/>, <https://osf.io/ct89g/>, & <https://osf.io/7a6rd/>. Both the replication protocols and replication datasets were scanned through to ensure that the planned analyses were feasible. However, no coding or analysis of either of them had taken place before the analyses were preregistered. Further details on the search strategy can be found in the ??? supplementary materials.

### Replication Datasets

The replication datasets refer to the data of each of the replication attempts for each of the original psychological effects undertaken as part of a Many Labs study. These publicly available datasets of the replication attempt were accessed through their OSF page and loaded into Rstudio [@R-Rstudio]. When available, codebooks, analysis scripts and study materials were used to identify the specific data for each measure. The variables associated with each relevant measure were extracted into their own dataframe containing the data from all studies in one replication set. These dataframes together formed the dataset used for the analyses.

In order to be considered relevant for the analysis, the measure had to consist of multiple scalar items, as single item measures were not usable for the planned analyses on score reliability [@wanous1996estimating], following up on simulation study of @stanley2014expectations. If cleaned data was available this was chosen over raw data, to ensure that variables were coded as intended (e.g. no reverse-coded items). Pilot data were omitted from the analyses entirely. These criterion resulted in suitable measure data from `r apa_num(length(unique(data_h3_multiple$g)), numerals = FALSE)` replication sets spread across on average `r apa_num(mean(table(data_h3_multiple$g)), digits = 0)` lab locations for the analyses of hypotheses 2 & 3.

```{r CleaningReplicationDatasetsData, include = FALSE, eval = FALSE}
##### Data already available, code does not need to be run!
### ML 1
# 1.3
data_1.3_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[22:29])
colnames(data_1.3_clean)[1] <- "g"
# 1.10
data_1.10_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[108:115])
colnames(data_1.10_clean)[1] <- "g"
# 1.11
data_1.11_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[73:76])
colnames(data_1.11_clean)[1] <- "g"
# 1.12.1
# not found
# 1.12.3
data_1.12.3.1_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[54:59])
colnames(data_1.12.3.1_clean)[1] <- "g"
data_1.12.3.2_clean <- cbind(as.factor(data_ml1[[5]]), data_ml1[60:65])
colnames(data_1.12.3.2_clean)[1] <- "g"

### ML 2
# 2.2
data_2.2_clean <- cbind(as.factor(data_2.2[[5]]), data_2.2[6:11])
colnames(data_2.2_clean)[1] <- "g"
# 2.3
# data does not appear suitable
# 2.4.1
data_2.4.1_clean <- cbind(as.factor(data_2.4.1[[5]]), data_2.4.1[6:11])
colnames(data_2.4.1_clean)[1] <- "g"
# 2.4.2
data_2.4.2_clean <- cbind(as.factor(data_2.4.2[[5]]), data_2.4.2[6:14])
colnames(data_2.4.2_clean)[1] <- "g"
# 2.8.2
data_2.8.2_clean <- cbind(as.factor(data_2.8.2[[6]]), data_2.8.2[9:13])
colnames(data_2.8.2_clean)[1] <- "g"
# 2.10.1
data_2.10.1_clean <- cbind(as.factor(data_2.10.1[[5]]), data_2.10.1[6:11])
colnames(data_2.10.1_clean)[1] <- "g"
# 2.12.1
data_2.12.1_clean <- cbind(as.factor(data_2.12[[5]]), data_2.12[c(6,7,8,9,10,31,32,33,34,35)]) 
data_2.12.1_clean[3465:6905,2:6] <- data_2.12.1_clean[3465:6905,7:11]
data_2.12.1_clean <- data_2.12.1_clean[1:6]
colnames(data_2.12.1_clean)[1] <- "g"
# 2.12.2
data_2.12.2_clean <- cbind(as.factor(data_2.12[[5]]), data_2.12[c(11,14,15,18,19,22,24,27,28,29,36,39,40,43,44,47,49,52,53,54)]) 
data_2.12.2_clean[3465:6905,2:11] <- data_2.12.2_clean[3465:6905,12:21]
data_2.12.2_clean <- data_2.12.2_clean[1:11]
colnames(data_2.12.2_clean)[1] <- "g"
# 2.12.3
data_2.12.3_clean <- cbind(as.factor(data_2.12[[5]]), data_2.12[c(12,13,16,17,20,21,23,25,26,30,37,38,41,42,45,46,48,50,51,55)]) 
data_2.12.3_clean[3465:6905,2:11] <- data_2.12.3_clean[3465:6905,12:21]
data_2.12.3_clean <- data_2.12.3_clean[1:11]
colnames(data_2.12.3_clean)[1] <- "g"
# 2.15
data_2.15_clean <- cbind(as.factor(data_2.15[[5]]), data_2.15[8:12])
colnames(data_2.15_clean)[1] <- "g"
# 2.19.1
# difficult to extract
# 2.19.2
# difficult to extract
# 2.20
data_2.20_clean <- cbind(as.factor(data_2.20[[5]]), data_2.20[6:45]) 
data_2.20_clean[3729:7396,2:21] <- data_2.20_clean[3729:7396,22:41]
data_2.20_clean <- data_2.20_clean[1:21] 
# coding so all 1's means somebody used rule-based grouping strategy
data_2.20_clean[,c(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)] <- ifelse(data_2.20_clean[,c(2, 4, 6, 8, 10, 12, 14, 16, 18, 20)] == 1, 1, 0)
data_2.20_clean[,c(3, 5, 7, 9, 11, 13, 15, 17, 19, 21)] <- ifelse(data_2.20_clean[,c(3, 5, 7, 9, 11, 13, 15, 17, 19, 21)] == 2, 1, 0)
colnames(data_2.20_clean)[1] <- "g"
# 2.23
data_2.23_clean <- cbind(as.factor(data_2.23[[5]]), data_2.23[c(7,8,12,13,15)])
colnames(data_2.23_clean)[1] <- "g"


### ML 3
# 3.2.1
data_3.2.1 <- cbind(as.factor(data_ml3[[1]]), data_ml3[77:86] - 1)
data_3.2.1.1_clean <- na.omit(data_3.2.1[1:6])
colnames(data_3.2.1.1_clean)[1] <- "g"
data_3.2.1.2_clean <- na.omit(data_3.2.1[c(1, 7:11)])
colnames(data_3.2.1.2_clean)[1] <- "g"
# 3.5
# data appears unusable
# 3.7.1
data_3.7.1_clean <- na.omit(cbind(as.factor(data_ml3[[1]]), data_ml3[38:42]))
colnames(data_3.7.1_clean)[1] <- "g"
# 3.7.2
data_3.7.2_clean <- na.omit(cbind(as.factor(data_ml3[[1]]), data_ml3[89:94]))
colnames(data_3.7.2_clean)[1] <- "g"
# 3.8.1
# a single measure was reported
# 3.8.2
data_3.8.2_clean <- na.omit(cbind(as.factor(data_ml3[[1]]), data_ml3[29:30])) 
colnames(data_3.8.2_clean)[1] <- "g"


### ML 5
# 5.1.1
data_5.1.1_clean <- cbind(as.factor(data_5.1[[2]]), data_5.1[13:27])
colnames(data_5.1.1_clean)[1] <- "g"
# 5.1.2
data_5.1.2_clean <- cbind(as.factor(data_5.1[[2]]), data_5.1[28:33])
colnames(data_5.1.2_clean)[1] <- "g"
# 5.4
data_5.4_clean <- cbind(as.factor(data_5.4[[1]]), data_5.4[18:41])
colnames(data_5.4_clean)[1] <- "g"
# 5.5.1 & 5.5.2
# from this dataset it appears that this data will be difficult to use.
# 5.5.2
# also difficult to use
# 5.7 
data_5.7_clean <- cbind(as.factor(data_5.7[[3]]), data_5.7[c(25, 34, 35, 36, 37, 38, 39, 40, 41, 42)])
colnames(data_5.7_clean)[1] <- "g"
# 5.9.1
data_5.9.1_clean <- na.omit(cbind(as.factor(data_5.9.1[[4]]), data_5.9.1[c(79, 83, 87, 91, 95, 98, 101)]))
colnames(data_5.9.1_clean)[1] <- "g"

### to summarize
# total number of likely usable: 15 (1.10, 1.11, 1.12.3.1, 1.12.3.2, 2.10.1, 
# 2.12.1, 2.12.2, 2.12.3, 2.15, 2.23, 3.7.1, 3.7.2, 3.8.2, 5.7, 5.9.1)
# total number of maybe usable: 6 (1.3, 2.20, 3.2.1.1, 3.2.1.2, 5.1.1, 5.4)
# total alpha coefficient comparison usable: 3 
#                                           1.11: 0.82
#                                           3.7.2: 0.67
#                                           5.9.1: 0.84


### Saving to Intermediate Data Folder
open_data(data = data_1.3_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_1.3_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_1.3_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_1.3_clean))), ".yml")) 

open_data(data = data_1.10_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_1.10_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_1.10_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_1.10_clean))), ".yml")) 

open_data(data = data_1.11_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_1.11_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_1.11_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_1.11_clean))), ".yml")) 

open_data(data = data_1.12.3.1_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_1.12.3.1_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_1.12.3.1_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_1.12.3.1_clean))), ".yml")) 

open_data(data = data_1.12.3.2_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_1.12.3.2_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_1.12.3.2_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_1.12.3.2_clean))), ".yml")) 

open_data(data = data_2.10.1_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_2.10.1_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_2.10.1_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_2.10.1_clean))), ".yml")) 

open_data(data = data_2.12.1_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_2.12.1_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_2.12.1_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_2.12.1_clean))), ".yml")) 

open_data(data = data_2.12.2_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_2.12.2_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_2.12.2_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_2.12.2_clean))), ".yml")) 

open_data(data = data_2.12.3_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_2.12.3_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_2.12.3_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_2.12.3_clean))), ".yml")) 

open_data(data = data_2.15_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_2.15_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_2.15_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_2.15_clean))), ".yml")) 

open_data(data = data_2.20_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_2.20_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_2.20_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_2.20_clean))), ".yml")) 

open_data(data = data_2.23_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_2.23_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_2.23_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_2.23_clean))), ".yml")) 

open_data(data = data_3.2.1.1_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_3.2.1.1_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_3.2.1.1_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_3.2.1.1_clean))), ".yml")) 

open_data(data = data_3.2.1.2_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_3.2.1.2_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_3.2.1.2_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_3.2.1.2_clean))), ".yml")) 

open_data(data = data_3.7.1_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_3.7.1_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_3.7.1_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_3.7.1_clean))), ".yml")) 

open_data(data = data_3.7.2_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_3.7.2_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_3.7.2_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_3.7.2_clean))), ".yml")) 

open_data(data = data_3.8.2_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_3.8.2_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_3.8.2_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_3.8.2_clean))), ".yml")) 

open_data(data = data_5.1.1_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_5.1.1_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_5.1.1_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_5.1.1_clean))), ".yml")) 

open_data(data = data_5.1.2_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_5.1.2_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_5.1.2_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_5.1.2_clean))), ".yml")) 

open_data(data = data_5.4_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_5.4_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_5.4_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_5.4_clean))), ".yml")) 

open_data(data = data_5.7_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_5.7_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_5.7_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_5.7_clean))), ".yml")) 

open_data(data = data_5.9.1_clean, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(data_5.9.1_clean))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(data_5.9.1_clean))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(data_5.9.1_clean))), ".yml")) 

```

### Preregistered Replication Protocols

The preregistered replication protocols refer to the publicly available protocols describing the background, methodology, and analysis of each replication set. These protocols were available through the OSF pages of each Many Labs project.

The OSF page for each Many Labs project was combed through, in order to identify the components of the research which contained information on the measurement practice and reported reliability. The focus was on finding the preregistered protocols which contained information on the entirety of the replication sets in each Many Labs project. For Many Labs 2, & 3 a common protocol could be identified, for Many Labs 1 the Proposal_V1.1 provided the equivalent information, while for Many Labs 5 protocols were available for each replication set separately.

The relevant OSF page of each replication set was searched through for a file which contained protocol in its name. If available, a version of the protocol labelled as revised, post-review, peer-review, or endorsed was selected. In case there was doubt about which was most appropriate, the latest uploaded relevant protocol was selected. Only regular protocols were taken, meaning those not labelled as data collection, analysis protocol or anything similar[^2].

[^2]: The steps taken to access the replication protocol data from Many Labs 5 were not preregistered, and only became clear upon further investigation of the accessible information. The specific names and locations of the files that were used as the datasets can be found under the supplementary materials of the OSF page of this article <https://osf.io/gft46/%7D>.

Finally, the reports of the Many Labs projects, meaning the published articles [@klein2014investigating; @klein2018many; @ebersole2016many; @ebersole2020many], were used to determine whether not the replication was considered successful.

### Original Study

The original study articles are the published articles of the original effect on which the Many Labs replication sets were based. In the case of Many Labs 5 [@ebersole2020] the original studies refer to the studies which first tested the effect to be replicated, not the earlier replication attempts of the same effects in the RPP [@osc2015estimating]. 

The original articles were identified using the citations in each replication protocol. Then through searches via Web of Science, Google Scholar, and PsychInfo (in that order), the articles were retrieved[^3].

[^3]: An issue was encountered while trying to access @hyman1950current. However, after contacting the first author of @klein2014investigating a suitable version of the article was obtained for use in the analyses.

```{r CleaningCodedData, include = FALSE, eval = FALSE}
##### Data already available, code does not need to be run!
# Selecting the relevant rows and columns for the data
coded_data_initial_sel <- coded_data_initial_raw[3:160, 18:57]
coded_data_revised_sel <- coded_data_revised_raw[3:160, 18:38]
coded_data_vignette_raw <- coded_data_vignette_raw[3:160, 2]

# Combining the datasets
coded_data_full <- cbind(coded_data_initial_sel, 
                         cbind(coded_data_revised_sel, coded_data_vignette_raw))

# filtering out unnecessary double columns
coded_data_full <- cbind(coded_data_full[, 1:40], coded_data_full[, 45:62])



### creating the cleaned dataset using the functions in the source code 
coded_data_clean <- calculating(recoding(restructuring(fixing(
  renaming(coded_data_full)))))



### Saving to Intermediate Data Folder
# Splitting data into replication and original
coded_data_replications <- coded_data_clean[1:77,]
coded_data_original <- coded_data_clean[78:157,]

# difference in dataset row number is due to the fact that the moral foundations
# questionnaire in original 2.4 is reported using all 5 of its factors, whereas
# in replication 2.4 only the two overarching groups of binding and 
# individualizing foundations are described.
# For that reason a shortened original dataset will be used for any direct
# comparisons between original and replication coding.
coded_data_original_shortened <- coded_data_original[c(1:17, 19, 22:80),] 
coded_data_original_shortened[c(18,19),5] <- 
  c("individualizing moral foundations", "binding moral foundations")
coded_data_original_shortened[18,13] <- NA
coded_data_original_shortened[19,13] <- NA


# exporting cleaned data
open_data(data = coded_data_replications, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(coded_data_replications))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(coded_data_replications))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(coded_data_replications))), ".yml")) 

open_data(data = coded_data_original, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(coded_data_original))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(coded_data_original))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(coded_data_original))), ".yml")) 

open_data(data = coded_data_original_shortened, filename = paste0(paste0(
  "Data/IntermediateData/", deparse(substitute(coded_data_original_shortened))), ".csv"),
  codebook = paste0(paste0(
    "Data/IntermediateData/codebook_", 
    deparse(substitute(coded_data_original_shortened))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/IntermediateData/value_labels_", 
    deparse(substitute(coded_data_original_shortened))), ".yml")) 


# clearing up memory space by removing raw files from the environment
rm(coded_data_initial_raw, coded_data_revised_raw, coded_data_vignette_raw, 
   data_2.10.1, data_2.12, data_2.15, data_2.19.1, data_2.2, data_2.20, 
   data_2.23, data_2.3, data_2.4.1, data_2.4.2, data_2.8.2, data_3.5, data_5.1,
   data_5.4, data_5.5, data_5.7, data_5.9.1, data_ml1, data_ml3)

```


## Measures {#Measures}

Relevant primary measures were identified by combing through the entirety of the methods section for each preregistered replication protocol. Then using questions and the accompanying search strategies as documented in the structured coding protocol of this study (available on the relevant OSF: <https://osf.io/9r8yt/>, and github pages: <https://github.com/CasGoos/measurement_and_replication.git>), information on the reported measurement evidence and reporting quality was assessed for both replication protocols and original studies. Descriptions of the extracted variables can be found in the sections below. The coding protocol and explanations on how the articles and replication protocols were searched through to answer the questions in the coding protocol can be found in the supplementary materials ???.

### Measurement Error Measures

In order to answer questions regarding reported reliability, the reported reliability coefficient was extracted from both the replication protocols and the original articles. The reported reliability coefficient is the score reliability reported per measure in the replication protocol or original article. Both the type (Cronbach's alpha, retest, interrater, etc.) and the value itself were extracted.

Because data from the Many Labs replications was available for each lab location within a replication set, it was possible to not only calculate score reliabilities for the scalar measures used, but also calculate it for each lab location separately. This made it possible to assess the variation of score reliabilities of the same measure across different contexts.

Cronbach's alpha was calculated as the main score reliability index used in the analyses, as remains the most commonly used indicator of the reliability of a measure. Cronbach's alpha was calculated using the alpha function from the psych R package [@R-psych]. Default arguments were used for the function.

```{r ReliabilityValidityTestabilityCheck, include = FALSE, warning = FALSE}
## Checking the measures to recalculate the reliability and factor analysis for.
## The question: "Are they psychometrics scales or not?", is key here.
View(coded_data_replications)

# Likely reliability: 1.10, 1.11, 1.12.3, 2.4.1, 2.4.2, 2.8.2, 2.10.1, 2.12.1, 
# 2.12.2, 2.12.3, 2.15, 2.19.2, 2.23, 3.7.1, 3.7.2, 3.8.1, 3.8.2, 5.7, 5.9.1
# Likely factor: 1.10, 1.11, 1.12.3, 2.4.1, 2.4.2, 2.8.2, 2.10.1, 2.12.1, 2.12.2,
# 2,12,3, 2.15, 2.19.2, 2.23, 3.7.1, 3.7.2, 3.8.1, 3.8.2, 5.7, 5.9.1
# Maybe reliability: 1.3, 1.12.1, 2.2, 2.3, 2.19.1, 2.20, 3.2.1, 3.5, 5.1.1, 
# 5.1.2, 5.4, 5.5.1, 5.5.2
# Maybe factor: 1.3, 1.12.1, 2.3, 2.19.1, 2.20, 3.2.1, 3.5, 5.1.1, 5.1.2, 5.4,
# 5.5.1, 5.5.2

# 1.9 is difficult, might be a scale as dv rather than voting behavior.
```

### QMP measures

The tests for hypotheses 4, 5, & 6 were all based on the QMPs coded across both original articles and replication protocols. QMPs were all coded to be either true, false, or not applicable if not relevant for that particular measure (e.g. reporting results from a factor analysis in case a single item measure was used). Questions related to modification had 'no modification' as an additional answer option, in case the specific aspect of the measure was explicitly not modified. Each coded QMP belonged to one of five QMP type categories that were derived from the classification in @flake2020measurement. An overview of the QMP categories of the coding variables can be seen in Table \@ref(tab:QMPCodingInfoTable).

```{r QMPCodingInfoTable, warning = FALSE}
QMP_info_dataframe <-data.frame(Category = c("Definition", "", "", "Operationalisation", "", "", "", "Selection/Creation", "", "", "Quantification", "Modification", "", "", "", "", ""),
           'N Questions' = c("1", "", "", "5", "", "", "", "4", "", "", "4", "6", "", "", "", "", ""),
           'Example Question' = c("A psychological/sociological definition",
            "is given to the name of the measured",
            "variable within the paper.", 
            "The administration format (pen-and-",
            "paper/computer) and environment (in",
            "public/in a lab) are described (Note:",
            "both should be present for a true rating).", 
            "The source of the scale is provided",
            "(in case the scale was newly developed",
            "this should be clearly stated).", 
            "The number of items are described.", 
            "Any format changes are mentioned",
            "(paper-and-pencil <–> computer), if no",
            "changes were made to the format, and",
            "this was mentioned then code as No",
            "modification. If it is not clear, then code",
            "as False."))


# transfer the data to an APA table for printing
apa_table(
  QMP_info_dataframe, align = c("l", "r", "l")
  , caption = "Information of QMP coding variables per category."
  , note = "N Questions refers only to the questions considered quantative as specified in the text below."
  , escape = FALSE, placement= "htp", booktabs = TRUE)

```

Selection and creation share a category as the justifications and requirements in selecting a measure are similar to those for creating a new measure. Modification variables were only assessed when it was clear that the measure was modified from an original source in some way, and were otherwise omitted from the QMP calculations for that measure.

Eleven out of twenty of the QMP variables used to calculate the QMP ratio have an equivalent in the coding key of @flake2022construct. The remaining nine were either altered from the coding key to better fit the QMP counting operationalization, all newly created or modified items were based on the definitions of the QMP categories and their criteria as specified in @flake2020measurement. Beside these quantitative variables, a total of four qualitative variables were also included to provide more detailed information on some aspects of the measure and its reporting. Details on the items used, their origin, changes, and justifications can be found in supplementary materials ???. 

For the analyses of these variables both the total QMP ratios and ratios per QMP type category were used. This ratio was based on the number of coded trues divided by the total number of applicable QMPs. QMPs were considered applicable if not coded as not applicable or no modification. These ratios were preferred over counts, since non applicable items could thus be accounted for. Formula \@ref(eq:QMPRatioType) shows the formula that was used to calculate the QMP ratio for a particular QMP type j, where $True_{ij}$ is 1 if $item_{ij}$ was coded as True and 0 otherwise, and $False_{ij}$ is 1 if $item_{ij}$ was coded as False and 0 otherwise, $j \subset \{ definition, operationalisation, selection/creation, quanitification, modification \}$, and $i = 1,..., n_{j-items}$.

```{=tex}
\begin{equation}
QMP_ratio_{j} = \frac{\sum_{n=1}^i True_{ij}}{\sum_{n=1}^i True_{ij} + False_{ij}} (\#eq:QMPRatioType)
\end{equation}
```

For the total QMP ratio:

```{=tex}
\begin{equation}
QMP_ratio = \frac{\sum_{n=1}^j \sum_{n=1}^i True_{ij}}{\sum_{n=1}^j \sum_{n=1}^i True_{ij} + False_{ij}} (\#eq:QMPRatioTotal)
\end{equation}
```

After the data was initially coded according to the preregistered coding protocol, it was determined, based on discussion with the supervisor, that some of the initial ratings were possibly to stringent. As a result, some of the items in the protocol were reformulated, so that the conclusions from the resulting analyses were based on a more lenient judgement of QMPs. The revision of the coding protocol happened after all the data were coded using the initial protocol. As a result the coded data using the initial protocol, and the coded data using the revised protocol are both available on this article's OSF (<https://osf.io/9r8yt/>) and github (<https://github.com/CasGoos/measurement_and_replication.git>) pages. The analyses, tables and figures presented in this article are all based on the revised coding protocol, the equivalent results based on QMPs obtained with the initial protocol can be found in Appendix D.

### Replicability Measure

Hypothesis 3 as well as 5 specify an association between either score reliability or QMPs respectively, and replication success. Replication success was based on the test of the main effect of a study. Since only primary measures were used to form the dataset, each measure related to the test of a main effect. Replication success was determined based on the significance of the meta-analytic effect of the replication set as reported in its respective Many Labs report. An effect was considered replicated if the meta-analytic p-value was lower than .05, and in the same direction as the original effect. In all cases, meta-analytic significance of an effect meant that support in the direction of the original effect was found.

## Analyses

Every hypothesis test in this study used an alpha significance cutoff of .05. No correction for multiple testing was applied, to ensure that the rate of false negatives remained low, the increase in false positive rate was accepted as they deemed less harmful to the exploratory aims of this study. 

```{r FinalPreparationData, include = FALSE, eval = FALSE}
##### Data already available, code does not need to be run!
### this code prepares the various pieces of data to be ready to use for the 
### analyses.
## confirmatory analyses
# for hypothesis 4
data_h4 <- data_prep_H4(coded_data_original_shortened, coded_data_replications)
# for hypothesis 5
data_h5 <- data_prep_H5(coded_data_replications)
# for hypothesis 6
data_h6 <- data_prep_H6(coded_data_original_shortened, coded_data_replications)

# for hypothesis 1
data_h1 <- data_prep_H1(coded_data_original_shortened, coded_data_replications)
# for hypothesis 2
data_h2 <- data_prep_H2(data_1.10_clean, data_1.11_clean, data_1.12.3.1_clean, data_1.12.3.2_clean, data_2.12.1_clean, data_2.12.2_clean, data_2.12.3_clean, data_2.15_clean, data_2.20_clean, data_2.23_clean, data_3.2.1.1_clean, data_3.2.1.2_clean, data_3.7.1_clean, data_3.7.2_clean, data_3.8.2_clean, data_5.1.1_clean, data_5.1.2_clean, data_5.7_clean, data_5.9.1_clean)
# for hypothesis 3
data_h3_multiple <- Data_prep_H3_multiple(data_h5, data_h2)
data_h3_avg <- Data_prep_H3_avg(data_h5, data_h3_multiple)


## graphs
# plot accompanying the results for Hypotheses 4, 5, & 6.
plot_456_data <- data_prep_plot_456(data_h6)
# plot accompanying the results for Hypotheses 4, & 6
plot_46_data <- data_prep_plot_46(data_h6[c(3, 6, 9, 12, 15, 18, 36, 39, 42, 45, 48, 51)])
plot_46_data_rev <- data_prep_plot_46(data_h6[c(3, 21, 24, 27, 30, 33, 36, 54, 57, 60, 63, 66)])
# plot accompanying the results for Hypotheses 2, & 3
plot_23_data_alpha <- data_prep_plot_23_alpha(data_h3_multiple)
plot_23_data_omega <- data_prep_plot_23_omega(data_h3_multiple)
plot_23_data_reported_alpha <- data_prep_plot_23_alpha_reported(coded_data_original_shortened, data_h3_avg, data_h3_multiple)


### Exporting the final datasets for the analyses
# the open data
open_data(data = data_h4, filename = paste0(paste0(
  "Data/AnalysisData/", deparse(substitute(data_h4))), ".csv"),
  codebook = paste0(paste0(
    "Data/AnalysisData/codebook_", 
    deparse(substitute(data_h4))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/AnalysisData/value_labels_", 
    deparse(substitute(data_h4))), ".yml")) 

open_data(data = data_h6, filename = paste0(paste0(
  "Data/AnalysisData/", deparse(substitute(data_h6))), ".csv"),
  codebook = paste0(paste0(
    "Data/AnalysisData/codebook_", 
    deparse(substitute(data_h6))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/AnalysisData/value_labels_", 
    deparse(substitute(data_h6))), ".yml")) 

open_data(data = data_h5, filename = paste0(paste0(
  "Data/AnalysisData/", deparse(substitute(data_h5))), ".csv"),
  codebook = paste0(paste0(
    "Data/AnalysisData/codebook_", 
    deparse(substitute(data_h5))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/AnalysisData/value_labels_", 
    deparse(substitute(data_h5))), ".yml")) 

open_data(data = data_h1, filename = paste0(paste0(
  "Data/AnalysisData/", deparse(substitute(data_h1))), ".csv"),
  codebook = paste0(paste0(
    "Data/AnalysisData/codebook_", 
    deparse(substitute(data_h1))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/AnalysisData/value_labels_", 
    deparse(substitute(data_h1))), ".yml")) 

open_data(data = plot_456_data, filename = paste0(paste0(
  "Data/AnalysisData/", deparse(substitute(plot_456_data))), ".csv"),
  codebook = paste0(paste0(
    "Data/AnalysisData/codebook_", 
    deparse(substitute(plot_456_data))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/AnalysisData/value_labels_", 
    deparse(substitute(plot_456_data))), ".yml")) 

open_data(data = plot_46_data, filename = paste0(paste0(
  "Data/AnalysisData/", deparse(substitute(Plot_46_data))), ".csv"),
  codebook = paste0(paste0(
    "Data/AnalysisData/codebook_", 
    deparse(substitute(Plot_46_data))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/AnalysisData/value_labels_", 
    deparse(substitute(Plot_46_data))), ".yml")) 

open_data(data = plot_46_data_rev, filename = paste0(paste0(
  "Data/AnalysisData/", deparse(substitute(plot_46_data_rev))), ".csv"),
  codebook = paste0(paste0(
    "Data/AnalysisData/codebook_", 
    deparse(substitute(plot_46_data_rev))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/AnalysisData/value_labels_", 
    deparse(substitute(plot_46_data_rev))), ".yml")) 

open_data(data = data_h2, filename = paste0(paste0(
  "Data/AnalysisData/", deparse(substitute(data_h2))), ".csv"),
  codebook = paste0(paste0(
    "Data/AnalysisData/codebook_", 
    deparse(substitute(data_h2))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/AnalysisData/value_labels_", 
    deparse(substitute(data_h2))), ".yml")) 

open_data(data = data_h3_multiple, filename = paste0(paste0(
  "Data/AnalysisData/", deparse(substitute(data_h3_multiple))), ".csv"),
  codebook = paste0(paste0(
    "Data/AnalysisData/codebook_", 
    deparse(substitute(data_h3_multiple))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/AnalysisData/value_labels_", 
    deparse(substitute(data_h3_multiple))), ".yml")) 

open_data(data = data_h3_avg, filename = paste0(paste0(
  "Data/AnalysisData/", deparse(substitute(data_h3_avg))), ".csv"),
  codebook = paste0(paste0(
    "Data/AnalysisData/codebook_", 
    deparse(substitute(data_h3_avg))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/AnalysisData/value_labels_", 
    deparse(substitute(data_h3_avg))), ".yml")) 

open_data(data = plot_23_data_alpha, filename = paste0(paste0(
  "Data/AnalysisData/", deparse(substitute(plot_23_data_alpha))), ".csv"),
  codebook = paste0(paste0(
    "Data/AnalysisData/codebook_", 
    deparse(substitute(plot_23_data_alpha))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/AnalysisData/value_labels_", 
    deparse(substitute(plot_23_data_alpha))), ".yml")) 

open_data(data = plot_23_data_omega, filename = paste0(paste0(
  "Data/AnalysisData/", deparse(substitute(plot_23_data_omega))), ".csv"),
  codebook = paste0(paste0(
    "Data/AnalysisData/codebook_", 
    deparse(substitute(plot_23_data_omega))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/AnalysisData/value_labels_", 
    deparse(substitute(plot_23_data_omega))), ".yml")) 

open_data(data = plot_23_data_reported_alpha, filename = paste0(paste0(
  "Data/AnalysisData/", deparse(substitute(plot_23_data_reported_alpha))), ".csv"),
  codebook = paste0(paste0(
    "Data/AnalysisData/codebook_", 
    deparse(substitute(plot_23_data_reported_alpha))), ".Rmd"),
  value_labels = paste0(paste0(
    "Data/AnalysisData/value_labels_", 
    deparse(substitute(plot_23_data_reported_alpha))), ".yml")) 
```

### Analyses Measurement Error

The pre-registered test for hypothesis 1, a t-test of the reported Cronbach's alpha coefficient values between original and replication, could not be conducted, since not enough score reliabilities were reported to conduct the test and expect sensible results. Instead, the total number of reported reliability coefficients were compared between the replication protocols and the original articles. This comparison was tested via a chi-square test using the base R stats package [@R-base].

```{r Hypothesis1Model, include = FALSE}
# first some descriptives of reliability reporting
# Measurement type per original and replication:
H1_descriptive_table_1 <- table(data_h1$rep_org, data_h1$N_items)

# amount of reliability reported per original and replication 
# (given that measure is a multiple item measure):
H1_descriptive_table_2 <- table(data_h1$rep_org[data_h1$N_items == "multiple item measure"], data_h1$reliability_type[data_h1$N_items == "multiple item measure"])

# New H1 test
# A chi square test for difference in N reliability reported for multiple 
# item measures.
H1_test_result <- chisq.test(data_h1$rep_org[data_h1$N_items == "multiple item measure"], data_h1$reliability_reported[data_h1$N_items == "multiple item measure"])

```

While reported reliabilities were rare, reliabilities could still be calculated from `r apa_num(length(unique(data_h3_multiple$g)), numerals = FALSE)` replication datasets. On top of that, reliabilities could be calculated for each individual lab per replication set so that the variance of the coefficients of a single measure could be assessed. In order to separate between and within variance in reliability, a multilevel random intercept model was specified with replication set as the grouping variable and the Cronbach's alpha value of a replication as dependent using the lmer function from the lme4 package [@R-lme4]. The formulas for this model, where $alpha_{ij}$ represents the calculated Cronbach's alpha coefficient of a single study at one lab location with $i = 1,..., n_{locations}$ and $j = 1,...,n_{replicationsets}$ look as follows:

```{=tex}
\begin{align}
& Level 1: alpha_{ij} = \beta_{0j} + e_{ij} \qquad\qquad\quad e_{ij} \sim N(0, \sigma_{e}^{2}) (\#eq:level1H2) \\
& Level 2: \beta_{0j} = \gamma_{00} + u_{0j} \qquad\qquad\qquad u_{0j} \sim N(0, \sigma_{u0}^{2}) (\#eq:level2H2) \\
& Mixed: alpha_{ij} = \gamma_{00} + u_{0j} + e_{ij} (\#eq:mixedH2)
\end{align}
```
```{r Hypothesis2Model, include = FALSE}
# intercept only model to check the within and between group variance.
H2_test_result_alpha <- lmer(formula = alpha ~ 1 + (1|g), data = data_h2)

```

The main author was not aware of the existence of any formal test as of the writing of the pre-regsitration for testing for within group variance after controlling for between group variance. Thus, the intraclass correlation (ICC) coefficient was pre-registered as the indicator of within vs. between variability. The formula used to calculate the ICC is presented below.

```{=tex}
\begin{equation}
ICC = \frac{u_{0j}}{(e_{ij} + u_{0j})} (\#eq:ICC)
\end{equation}
```
A high ICC indicates low within group variance relative to between group variance, which would be in line with hypothesis 2.

```{r ICCCalculations, include = FALSE}
# Variances
within_group_variance_alpha <- attr(VarCorr(H2_test_result_alpha), "sc")^2
between_group_variance_alpha <- attr(VarCorr(H2_test_result_alpha)$g, "stddev")^2

# ICC
ICC_alpha <- between_group_variance_alpha / (within_group_variance_alpha + between_group_variance_alpha)

```

After the pre-registration was registered, and data had been collected and analysed, it came to the author's attention that it is possible to conduct meta-analyses on reliability coefficients, commonly referred to as Reliability Generalization Meta-Analyses [@botella2012ManagingHeterogeneityVariance; @lopez-ibanez2024ReliabilityGeneralizationMetaanalysis]. Using meta-analysis, it is possible to quantify within the model the degree of true variation in the metric of interest across samples [@cooper2019handbook]. Subsequently, the true variation in the metric, or heterogeneity, can be tested to constitute a significant part of the total variation in the observed estimates of the metric or not. 

Because heterogeneity estimates line up well as a test for hypothesis 2, a random-effects meta-analysis was conducted on the calculated Cronbach's alpha for each measure for which replication datasets were available and usable. The rma function from metafor package [@R-metafor] was used to run the meta-analysis. No correction for bias or weighting of the effects has been implemented, to reduce model complexity, and because the data came from Many Labs replication projects and not typical literature.

A meta-analysis requires an estimate of the standard error in order to be conducted. For Cronbach's alpha, formulas 2 & 3 from @duhachek2004AlphaStandardError was used to calculate alpha's standard error. Heterogeneity was estimated using the tau value, meaning the standard deviation of the distribution of true Cronbach's alpha coefficients for that measure, and tested using the Cochran's Q test for each measure. 

In order to test the association between reliability and replicability that had been indicated by @stanley2014expectations on non-simulated data, the calculated reliabilties were used as a predictor for the coded replication success of the effect the measure was used for. The logistic regression shown in equation \@ref(eq:H3Main) was used to model this association, where $f(.)$ represents the logit link function, $i = 1,..., n_{replications}$, the outcome $replicated$ which was coded as 1 if the meta-analytic effect in the replication set matches the original effect in direction at a significance alpha level of .05, and 0 otherwise, and the predictor $alpha$ referred to the averaged calculated Cronbach's alpha coefficient:


```{=tex}
\begin{equation}
f(replicated_{i}) = \beta_{0} + \beta_{1} alpha_{i} (\#eq:H3Main)
\end{equation}
```
```{r Hypothesis3MainTest, include = FALSE}
## The main model to test hypothesis 3, through a logistic regression model.
H3_test_result_main_alpha <- apa_print(glm(formula = replication ~ 1 + alpha, 
                                      family = binomial(), data = data_h3_avg))

# coefficient is positive but not significant.

# adding OR to the result
H3_main_full_results_alpha_with_OR <- OR_to_apa_full_supplier(H3_test_result_main_alpha$full_result$alpha, negative_b = FALSE)
  
# well this OR is quite a ridiculous number now..., but then again a step from 0 alpha to 1, is quite a jump too...

```

### Analyses QMPs

The reported analyses were primarily based on the revised set of QMP data. The results using the original QMP data are mentioned when the results led to different conclusions. The analysis code for both the original and revised QMP data are available in the R script on both the OSF (<https://osf.io/9r8yt/>) and github (<https://github.com/CasGoos/measurement_and_replication.git>) pages of this article.

Hypothesis 4 & 6 were both tested using a model with the same dependent variable: total QMP ratio. In order to model a ratio dependent variable, beta regression was used. Beta regression models operate similarly to standard generalized linear models with similar interpretations of the results. What differentiates them is that they are suitable to model dependent variables with values in the interval of $(0, 1)$, meaning ratios. Furthermore, they are well suited for modelling heteroskedastic and asymmetrically distributed dependent variables [@cribari2010beta]. The beta regression model was implemented using the betareg function within the betareg package [@R-betareg_a; @R-betareg_b]. Defaults were used for all function parameters besides the model formula and data. All beta regression models used the beta link function, which is as follows:

```{=tex}
\begin{equation}
g(.) = (0, 1) \mapsto \mathbb{R} (\#eq:BetaLink)
\end{equation}
```

These models assume that the QMPs are the same across each replication set. This was necessary as only one protocol was available for coding per replication set. The assumption was deemed likely to hold, since each replication within a set was based on the same protocol containing all the measurement details that were reported and upon which measurement was based.

To test hypothesis 4, the total QMP ratios between replication and original research was compared. The model formula for this test looked as follows:

```{=tex}
\begin{equation}
g(QMP\_ratio_{i}) = \beta_{0} + \beta_{1} replication_{i} (\#eq:H4Main)
\end{equation}
```
```{r Hypothesis4Model, include = FALSE}
## Hypothesis 4 beta regression model
# running the models with revised QMP ratio's
H4_test_results_REV <- betareg(QMP_REV_ratio ~ rep_org, data = data_h4)

```

Where $replication_{i}$ is a dummy coded variable, which was coded as 1 in case the reported QMP ratio belonged to a replications, and 0 if it belonged to an original study. For hypothesis 5 the QMP ratio was used as the predictor for replication success, while replication success was used as the outcome[^4].

[^4]: This analysis was changed from the pre-registered analysis. In the pre-registered analysis a beta-regression was used to model the same relation, only with replication success as the predictor variables and replication QMP ratio as the outcome. The outcome was swapped in the article for the sake of interpretability. The results based on the original analyses are presented in Appendix A.


```{=tex}
\begin{equation}
logit(replicated_{i}) = \beta_{0} + \beta_{1} QMP\_ratio_{i} (\#eq:H5Main)
\end{equation}
```

Where $replicated$ is a dummy coded variable, which is the same as for equation \@ref(eq:H3Main), and $logit(.)$ refers to the logit link function commonly used for logistic regression models. Note here that unlike in the test for hypothesis 4, $QMP\_ratio_{i}$ refers only to the QMP ratios of the replications, meaning that original article QMPs were not included in this analysis.

```{r Hypothesis5Model, include = FALSE}
## Hypothesis 5 logistic regression model
H5_test_results_REV <- apa_print(glm(hypothesis_support ~ QMP_REV_ratio, data = data_h5, family = binomial))

H5_test_results_REV_with_OR <- OR_to_apa_full_supplier(H5_test_results_REV$full_result$QMP_REV_ratio, negative_b = TRUE)

```

The relationship between original article QMPs and replication protocol QMPs was modeled to test hypothesis 6. The outcome variable is the same as the predictor in the model for hypothesis 5. The independent variable in this model is instead the QMP ratio for the original articles. The resulting model formula is as follows:

```{=tex}
\begin{equation}
g(QMP\_ratio\_replication_{i}) = \beta_{0} + \beta_{1} QMP\_ratio\_original_{i} (\#eq:H6Main)
\end{equation}
```
```{r Hypothesis6Model, include = FALSE}
## Hypothesis 6 beta regression model
H6_test_results_REV <- betareg(Rep_QMP_REV_ratio ~ QMP_REV_ratio, data = data_h6)

```

MOVE TO EXTERNAL SEARCH STRATEGY FILE: 
The scope of the coding was limited to the abstract, method, procedure, materials, measure, and initial parts of the results sections. Additionally, a keyword search using the name of the measured variable was used to search for any potentially missed information. An exception to this procedure was a question asking about the presence of example items. For this question, information from supplementary materials was uniquely considered as relevant. Another exception was the question regarding the replication of the original effect. Tables from the final report of each Many Labs project were used to answer that question.

These changes (to the QMP coding protocol) included: accepting linked online appendices as a data source for example items; assuming that if no differences between replication and original are measurement is mentioned, that there is no difference, even if it is not explicitly clear; and generally simplified criteria for what constituted sufficiently clear reporting.

SUPPLEMENTARY MATERIALS ON ITEM REVISION AND ORIGIN:
Two of the qualitative items have an equivalent in the coding key of Variables related to the specific measurement scaling and purpose in the paper, were omitted from the analyses of this study as they were deemed neither relevant for the analyses of this study, nor necessary in matching this study to the key results of @flake2022construct. Questions about more general measurement features such as the number of items and how scores were aggregated were altered from @flake2022construct to better fit the use of QMP ratios and associations between them and other coded aspects.

These (qualitative) variables assessed with two having an equivalent in @flake2022construct: the specific version of the measure (if reported); whether and what previous materials were used for the study; what, if any, psychometric evidence beyond reliability score (e.g. factor analysis, IRT model, convergent validity) was provided; and whether it was specified or not that the measure was or was not modified.


# Results

## Data Visualization
This study's focus is mainly exploratory and descriptive. Additionally, the number of coded and extracted data is quite substantial. Therefore, the result section will start with a number of visuals and descriptives to provide some clarity on the data and its characteristics.

Figure \@ref(fig:ReliabilityReportingFlowDiagram) depicts the flow of measures for which reliability was reported. The first split shows the number of measures which consisted of multiple items in a scale, which were approximately equal between replication and original (`r apa_num(table(coded_data_replications$N_items)["multiple item measure"])` and `r apa_num(table(coded_data_original_shortened$N_items)["multiple item measure"])`, respectively). Reliability was expected to be reported only for these measures. Taking these multiple item measures, the graph illustrates that `r apa_num(sum(table(coded_data_replications$N_items, coded_data_replications$reliability_type)["multiple item measure", - c(1, 3)]), numerals = FALSE)` measures in the replication protocols and `r apa_num(sum(table(coded_data_original_shortened$N_items, coded_data_original_shortened$reliability_type)["multiple item measure", - c(1, 3, 4)]), numerals = FALSE)` in the original articles reported a reliability coefficient. Looking closer at these reported reliability coefficients, it can be seen in the graph that the most commonly reported coefficient was the Cronbach's alpha, which accounted for `r apa_num(sum(table(coded_data_replications$N_items, coded_data_replications$reliability_type)["multiple item measure", "alpha"]), numerals = FALSE)` out of `r apa_num(sum(table(coded_data_replications$N_items, coded_data_replications$reliability_type)["multiple item measure", - c(1, 3)]), numerals = FALSE)` reported reliabilities in the replication protocols, and `r apa_num(sum(table(coded_data_original_shortened$N_items, coded_data_original_shortened$reliability_type)["multiple item measure", "alpha"]), numerals = FALSE)` out of `r apa_num(sum(table(coded_data_original_shortened$N_items, coded_data_original_shortened$reliability_type)["multiple item measure", - c(1, 3, 4)]), numerals = FALSE)` in the original articles.

```{r ReliabilityReportingFlowDiagram, fig.cap = "Reliability reporting flow diagram. Figure shows the number of measures as reported in both the replication protocols and original article, which meet the criterion in the box within the diagram and those criteria before it."}
knitr::include_graphics(path = "../../SupplementaryMaterials/reliability_reporting_flow_diagram.png")
```

If we look at the reliabilities obtained from the Many Labs responses on each measure for which scalar data could be accessed, the distributions of the Cronbach's alpha coefficients for each replication set for are displayed in Figure \@ref(fig:Plot23AlphaCode), separated by successful and unsuccessful replication. The average calculated Cronbach's alpha coefficient across replication sets was `r apa_num(mean(data_h3_multiple$alpha), digits = 3)` with a standard deviation of `r apa_num(sd(data_h3_multiple$alpha, na.rm = TRUE), digits = 3)`

```{r Plot23AlphaCode,warning = FALSE,  fig.cap = "Distributions of calculated Cronbach’s alpha coefficients (> 0) calculated for the responses on a measure at each lab location, across the eighteen distinct measures for which suitable raw data was available to calculate Cronbach’s alpha coefficients from. The green lines indicate the meta-analytic prediction interval lower and upper bound. The blue triangles indicate the reported alpha coefficient for that measure from the original article, when available. The Tau column besides the figure shows the tau heterogeneity estimate based on a meta-analysis of the calculated reliabilities for each measure, meta-analysis for which the Q-test for Heterogeneity was signicant at alpha < .05 are in black, not significant results are left grey. The Diff column shows the difference between reported reliability and the average reliability calculated from the Many Labs data for the applicable measures, the differences in bold are for reported reliabilities that fell outside the 95% quantile of calculated reliability scores."}

# data re-ordering (was not saved in export)
plot_23_data_alpha_reordered <- data_prep_plot_23_alpha(plot_23_data_alpha)

### use ggarange for aranging things at the side of this graph

# plot for alpha
ggplot(plot_23_data_alpha_reordered, aes(x = alpha, y = g)) +
  geom_boxplot(outlier.shape = NA) +
  geom_hline(yintercept = 6.5, color = "red", size = 1) +
  geom_point(alpha = 0.1) +
  
  # adding in the tau values
  geom_text(label = format(plot_23_data_alpha_reordered$tau, digits = 1), x = 1.15, size = 2.8, alpha = ifelse(plot_23_data_alpha_reordered$QEp < .05, 1, 0)) +
  geom_text(label = format(plot_23_data_alpha_reordered$tau, digits = 1), x = 1.15, size = 2.8, alpha = ifelse(plot_23_data_alpha_reordered$QEp >= .05, 1, 0), colour = "grey") +
  
  # adding in the alpha annotations
   annotation_custom(grob = textGrob(label = format(round(plot_23_data_reported_alpha$coefficient_difference[1], 2), nsmall = 2), hjust = 0, gp = gpar(fontsize = 8, fontface = ifelse(plot_23_data_reported_alpha$significance[1], "bold", "plain"))), ymin = 1, ymax = 1, xmin = 1.24, xmax = 1.24) +
  annotation_custom(grob = textGrob(label = format(round(plot_23_data_reported_alpha$coefficient_difference[2], 2), nsmall = 2), hjust = 0, gp = gpar(fontsize = 8, fontface = ifelse(plot_23_data_reported_alpha$significance[2], "bold", "plain"))), ymin = 2, ymax = 2, xmin = 1.24, xmax = 1.24) +
  annotation_custom(grob = textGrob(label = format(round(plot_23_data_reported_alpha$coefficient_difference[3], 2), nsmall = 2), hjust = 0, gp = gpar(fontsize = 8, fontface = ifelse(plot_23_data_reported_alpha$significance[3], "bold", "plain"))), ymin = 3, ymax = 3, xmin = 1.24, xmax = 1.24) +
  annotation_custom(grob = textGrob(label = format(round(plot_23_data_reported_alpha$coefficient_difference[4], 2), nsmall = 2), hjust = 0, gp = gpar(fontsize = 8, fontface = ifelse(plot_23_data_reported_alpha$significance[4], "bold", "plain"))), ymin = 5, ymax = 5, xmin = 1.24, xmax = 1.24) +
  annotation_custom(grob = textGrob(label = format(round(plot_23_data_reported_alpha$coefficient_difference[5], 2), nsmall = 2), hjust = 0, gp = gpar(fontsize = 8, fontface = ifelse(plot_23_data_reported_alpha$significance[5], "bold", "plain"))), ymin = 8, ymax = 8, xmin = 1.25, xmax = 1.25) +
  annotation_custom(grob = textGrob(label = format(round(plot_23_data_reported_alpha$coefficient_difference[6], 2), nsmall = 2), hjust = 0, gp = gpar(fontsize = 8, fontface = ifelse(plot_23_data_reported_alpha$significance[6], "bold", "plain"))), ymin = 9, ymax = 9, xmin = 1.24, xmax = 1.24) +
  annotation_custom(grob = textGrob(label = format(round(plot_23_data_reported_alpha$coefficient_difference[7], 2), nsmall = 2), hjust = 0, gp = gpar(fontsize = 8, fontface = ifelse(plot_23_data_reported_alpha$significance[7], "bold", "plain"))), ymin = 10, ymax = 10, xmin = 1.24, xmax = 1.24) +
  annotation_custom(grob = textGrob(label = format(round(plot_23_data_reported_alpha$coefficient_difference[8], 2), nsmall = 2), hjust = 0, gp = gpar(fontsize = 8, fontface = ifelse(plot_23_data_reported_alpha$significance[8], "bold", "plain"))), ymin = 11, ymax = 11, xmin = 1.24, xmax = 1.24) +
  annotation_custom(grob = textGrob(label = format(round(plot_23_data_reported_alpha$coefficient_difference[9], 2), nsmall = 2), hjust = 0, gp = gpar(fontsize = 8, fontface = ifelse(plot_23_data_reported_alpha$significance[9], "bold", "plain"))), ymin = 12, ymax = 12, xmin = 1.25, xmax = 1.25) +
  
  theme_minimal() +
  theme(legend.position = "none", plot.margin = unit(c(1, 6.5, 1, 1), "lines")) +
  
  # adding the necessary indicative texts
  annotation_custom(grob = textGrob(label = "Not Replicated", hjust = 0, gp = gpar(fontsize = 10)), ymin = 7.25, ymax = 7.25, xmin = 0.01, xmax = 0.01) +
  annotation_custom(grob = textGrob(label = "Replicated", hjust = 0, gp = gpar(fontsize = 10)), ymin = 6, ymax = 6, xmin = 0.01, xmax = 0.01) +
  annotation_custom(grob = textGrob(label = "Tau", hjust = 0, gp = gpar(fontsize = 12)), ymin = 20.2, ymax = 20.2, xmin = 1.08, xmax = 1.08) +
  annotation_custom(grob = textGrob(label = "Diff", hjust = 0, gp = gpar(fontsize = 12)), ymin = 20.2, ymax = 20.2, xmin = 1.24, xmax = 1.24) +
  
  coord_cartesian(xlim = c(0, 1), clip = "off") +
  
  # adding the blue triangles for reported reliability and green prediction intervals
  geom_point(data = plot_23_data_reported_alpha, mapping = aes(x = coefficient_reported, y = article_order), color = "blue", shape = 17, size = 3) +
  geom_point(mapping = aes(x = pi.lb), color = "green", shape = 124, size = 2.5) + 
  geom_point(mapping = aes(x = pi.ub), color = "green", shape = 124, size = 2.5) + 
  
  ylab("") +
  xlab("Cronbach's alpha") 
```

In order to visualize the results on QMPs, the proportions of the QMP ratio obtained for each QMP type (see Table \@ref(tab:QMPCodingInfoTable)) are shown across the top layer of Figure \@ref(fig:Plot456Code) for both replication protocols and original articles. While in the bottom layer of Figure \@ref(fig:Plot456Code) the overlayed distributions of the total QMP ratios from the replication protocols and original studies are displaued, with dots indicating each datapoint, and the line indicating the mean for either distribution. A large number of QMP ratios are missing for the QMP modification type , because only `r apa_num(table(coded_data_replications$mod_check)["True"])` out of `r apa_num(length(coded_data_replications$mod_check))` measures in replications, and only `r apa_num(table(coded_data_original_shortened$mod_check)["True"], numerals = FALSE)` measures used in original studies were modified. ???Cronbach's alpha of the QMP items???

```{r Plot456Code, warning = FALSE, fig.cap = "QMP ratio counts for each QMP Type and QMP total ratio distribution grouped by whether the QMP ratio was obtained from an original article or a replication protocol. The top row shows for each QMP type the proportions of each QMP ratio obtained, darker colours represent proportionally more QMPS, grey means modification did not occur for that measure. The bottom graph shows the distributions of total QMP ratios for both replication protocols and original articles, with the line indicating the mean QMP ratio. The specific observed values are indicated along the bottom row with dots"}
# re-ordering the QMP-types, because it didn't save
plot_456_data$QMP_type <- factor(plot_456_data$QMP_type, levels = c("Definition", "Selection", "Operationalization", "Quantification", "Modification", "Total"))


### ratio (REV) Original and Rep
Plot_456_1 <- ggplot(plot_456_data[plot_456_data$QMP_type != "Total",], 
                      aes(x = RepOrg, fill = as.factor(QMP_ratio_1_REV))) + 
  geom_bar(position = "fill") +
  theme_minimal() +
  scale_fill_manual(values = c("#ff766c", "#ef6f65", "#ce5f57", "#be5851", "#aa4f48", "#994741", "#803b36", "#672f2b", "#562824", "#411e1b", "#341816", "#030101","#4366ff", "#4265fc", "#3551cb", "#324dc1", "#324cbe", "#2d44aa", "#2c44a9", "#283d99", "#223380", "#162255", "#10193e", "#010206", "#000102")) +
  theme(legend.position = "none") +
  facet_grid(~ QMP_type) + 
  ylab("Relative Prevalence of\nQMP Ratio in Sample") +
  xlab("QMP Type") +
  ggtitle("QMP Ratio's Per QMP Type Grouped by Original and Replication")

Mean_Rat_Rep <- mean(plot_456_data$QMP_ratio_REV[
  plot_456_data$QMP_type == "Total" & plot_456_data$RepOrg == "Replication"])
Mean_Rat_Org <- mean(plot_456_data$QMP_ratio_REV[
  plot_456_data$QMP_type == "Total" & plot_456_data$RepOrg == "Original"])

Plot_456_2 <- ggplot(plot_456_data[plot_456_data$QMP_type == "Total",], 
       aes(x = QMP_ratio_REV, fill = RepOrg, colour = RepOrg)) +
  geom_density(alpha = 0.2) +
  geom_point(y = 0, alpha = 0.2, size = 3) +
  geom_segment(aes(x = Mean_Rat_Rep, xend = Mean_Rat_Rep, y = 0, yend = 2.66)
               , color = "blue4") +
  geom_segment(aes(x = Mean_Rat_Org, xend = Mean_Rat_Org, y = 0, yend = 2.183)
               , color = "brown") +
  coord_cartesian(xlim = c(0, 1)) +
  theme_ridges() + 
  theme(axis.text.y=element_blank(),
        axis.ticks.y=element_blank()) +
  scale_fill_manual(values=c("brown", "blue4")) +
  scale_color_manual(values=c("brown2", "blue")) +
  scale_x_continuous(breaks = c(0, Mean_Rat_Rep, 0.25, Mean_Rat_Org, 
                                0.5, 0.75, 1), 
                     labels = c("0", "0.18", "0.25", "0.31", "0.5", 
                                "0.75", "1")) + 
  ggtitle("Distribution of Total QMP Ratio's by Original and Replication") +
  guides(color = "none") +
  labs(fill = "Coded from") +
  ylab("") +
  xlab("") 

# combine both plots into one figure
grid.arrange(Plot_456_1, 
             Plot_456_2, 
             nrow = 2)

```

Table \@ref(tab:MeasurementReportingDescriptivesTable) shows additional descriptive information on measurement reporting based on counts of reported information that were not used to calculate QMP ratios. When it was not stated that existing materials were used, it was expected that the remaining articles would clearly specify that they newly created the materials, however this was not always the case for neither original articles (`r table(coded_data_original_shortened$sel_existing)[["False, materials were newly developped for this study"]]` out of `r nrow(coded_data_original_shortened) - table(coded_data_original_shortened$sel_existing)[["True, namely:"]]` relevant measures) nor replication protocols (`r table(coded_data_original_shortened$sel_existing)[["False, materials were newly developped for this study"]]` out of `rnrow(coded_data_replications) - table(coded_data_replications$sel_existing)[["True, namely:"]]` relevant measures).

```{r MeasurementReportingDescriptivesTable}
# construct the replication descriptives table to be printed
reporting_descriptives_table <- data.frame("Original Article" = c(length(coded_data_original_shortened$op_version[coded_data_original_shortened$op_version != ""]), table(coded_data_original_shortened$sel_existing)[["True, namely:"]], table(coded_data_original_shortened$sel_existing)[["False, materials were newly developped for this study"]], sum(coded_data_original_shortened$sel_psychometric_evidence_REV != "None" & coded_data_original_shortened$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)"), table(coded_data_original_shortened$mod_check_REV)[["True"]], sum(coded_data_original_shortened$mod_time == "After/During"), nrow(coded_data_original_shortened)), 
                                             "Replication Protocol" = c(length(coded_data_replications$op_version[coded_data_replications$op_version != ""]), table(coded_data_replications$sel_existing)[["True, namely:"]], table(coded_data_replications$sel_existing)[["False, materials were newly developped for this study"]], sum(coded_data_replications$sel_psychometric_evidence_REV != "None" & coded_data_replications$sel_psychometric_evidence_REV != "Not Apllicable (only report this if psychometric evidence would not be possible for this measure, otherwise report as None)"), table(coded_data_replications$mod_check_REV)[["True"]], sum(coded_data_replications$mod_time == "After/During"), nrow(coded_data_replications)))


# Changing rownames to reflect the variables
rownames(reporting_descriptives_table) <- c("Specified measure version", "Used existing materials", "Specified being newly created", "Reported psychometric evidence", "N modified", "Modified after/during data collection", "Total")

# Changing column names to be less robot speak
colnames(reporting_descriptives_table) <- c("Original Article", "Replication Protocol")


# converting columns to string to then add a superscript to one of the elements
# in the table.
reporting_descriptives_table$"Replication Protocol" <- as.character(reporting_descriptives_table$"Replication Protocol")

# adding a superscript for a later footnote
reporting_descriptives_table$"Replication Protocol"[3] <- "$1^a$"

  
# print the table in apa formatting
apa_table(
  reporting_descriptives_table, align = c("l", "r", "r")
  , caption = "Descriptives of Measurement Reporting Variables not used for calculating QMP Ratios for both Original Articles and Replication Protocols"
  , note = "$^a$ the relevant question asking participants to rate how much they agreed with a certain quote remained the same, however the quote itself and the people it was attributed too were both completely new compared to the original"
  , escape = FALSE, placement = "htp", midrules = 6)

```

Finally, table \@ref(tab:ReplicationRatioTable) displays the proportion of replication success based on the criteria of a meta-significance cutoff smaller than .05 per Many Labs project.

```{r ReplicationRatioTable, warning = FALSE}
# construct the replication ratio table to be printed
replication_ratio_table <- data.frame("Many Labs Version" = c("1", "2", "3", "5", "Total"),
                                      "N Total" = c(table(coded_data_replications$many_labs_version), nrow(coded_data_replications)), 
                                      "N Replicated" = c(table(coded_data_replications$many_labs_version, coded_data_replications$hypothesis_support)[, "Yes"], table(coded_data_replications$hypothesis_support)["Yes"]))

# calculate the replication ratio from the N measures whose studies were replicated
# and the N total measures, per Many Labs study.
replication_ratio_table$"Replication Ratio" <- round(replication_ratio_table$N.Replicated / replication_ratio_table$N.Total, digits = 2)

# Changing column names to be less computer speak-y looking
colnames(replication_ratio_table) <- c("Many Labs Version", "N Total", "N Replicated",
                                       "Replication Ratio")

# converting columns to string to then add a superscript to one of the elements
# in the table.
replication_ratio_table$"N Replicated" <- as.character(replication_ratio_table$"N Replicated")

# adding a superscript for a later footnote
replication_ratio_table$"N Replicated"[3] <- "$3^a$"

  
# print the table in apa formatting
apa_table(
  replication_ratio_table, align = c("l", "r", "r", "r")
  , caption = "Ratio of measures for which the effect was considered replicated, across many labs projects"
  , note = "$^a$replication was assessed as unclear for three of the measures, since their effect was not fully replicated. These have been treated as not replicated within this table and further analyses throughout the article"
  , escape = FALSE, placement = "htp", midrules = 4)

```

## Reliability
Figure \@ref(fig:ReliabilityReportingFlowDiagram) illustrates that multiple item measures are most commonly not accompanied by a reported reliability coefficient in neither original articles (`r sum(coded_data_original_shortened$reliability_type[coded_data_original_shortened$N_items == "multiple item measure"] != "Not Reported")` out of `r sum(coded_data_original_shortened$N_items == "multiple item measure")` multiple item measures) nor replication protocols (`r sum(coded_data_replications$reliability_type[coded_data_replications$N_items == "multiple item measure"] != "Not Reported")` out of `r sum(coded_data_replications$N_items == "multiple item measure")` multiple item measures). Additionally, when comparing between the two using a chi square comparison, it was observed that reliability coefficients are reported significantly less often in replication protocols than in original articles ($\chi^{2}$(`r apa_num(H1_test_result$parameter)`) = `r apa_num(H1_test_result$statistic)`, *p* = `r apa_p(H1_test_result$p.value)`). These results are contradictory to hypothesis 1, which predicted the opposite.

The ICC of the between-group (across different replication set) variance and within-group (lab locations within a replication set) variance in the Cronbach's alpha coefficient was approximately `r apa_num(ICC_alpha, digits = 3)`. Because this number is larger than .5, it suggests that variance was larger between replication sets than within. 

A more sophisticated, albeit not pre-registered, exploratory analysis was also performed using a meta-analysis of the calculated reliability coefficients. The tau score (the indicator of true differences in reliability scores between labs) is displayed for each replication set's calculated Cronbach's alpha coefficients in Figure \@ref(fig:Plot23AlphaCode). The tau scores vary considerably between replication sets, some had a Tau estimate of approximately 0, while others were above .1. The latter result implies that for the measurements on that replication set, the standard deviation of the distribution of true reliability coefficients between the replications from various lab locations was more than .1 point of Cronbach's alpha [@cooper2019handbook]. Based on the Q-test for heterogeneity, `r sum(data_h3_avg$QEp < .05)` out of the total of `r nrow(data_h3_avg)` estimates of heterogeneity were significant. 

However, since the Q-test is sensitive to the number of studies in the meta-analyses [@li2015DilemmaHeterogeneityTests], some of the discrepancy in the estimate of heterogeneity based on this test may in part be due to the fact that for some measures the sample size of labs that used them was too low. Therefore, while consistent evidence for heterogeneity would imply true variance in Cronbach's alpha within replication sets would run counter to hypothesis 2, the evidence id inconsistent when looking across all studies, and remains sensitive to the sample size in the meta-analyses.

Figure \@ref(fig:Plot23AlphaCode) further shows whether the replication sets were successful when judged at a meta-analytic significance level of .05, or not. A logistic regression was performed to detect if the calculated reliabilities related to the replication success.

The logistic model indicated that Cronbach's alpha did not significantly relate to replication success in the main logistic regression model (Cronbach's alpha: `r H3_main_full_results_alpha_with_OR`). Similar results were found based on the calculated omega coefficient (see Appendix C) and in all of the multilevel sensitivity analyses models (see Appendix D). These results are contradictory to the expectation in hypothesis 3. However, the reliability coefficient could be calculated for only a small sample number of measures. As a result, the estimates of the relation between reliability and replication success obtained using these models each come with large uncertainty and remains inconclusive. 

## Questionable Measurement Practices

The bottom pannel of Figure \@ref(fig:Plot456Code) already shows that original articles contained a significantly larger proportion of QMPs than replication protocols for the measures of the same effects. Beta-regression was used to test the difference in QMP ratio between original articles and replication protocols. The results of indicated that this difference was significant (`r betareg_output_to_apa_full(H4_test_results_REV)`). This result is in line with hypothesis 4.

A logistic regression model was used to test the association between the QMP ratio in replication protocols and replication success. Results showed that a decrease in the ratio of QMPs in replication protocols significantly related to successful replication (`r H5_test_results_REV_with_OR`). 

```{r Plot46RevisedDataCode, error = FALSE, warning = FALSE, fig.cap = "Scatterplot of original and replication total QMP ratio’s with linear regression line. Each dot in the figure describes the QMP ratio in that graph across both the original article and its replication protocol. Note: jitter was applied to these dots in order to show the number of observations at points where multiple dots were present"}
ggplot(plot_46_data_rev[plot_46_data_rev$QMP_type == "QMP.Ratio" & plot_46_data_rev$QMP_Rep_type == "Rep.QMP.Ratio",], aes(QMP_ratio, QMP_Rep_ratio)) + 
  geom_point() +
  geom_jitter(width = 0.01, height = 0.01) +
  stat_smooth(method="lm") + 
  theme_minimal() +
  theme(strip.text.y = element_text(size = 7), panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = "black")) +
  ylab("QMP Ratio (Replication Protocol)") +
  xlab("QMP Ratio (Original Article)") 
```

The relation between QMP ratios in the replication protocols and corresponding original articles were investigated using a beta regression model. It showed that total QMP ratio in the original article can significantly relates to the total QMP ratio in the subsequent replication (`r betareg_output_to_apa_full(H6_test_results_REV)`), which provides some evidence in favor of hypothesis 6. Figure \@ref(fig:Plot46RevisedDataCode) displays this relationship visually.

# Discussion

In the current study, measurement error and QMPs were assessed in both replications and original research. These measurement related concepts were then linked to replicability in order to investigate them as potential causes inhibiting successful replications. The results of the tests for the hypotheses were often underpowered, especially the tests of hypotheses 2 & 3 for which only `r nrow(data_h3_avg)` measures were available to calculate reliability from. Additionally, most tests showed mixed results. However, when including exploratory and descriptive analyses, a clearer picture on measurement in replication and original research emerges.

## Risky Measurement Usage

??? Results based on calculated reliability are somewhat unstable, but we can definitely say that a large number of single item measures are used, not many multiple item measures are accompanied by a reported alpha coefficient, and when looking at the heterogeneity and selective reporting of alpha we see this...???

This study assessed the quality of measurement and measurement reporting in original and replication research by investigating the quality of measurement in the Many Labs projects, and the original articles they attempted to replicate. Findings were in line with similar investigations in the literature by @shaw2020measurement & @flake2022construct. These two studies found that validity evidence and reliability coefficient reporting for measures was lacking in replications and original studies. lllnot enough reliability coefficients were reported to conduct the planned analysis for hypothesis 1, and of the 77 measures only X measures in original articles and X in replication protocol reported any validity evidence. These findings show that reliability and validity evidence was similarly underreported in our sample, a problem that has also been addressed in earlier literature on this topic [@vacha1999practices; @green2011recent].

The adjusted test for hypothesis 1, while not able to test differences in actual coefficient values, did show contradictory to hypothesis 1 and similarly to @flake2022construct, that original articles reported reliability coefficients more often than replication studies. Still, even for original articles a little over two-thirds of the multiple item measures were also reported without reliability evidence. ???Interestingly enough, when looking at the measures for which alpha could be calculated, alpha coefficients were reported only for those measures that were among the higher calculated mean alpha among the sample of measures for which alpha could be calculated as can be seen in figure \@ref(fig:Plot23AlphaCode)???

This only covers part of the spectrum of the measures found in the sample, across both original and replication studies, less than half of the coded measures were multiple item scales. Research commonly made use of single item measures (see Figure \@ref(fig:ReliabilityReportingFlowDiagram) for more details), an observation that was also made by @shaw2020measurement. The use of these single item measures comes with risks, since by virtue of being singular they assume that whatever they are measuring is unidimensional, while not being able to test this assumption [@shaw2020measurement], ???and reliability risks???. This is particularly concerning given that the analyses of the calculated reliabilities of the multiple item measures showed that some measures ???EXPLORATORY ANALYSIS IDEA: relation of the calculated reliabilities and N items of the measure???, and exploratory analyses of the multiple item measure replication datas, for which unidimensionality could be tested, showed only little support for unidimensionality (see Appendix A as was also observed by @shaw2020measurement).

???APPENDIX with SENSITIVITY ANALYSES/UNIDIMIMENSIONALITY ANALYSIS???: This is potentially quite worrying given that the exploratory analyses of the multiple item measures, for which unidimensionality could be tested, showed only little support for unidimensionality of the measures used in replications. While the measures that were tested for unidimensionality were mostly used as single indicators in the analyses for the replication protocol, one could argue this does not necessarily mean these measures all assessed constructs with a unidimensional structure. For example, IQ is often used as a singular indicator, even though it consists of several factors [@weiss2013wisc]. However, it should be noted that at least with respect to Many Labs 2, all of the measures that were considered unidimensional for the analyses in this study were also considered unidimensional in the investigation by @shaw2020measurement} who also observed a lack of unidimensionality evidence in their investigation of Many Labs 2. To conclude, there is an indication that validity evidence for measures used in replication research is lacking overall.]]]

Several results did show more promising findings for the way measurement related issues are treated in replications. Firstly, the support found for hypothesis 2 provided some evidence showing that most of the variability in reliability coefficients is between measures, rather than within measures across different locations. This would be a prerequisite showing that the measurement operated similarly across these locations, which would make the scores from the different lab locations directly comparable and aggregatable. However, the small sample number of measures for which reliability could be assessed does mean that this estimate remains uncertain. Secondly, most replication protocols clearly stated that their measure was either newly created or based on existing materials. Still, in most instances neither original nor replication provided information on the exact version of their measure. This may largely be due to the fact that many of the used measures were single-item, for which specifying the exact versions may not be seen as crucial because they more or less describe themselves. Finally, QMPs were overall less common in replication protocols than they were in original articles, supporting hypothesis 4. This runs counter to the findings of @shaw2020measurement & @flake2022construct, who found that measures came with less validity evidence in replications than in original studies, as the result of QMPs.

The structured replication protocols that were used in the Many Labs projects, I suspect improved the reporting practices and measurement consistency in replication protocols compared to original articles. Most replication protocols even contained a section under each replicated study, which listed any deviations from the original and as a result many potential QMPs were averted. [@protzko2020high] already demonstrated that transparent reporting of methodology before conducting research can improve the robustness of psychological science, and I believe the same holds true here.

## Impact of reliability on replicability

Contrary to expectations (see Hypothesis 3), less reliable measures were not associated with a decrease in replicability. This result is surprising, since @stanley2014expectations demonstrated the potential impact that measurement error can have on the veracity of determining replication success based on a significance test without attenuating for measurement error. The significance test cut-off criteria that was used in this study to assess replication success similarly did not attenuate for measurement error, and thus results in line with the simulation studies of @stanley2014expectations were expected.

While no evidence for hypothesis 3 was found, caution in interpreting the test results is needed. The number of measures for which reliability was calculated was quite low (`r apa_num(sum(!is.na(data_h3_avg$alpha)), numerals = FALSE)`), it could be that the test was simply underpowered to detect the effect. On the other hand, it might be that the detrimental influence of measurement error on interpretations of replication results is small in large scale replication projects. Across the multiple replications, measurement error's influence on the effect could have been cancelled out at the meta-analytic level. This would require the measurement error to be random and small, and as a result the average value of the effect and the error around would remain similar, thus not altering the result of the meta-analytic significance test ???logic/cited work???.

[[[??? reliability meta-analysis???

## Impact of QMPs on replicability

Results regarding the relationship between QMPs and replicability were more in line with expectations. In particular, support was found for hypothesis 5, showing that an increase in total QMP ratio was associated with a decrease in replicability. Existing literature has already warned about the potential detrimental effects of QMPs on replications [@shaw2020measurement; @flake2022construct]. This result provides further evidence supporting these claims.

The effect of QMPs on replicability can additionally be traced back to the effect of original article QMPs on replication QMPs. The test for hypothesis 6 showed that, in line with expectations, there was a positive association between the total QMPs of an original study and the total QMPs in the protocol of the replication for that study. The idea here is that when information on or support for the use of a measurement is lacking, any subsequent replication attempt is expected to also lack information on the measure. As a result, both the underreporting of measurement details and use of unverified measurements may persist. [make the domino effect more clear/better written here]

Furthermore, ensuring that the original measurement is mimicked becomes difficult to verify when the measurement was not documented well originally. As a result, the relation between the original and replication studies weakens, lowering the chances that similar results will be found, and possibly even invalidating the replication's relation to the original study as a replication [@flake2022construct].

However, it is worth noting that hese relations were only found for QMPs coded with the revised coding protocol. Neither hypothesis was supported by the data on QMPs obtained with the initial coding protocol (see Appendix D). The initial protocol was revised because the ratings based on that protocol were considered to stringent, and as a result, several items were reformulated so that criteria for what constituted sufficiently clear reporting were lowered. This meant that in the revised protocol, coded QMPs would need to clearly inhibit evaluation of the measurement, whereas for the initial protocol some aspects of the measurement reporting which were coded as questionable, may have been questionable only to a small degree. As a consequence, QMPs coded with the revised protocol represent measurement practices that represent on average more questionable practices than QMPs coded with the initial protocol. With this in mind, the difference between total QMP ratio of initial and revised protocol can be seen as a form of effect size difference. It could be that only replication QMPs that represented robust violations of good measurement practices significantly lower the probability of successful replication. In that case, a stronger relation between replicability and QMPs would be found with the revised protocol, since these QMPs represented more clearly questionable practices in measurement reporting than QMPs coded with the initial protocol, which would explain the disparate results that were found.

## Limitations & Future Research

The design of this study came with certain limitations. First of all, there was no (quasi-)experimental manipulation included of the quality or reporting practices of the measures. As a result, the ability to investigate the exact causal links between QMPs and replicability was limited. Secondly, alternative ways to analyse the data could have been used, or certain parts of the data could have been examined more closely. Several methodologies for doing so have already been suggested throughout the article such as ???insert methodology here???, and others are presented in the appendix ???sensitvity and exploratory analysis appendix???. Future research can use this study as a basis to find relevant data sources and methodologies to use in order to compliment and extend the findings of this study, or test causal links between measurement usage and replicability.

It is important to also consider whether or not replication protocols and research articles can be fairly compared in terms of QMPs. Articles are generally longer than a study's description within a replication protocol, and as a result it seems reasonable to expect research articles to generally contain more measurement details than replication protocols. Regardless, this study assumed that a replication protocol is comparable to an article in providing an overview of the measure and how it was used, as it is the only comparable source for such information that would otherwise be difficult to find anywhere else. This assumption seems practical if not tenable when the replication protocol is the only available sources of measurement details. However, in case more detailed information on a measurement is available in other sources, such as an online appendix, it could be important data to include when assessing QMPs. This research did not make use of any online appendices in assessing QMPs, future research may wish to include them and other sources as part of the data. \<-- try to collpase with previous paragraph???

Another limitation of this study is that the results differed based on whether data from the revised or the initial coding protocol was used. The revised coding protocol was adapted from the initial coding protocol before any analyses were performed, it is however still possible that a change in protocol may have biased results. Future research investigating this topic should use a coding protocol which is verified independently by several researchers on a sample of articles and protocols first, before being used to code the QMP data that will be analysed. As a first step, the coding protocol from this study and the coding key from @flake2022construct can be used as a basis. \<- importance ???

As a last note on QMPs, it is worth highlighting the way the QMP ratios were calculated. The total QMP ratio was based on the counts for each of the items from all the different QMP types divided by the total number of applicable questions. However, for each QMP type the number of items was different. This means that QMP types with a larger number of items (such as operationalisation) had a greater impact on the total QMP ratio than QMP types with a smaller number of items (such as definition). If the impact for each QMP type on each other and replicability is considered equally important, the total QMP ratio may not be considered a valid indice. On the other hand, it could be that each item should be considered equally important rather than each QMP type. For this study, the protocol was made and the analyses were conducted under the assumption that each item had equal weight. However, this is an assumption this study cannot provide evidence for. [Focus more on the fact that there is a mixture in the QMP ratio’s: which influences the relations perhaps like ...]

The small sample of measures for which reliability coefficients and unidimensional factor structure could be assessed limited the degree to which substantial interpretations can be made from the reliability and validity analyses. While future research will also unlikely be able to increase the number of multiple item measures used by replications, future research can broaden their analyses to include a larger number of replications, or take into account a type of measure which this study mostly ignored in the analyses of reliability and validity: single-item measures.

Reliability assessment for single item measures differs substantially from multiple item measures [@wanous1996estimating], and for this reason was not included as part of the analyses for this study. Furthermore, single items cannot be tested on unidimensionality, since a single item is already assumed to measure only a single dimension by definition. However, future research might consider methods specifically designed to assess reliability and validity of single item measures. This could prove to be a worthwhile contribution, given the proportionally large number of single item measures that were encountered in the data for this study (see Figure \@ref(fig:ReliabilityReportingFlowDiagram)).

Finally, this paper investigated the consistency of measures, by assessing the variance in reliability across different labs wherein the measure was conducted, and comparing it to the variance in reliability between the different measures. Besides comparing the degree of reliability across labs, it is also possible to assess whether a measure assesses the same underlying construct across lab locations. This can be done through measurement invariance testing. Measurement invariance is a concept in latent factor modelling that illustrates the extent to which the relationship between items and factors is dependent on some other variable, usually a group [@mellenbergh1989item]. @shaw2020measurement investigated measurement invariance for the responses to measures at each lab location within the replication sets of Many Labs 2. Investigations similar to this have also been suggested for future replication projects [@flake2022construct]. For these reasons, future research should consider measurement invariance tests, if assessing measurement consistency in replications across different labs. \<-- also include other suggestions like for paragraph 1 of this section???

## Implications for Research and Replications in Psychological Science

Taking all the findings together, the following assessments will be relevant to consider for future replication research. With respect to the quality of measurement reporting, the small number of reported reliabilities, small number of reported psychometric validity evidence, and lack of unidimensionality evidence for data aggregated to a single indicator corroborate findings by @shaw2020measurement & @flake2022construct. Evidence supporting the measures used in replication should be reported more frequently, so that they can be evaluated and the validity of replication research can be improved.

There are additional validity issues in measurement as the result of QMPs. While it is debatable whether or not $17.9\%$ [add code for this percentage] average QMP ratio in replications is too much, the association between original paper and replication protocol QMPs demonstrates how a lack of information for measurement in the original can spill over to the reporting in replications. This could lead to issues for replication attempts if the measure in an original article has poor reliability and validity. In that case, the tests in the original article are based on data from measures that do not accurately capture the construct they were intended to measure. The results of any direct replication attempt will face the same issues, and as a consequence the replication is unlikely to provide a relevant test of a psychological theory, as the original study itself was not a robust test of that psychological theory to begin with [@isager2020deciding, @isager2021deciding, @nuijten2022assessing]. Thus, future replicators should first evaluate the quality of the measurement (reporting) in an article before deciding to replicate it. \<-- work this narrative into the discussion section for QMP -\> replicability???

The results also suggest that the use of structured protocols in the reporting of measurement may be an effective way of reducing QMPs, and increasing the reporting of reliability and validity evidence. However, even in the Many Labs projects, which made use of structured protocols to report their measures, low reliability and validity evidence reporting was found. These protocols could therefore still increase the attention they give to reporting reliability and validity evidence. When original articles do not report their measurement details, replication attempts cannot guarantee a direct replication of the original. By improving the quality of measurement reporting in original articles, the validity of both original research results and future replication attempts would increase. It would be particularly important to focus on remedying strong violations of good measurement reporting, as these were shown to have a significant detrimental impact on replicability. This suggests that even small but critical improvements in the reporting of measurement information could lead to a substantial increase in the replicability of psychological science. \<-- work into paragraph above and shorten heavily... paragraph 2 is about the importance of structured protocols. Paragraph 3 about the need to consider the options when replicating a study.

In case the quality appears low, replication attempts can still proceed with some adjustments. One option is to change the focus of the replication to be conceptual rather than direct. In that case the measure and the operationalisation can be freely altered from the original, which means that a more sufficient test of the theory can be developed. If deemed necessary, a direct replication can always be performed later based on the conceptual replication. In that case, the direct replication will be able to provide a sufficient test of a psychological theory, on the condition that the conceptual replication's test improvements led to it becoming a sufficient test. \<-- less direct vs. conceptual???

# Conclusion

Overall, the observed quality of measurement and frequency of measurement reporting in psychological research and replications was low, which is in line with other research on the topic. Improvements in the reporting on measurement are needed for both original and replication research. The measure source, content, and how it was used should be described in detail, and evidence illustrating the reliability and validity of the measure should be provided. Not only would the clarity of research improve, higher quality of measurement reporting in replications was shown to be related to an increase in replicability of the findings related to that measure. Tentative evidence was also found for spill-over effects of the quality of measurement reporting from original study to replications. These results combined suggest that improvements in the reporting of measurements are necessary in both replications and original articles. Furthermore, future replicators should carefully consider the quality of measurement in a study before attempting a direct replication, even considering a conceptual replication instead if the measurement in the original was not of sufficient quality. However, the evidence for the relationship between measurement quality and replicability was not robustly identified, and future research will be needed to confirm the exact nature of this relationship.

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
