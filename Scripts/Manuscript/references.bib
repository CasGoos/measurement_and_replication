@article{vanlissaWORCS2021,
  title = {{{WORCS}}: {{A}} Workflow for Open Reproducible Code in Science},
  author = {Van Lissa, Caspar J. and Brandmaier, Andreas M. and Brinkman, Loek and Lamprecht, Anna-Lena and Peikert, Aaron and Struiksma, Marijn E. and Vreede, Barbara M.I.},
  year = {2021},
  journal = {Data Science},
  volume = {4},
  number = {1},
  pages = {29--49},
  publisher = {{IOS Press}},
  issn = {2451-8492},
  doi = {10.3233/DS-210031}
}

@book{americaneducationalresearchassociationStandardsEducationalPsychological2014,
  title = {Standards for Educational and Psychological Testing},
  author = {{American Educational Research Association} and {American Psychological Association} and {National Council on Measurement in Education}},
  year = {2014},
  publisher = {American Educational Research Association},
  address = {Washington, DC},
  isbn = {978-0-935302-35-6},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\YVY478GP\American Educational Research Association et al. - 2014 - Standards for educational and psychological testin.pdf}
}

@article{anvariDefragmentingPsychology2025,
  title = {Defragmenting Psychology},
  author = {Anvari, Farid and Alsalti, Taym and Oehler, Lorenz A. and Hussey, Ian and Elson, Malte and Arslan, Ruben C.},
  year = {2025},
  month = may,
  journal = {Nature Human Behaviour},
  volume = {9},
  number = {5},
  pages = {836--839},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-025-02138-0},
  urldate = {2025-06-23},
  abstract = {Psychology is fragmented into the study of a myriad of constructs and measures, most of which are used very rarely. This hinders cumulative knowledge generation. We call on the field to defragment psychology and prevent further fragmentation. We provide four key recommendations to achieve this and summarize the needed actions.},
  copyright = {2025 Springer Nature Limited},
  langid = {english},
  keywords = {found:peers,measurement,method:proposal,validity},
  file = {C:\Users\u686785\Zotero\storage\W3HLRR9X\Anvari et al_2025_Defragmenting psychology.pdf}
}

@misc{anvariFragmentedFieldConstruct2025,
  title = {A Fragmented Field: {{Construct}} and Measure Proliferation in Psychology},
  shorttitle = {A Fragmented Field},
  author = {Anvari, Farid and Alsalti, Taym and Oehler, Lorenz and Marion, Zach and Hussey, Ian and Elson, Malte and Arslan, Ruben},
  year = {2025},
  month = jun,
  publisher = {OSF},
  doi = {10.31234/osf.io/b4muj_v3},
  urldate = {2025-08-19},
  abstract = {We examined the extent to which constructs and measures have proliferated in psychological science. We integrated two large databases obtained from the American Psychology Association (APA) that they have used to keep track of constructs, measures, and research in the psychological science literature for the past 30 years. Our descriptive analyses finds that (i) thousands of new constructs and measures are published each year, (ii) most measures are used very few times, and (iii) there is no trend towards consensus or standardization in the use of constructs and measures; in fact, there is a trend towards even greater fragmentation over time. That is, constructs and measures are proliferating. We conclude that measurement in the psychological science literature is fragmented, creating problems such as redundancy and confusion, and stifling cumulative scientific progress. We conclude by providing suggestions for what researchers can do about this problem.},
  archiveprefix = {OSF},
  langid = {american},
  file = {C:\Users\u686785\Zotero\storage\4CPUAYR8\Anvari et al_2025_A fragmented field.pdf}
}

@article{ashbaughPsychometricValidationEnglish2016,
  title = {Psychometric {{Validation}} of the {{English}} and {{French Versions}} of the {{Posttraumatic Stress Disorder Checklist}} for {{DSM-5}} ({{PCL-5}})},
  author = {Ashbaugh, Andrea R. and {Houle-Johnson}, Stephanie and Herbert, Christophe and {El-Hage}, Wissam and Brunet, Alain},
  editor = {Mazza, Marianna},
  year = {2016},
  month = oct,
  journal = {PLOS ONE},
  volume = {11},
  number = {10},
  pages = {e0161645},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0161645},
  urldate = {2023-02-16},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\Z769JZCJ\Ashbaugh et al. - 2016 - Psychometric Validation of the English and French .pdf}
}

@article{barryValidityReliabilityReporting2014,
  title = {Validity and {{Reliability Reporting Practices}} in the {{Field}} of {{Health Education}} and {{Behavior}}: {{A Review}} of {{Seven Journals}}},
  shorttitle = {Validity and {{Reliability Reporting Practices}} in the {{Field}} of {{Health Education}} and {{Behavior}}},
  author = {Barry, Adam E. and Chaney, Beth and {Piazza-Gardner}, Anna K. and Chavarria, Enmanuel A.},
  year = {2014},
  month = feb,
  journal = {Health Education \& Behavior},
  volume = {41},
  number = {1},
  pages = {12--18},
  publisher = {SAGE Publications Inc},
  issn = {1090-1981},
  doi = {10.1177/1090198113483139},
  urldate = {2024-06-26},
  abstract = {Health education and behavior researchers and practitioners often develop, adapt, or adopt surveys/scales to quantify and measure cognitive, behavioral, emotional, and psychosocial characteristics. To ensure the integrity of data collected from these scales, it is vital that psychometric properties (i.e., validity and reliability) be assessed. The purpose of this investigation was to (a) determine the frequency with which published articles appearing in health education and behavior journals report the psychometric properties of the scales/subscales employed and (b) outline the methods used to determine the reliability and validity of the scores produced. The results reported herein are based on a final sample of 967 published articles, spanning seven prominent health education and behavior journals between 2007 and 2010. Of the 967 articles examined, an exceedingly high percentage failed to report any validity (ranging from 40\% to 93\%) or reliability (ranging from 35\% to 80\%) statistics in their articles. For health education/behavior practitioners and researchers to maximize the utility and applicability of their findings, they must evaluate the psychometric properties of the instrument employed, a practice that is currently underrepresented in the literature. By not ensuring the instruments employed in a given study were able to produce accurate and consistent scores, researchers cannot be certain they actually measured the behaviors and/or constructs reported.},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\XWM6WNXH\Barry et al. - 2014 - Validity and Reliability Reporting Practices in th.pdf}
}

@article{bartlettLinearMixedModels2009,
  title = {Linear Mixed Models for Replication Data to Efficiently Allow for Covariate Measurement Error},
  author = {Bartlett, Jonathan W. and De Stavola, Bianca L. and Frost, Chris},
  year = {2009},
  journal = {Statistics in Medicine},
  volume = {28},
  number = {25},
  pages = {3158--3178},
  issn = {1097-0258},
  doi = {10.1002/sim.3713},
  urldate = {2023-02-16},
  abstract = {It is well known that measurement error in the covariates of regression models generally causes bias in parameter estimates. Correction for such biases requires information concerning the measurement error, which is often in the form of internal validation or replication data. Regression calibration (RC) is a popular approach to correct for covariate measurement error, which involves predicting the true covariate using error-prone measurements. Likelihood methods have previously been proposed as an alternative approach to estimate the parameters in models affected by measurement error, but have been relatively infrequently employed in medical statistics and epidemiology, partly because of computational complexity and concerns regarding robustness to distributional assumptions. We show how a standard random-intercepts model can be used to obtain maximum likelihood (ML) estimates when the outcome model is linear or logistic regression under certain normality assumptions, when internal error-prone replicate measurements are available. Through simulations we show that for linear regression, ML gives more efficient estimates than RC, although the gain is typically small. Furthermore, we show that RC and ML estimates remain consistent even when the normality assumptions are violated. For logistic regression, our implementation of ML is consistent if the true covariate is conditionally normal given the outcome, in contrast to RC. In simulations, this ML estimator showed less bias in situations where RC gives non-negligible biases. Our proposal makes the ML approach to dealing with covariate measurement error more accessible to researchers, which we hope will improve its viability as a useful alternative to methods such as RC. Copyright {\copyright} 2009 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {C\:\\Users\\u686785\\Zotero\\storage\\QCMXVCPG\\Bartlett et al. - 2009 - Linear mixed models for replication data to effici.pdf;C\:\\Users\\u686785\\Zotero\\storage\\M4MIH2UH\\sim.html}
}

@article{beckmanHowReliableAre2004,
  title = {How Reliable Are Assessments of Clinical Teaching? {{A}} Review of the Published Instruments},
  shorttitle = {How Reliable Are Assessments of Clinical Teaching?},
  author = {Beckman, Thomas J. and Ghosh, Amit K. and Cook, David A. and Erwin, Patricia J. and Mandrekar, Jayawant N.},
  year = {2004},
  month = sep,
  journal = {Journal of General Internal Medicine},
  volume = {19},
  number = {9},
  pages = {971--977},
  issn = {0884-8734},
  doi = {10.1111/j.1525-1497.2004.40066.x},
  abstract = {BACKGROUND: Learner feedback is the primary method for evaluating clinical faculty, despite few existing standards for measuring learner assessments. OBJECTIVE: To review the published literature on instruments for evaluating clinical teachers and to summarize themes that will aid in developing universally appealing tools. DESIGN: Searching 5 electronic databases revealed over 330 articles. Excluded were reviews, editorials, and qualitative studies. Twenty-one articles describing instruments designed for evaluating clinical faculty by learners were found. Three investigators studied these papers and tabulated characteristics of the learning environments and validation methods. Salient themes among the evaluation studies were determined. MAIN RESULTS: Many studies combined evaluations from both outpatient and inpatient settings and some authors combined evaluations from different learner levels. Wide ranges in numbers of teachers, evaluators, evaluations, and scale items were observed. The most frequently encountered statistical methods were factor analysis and determining internal consistency reliability with Cronbach's alpha. Less common methods were the use of test-retest reliability, interrater reliability, and convergent validity between validated instruments. Fourteen domains of teaching were identified and the most frequently studied domains were interpersonal and clinical-teaching skills. CONCLUSIONS: Characteristics of teacher evaluations vary between educational settings and between different learner levels, indicating that future studies should utilize more narrowly defined study populations. A variety of validation methods including temporal stability, interrater reliability, and convergent validity should be considered. Finally, existing data support the validation of instruments comprised solely of interpersonal and clinical-teaching domains.},
  langid = {english},
  pmcid = {PMC1492515},
  pmid = {15333063},
  keywords = {measurement reporting,reliability reporting}
}

@article{borensteinAvoidingCommonMistakes2024,
  title = {Avoiding Common Mistakes in Meta-Analysis: {{Understanding}} the Distinct Roles of {{Q}}, {{I-squared}}, Tau-Squared, and the Prediction Interval in Reporting Heterogeneity},
  shorttitle = {Avoiding Common Mistakes in Meta-Analysis},
  author = {Borenstein, Michael},
  year = {2024},
  journal = {Research Synthesis Methods},
  volume = {15},
  number = {2},
  pages = {354--368},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1678},
  urldate = {2025-08-08},
  abstract = {In any meta-analysis, it is critically important to report the dispersion in effects as well as the mean effect. If an intervention has a moderate clinical impact on average we also need to know if the impact is moderate for all relevant populations, or if it varies from trivial in some to major in others. Or indeed, if the intervention is beneficial in some cases but harmful in others. Researchers typically report a series of statistics such as the Q-value, the p-value, and I2, which are intended to address this issue. Often, they use these statistics to classify the heterogeneity as being low, moderate, or high and then use these classifications when considering the potential utility of the intervention. While this practice is ubiquitous, it is nevertheless incorrect. The statistics mentioned above do not actually tell us how much the effect size varies. Classifications of heterogeneity based on these statistics are uninformative at best, and often misleading. My goal in this paper is to explain what these statistics do tell us, and that none of them tells us how much the effect size varies. Then I will introduce the prediction interval, the statistic that does tell us how much the effect size varies, and that addresses the question we have in mind when we ask about heterogeneity. This paper is adapted from a chapter in ``Common Mistakes in Meta-Analysis and How to Avoid Them.'' A free PDF of the book is available at https://www.Meta-Analysis.com/rsm.},
  langid = {english},
  keywords = {Meta-analysis heterogeneity},
  file = {C:\Users\u686785\Zotero\storage\4442FV43\Borenstein_2024_Avoiding common mistakes in meta-analysis.pdf}
}

@article{borgstedeMeaningfulMeasurementRequires2023,
  title = {Meaningful Measurement Requires Substantive Formal Theory},
  author = {Borgstede, Matthias and Eggert, Frank},
  year = {2023},
  month = feb,
  journal = {Theory \& Psychology},
  volume = {33},
  number = {1},
  pages = {153--159},
  publisher = {SAGE Publications Ltd},
  issn = {0959-3543},
  doi = {10.1177/09593543221139811},
  urldate = {2025-08-12},
  abstract = {In this article, we take the opportunity to elaborate on some aspects of our article ``Squaring the Circle: From Latent Variables to Theory-Based Measurement'' (Borgstede \& Eggert, 2023) that gave rise to the concerns uttered by Hasselman (2023) and Slaney (2023), and to clarify why we think that theory-based measurement is indeed necessary and sufficient for the establishment of meaningful psychological measurement procedures. Moreover, we will illustrate how theory-based measurement might be accomplished in psychology by means of an example from behavioral selection theory.},
  langid = {english},
  keywords = {measurement theory},
  file = {C:\Users\u686785\Zotero\storage\QZKAJHBB\Borgstede_Eggert_2023_Meaningful measurement requires substantive formal theory.pdf}
}

@article{botellaManagingHeterogeneityVariance2012,
  title = {Managing {{Heterogeneity}} of {{Variance}} in {{Studies}} of {{Reliability Generalization With Alpha Coefficients}}},
  author = {Botella, Juan and Suero, Manuel},
  year = {2012},
  month = jan,
  journal = {Methodology},
  publisher = {Hogrefe Publishing},
  issn = {1614-2241},
  urldate = {2024-03-12},
  abstract = {In Reliability Generalization (RG) meta-analyses, the importance of bearing in mind the problems of range restriction or biased sampling and their influence on reliability estimation has often been...},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\LWTFFIYP\a000039.html}
}

@article{brachemReplikationskrisePhackingUnd2022,
  title = {Replikationskrise, p-Hacking Und {{Open Science}}},
  author = {Brachem, Johannes and Frank, Maximilian and Kvetnaya, Tatiana and Schramm, Leonhard F. F. and Volz, Leonhard},
  year = {2022},
  month = jan,
  journal = {Psychologische Rundschau},
  volume = {73},
  number = {1},
  pages = {1--17},
  publisher = {Hogrefe Verlag},
  issn = {0033-3042},
  doi = {10.1026/0033-3042/a000562},
  urldate = {2023-02-20},
  abstract = {Zusammenfassung. In den letzten Jahren gab es innerhalb der Psychologie eine intensive Auseinandersetzung mit den Auswirkungen der Replikationskrise sowie dem hieraus entstandenen Diskurs {\"u}ber die Weiterentwicklung der Disziplin. Als ein Grund f{\"u}r die mangelnde Replizierbarkeit psychologischer Forschung wurde die Verwendung fragw{\"u}rdiger Forschungspraktiken (eng. QRPs) identifiziert. W{\"a}hrend es umfangreiche Untersuchungen zur Pr{\"a}valenz von QRPs unter Wissenschaftler*innen gibt, ist bisher wenig {\"u}ber die Verbreitung dieser Praktiken unter Studierenden bekannt. Mit der hier vorgestellten Arbeit wurde erstmals eine gr{\"o}{\ss}ere Befragung unter 1397 Psychologie-Studierenden im deutschsprachigen Raum durchgef{\"u}hrt, um die Verbreitung von QRPs in studentischen Projekten sowie den aktuellen Stand der akademischen Lehre in Bezug auf die Replikationskrise und Open Science zu erheben. Die gemeinsame Betrachtung der Lehre und des Einsatzes fragw{\"u}rdiger Forschungspraktiken versprechen Aufschluss dar{\"u}ber, wie die psychologische Lehre mit dem empirischen Vorgehen der Studierenden zusammenh{\"a}ngt. Die Ergebnisse zeigen, dass QRPs auch in studentischen Projekten vorkommen, wobei gro{\ss}e Unterschiede in der Verbreitung einzelner QRPs bestehen. Auch zwischen den verschiedenen Projekttypen zeigten sich Unterschiede, so war die Anwendung von QRPs in Experimentalpraktika am st{\"a}rksten und in Masterarbeiten am schw{\"a}chsten ausgepr{\"a}gt. Unsere Daten weisen insgesamt darauf hin, dass die selbstberichtete Verbreitung von QRPs {\"u}ber den Studienverlauf abnimmt. Zudem scheint ein Gro{\ss}teil der Studierenden bereits mit der Thematik der Replikationskrise in der Lehre in Ber{\"u}hrung gekommen zu sein. Deren Behandlung findet gr{\"o}{\ss}tenteils in der Methodenlehre und weniger in inhaltlich spezialisierten Lehrveranstaltungen statt. Wir geben abschlie{\ss}end Impulse zur Weiterentwicklung der psychologischen Lehre, in denen die Prinzipien der Offenheit, Transparenz und Kollaboration beim Hervorbringen inhaltlich robuster Forschung bereits w{\"a}hrend des Studiums im Vordergrund stehen.}
}

@article{bressanConfoundsFailedReplications2019,
  title = {Confounds in ``{{Failed}}'' {{Replications}}},
  author = {Bressan, Paola},
  year = {2019},
  journal = {Frontiers in Psychology},
  volume = {10},
  issn = {1664-1078},
  urldate = {2023-02-20},
  abstract = {Reproducibility is essential to science, yet a distressingly large number of research findings do not seem to replicate. Here I discuss one underappreciated reason for this state of affairs. I make my case by noting that, due to artifacts, several of the replication failures of the vastly advertised Open Science Collaboration's Reproducibility Project: Psychology turned out to be invalid. Although these artifacts would have been obvious on perusal of the data, such perusal was deemed undesirable because of its post hoc nature and was left out. However, while data do not lie, unforeseen confounds can render them unable to speak to the question of interest. I look further into one unusual case in which a major artifact could be removed statistically---the nonreplication of the effect of fertility on partnered women's preference for single over attached men. I show that the ``failed replication'' datasets contain a gross bias in stimulus allocation which is absent in the original dataset; controlling for it replicates the original study's main finding. I conclude that, before being used to make a scientific point, all data should undergo a minimal quality control---a provision, it appears, not always required of those collected for purpose of replication. Because unexpected confounds and biases can be laid bare only after the fact, we must get over our understandable reluctance to engage in anything post hoc. The reproach attached to p-hacking cannot exempt us from the obligation to (openly) take a good look at our data.},
  file = {C:\Users\u686785\Zotero\storage\VXJWPC8H\Bressan - 2019 - Confounds in “Failed” Replications.pdf}
}

@article{camererEvaluatingReplicabilityLaboratory2016,
  title = {Evaluating Replicability of Laboratory Experiments in Economics},
  author = {Camerer, Colin F. and Dreber, Anna and Forsell, Eskil and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Almenberg, Johan and Altmejd, Adam and Chan, Taizan and Heikensten, Emma and Holzmeister, Felix and Imai, Taisuke and Isaksson, Siri and Nave, Gideon and Pfeiffer, Thomas and Razen, Michael and Wu, Hang},
  year = {2016},
  month = mar,
  journal = {Science},
  volume = {351},
  number = {6280},
  pages = {1433--1436},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aaf0918},
  urldate = {2023-02-21},
  abstract = {The replicability of some scientific findings has recently been called into question. To contribute data about replicability in economics, we replicated 18 studies published in the American Economic Review and the Quarterly Journal of Economics between 2011 and 2014. All of these replications followed predefined analysis plans that were made publicly available beforehand, and they all have a statistical power of at least 90\% to detect the original effect size at the 5\% significance level. We found a significant effect in the same direction as in the original study for 11 replications (61\%); on average, the replicated effect size is 66\% of the original. The replicability rate varies between 67\% and 78\% for four additional replicability indicators, including a prediction market measure of peer beliefs.},
  file = {C:\Users\u686785\Zotero\storage\I24E9GES\Camerer et al. - 2016 - Evaluating replicability of laboratory experiments.pdf}
}

@article{camererEvaluatingReplicabilitySocial2018,
  title = {Evaluating the Replicability of Social Science Experiments in {{Nature}} and {{Science}} between 2010 and 2015},
  author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
  year = {2018},
  month = sep,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {9},
  pages = {637--644},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0399-z},
  urldate = {2023-02-28},
  abstract = {Being able to replicate scientific findings is crucial for scientific progress1--15. We replicate 21 systematically selected experimental studies in the social sciences published in Nature and Science between 2010 and 201516--36. The replications follow analysis plans reviewed by the original authors and pre-registered prior to the replications. The replications are high powered, with sample sizes on average about five times higher than in the original studies. We find a significant effect in the same direction as the original study for 13 (62\%) studies, and the effect size of the replications is on average about 50\% of the original effect size. Replicability varies between 12 (57\%) and 14 (67\%) studies for complementary replicability indicators. Consistent with these results, the estimated true-positive rate is 67\% in a Bayesian analysis. The relative effect size of true positives is estimated to be 71\%, suggesting that both false positives and inflated effect sizes of true positives contribute to imperfect reproducibility. Furthermore, we find that peer beliefs of replicability are strongly related to replicability, suggesting that the research community could predict which results would replicate and that failures to replicate were not the result of chance alone.},
  copyright = {2018 The Author(s)},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\2Y8DHJ5Q\Camerer et al. - 2018 - Evaluating the replicability of social science exp.pdf}
}

@article{campbellMeasurementErrorMetaanalysis2021,
  title = {Measurement Error in Meta-Analysis ({{MEMA}})---{{A Bayesian}} Framework for Continuous Outcome Data Subject to Non-Differential Measurement Error},
  author = {Campbell, Harlan and {de Jong}, Valentijn M. T. and Maxwell, Lauren and Jaenisch, Thomas and Debray, Thomas P. A. and Gustafson, Paul},
  year = {2021},
  journal = {Research Synthesis Methods},
  volume = {12},
  number = {6},
  pages = {796--815},
  issn = {1759-2887},
  doi = {10.1002/jrsm.1515},
  urldate = {2023-02-22},
  abstract = {Ideally, a meta-analysis will summarize data from several unbiased studies. Here we look into the less than ideal situation in which contributing studies may be compromised by non-differential measurement error in the exposure variable. Specifically, we consider a meta-analysis for the association between a continuous outcome variable and one or more continuous exposure variables, where the associations may be quantified as regression coefficients of a linear regression model. A flexible Bayesian framework is developed which allows one to obtain appropriate point and interval estimates with varying degrees of prior knowledge about the magnitude of the measurement error. We also demonstrate how, if individual-participant data (IPD) are available, the Bayesian meta-analysis model can adjust for multiple participant-level covariates, these being measured with or without measurement error.},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\MZD3UHX6\Campbell et al. - 2021 - Measurement error in meta-analysis (MEMA)—A Bayesi.pdf}
}

@misc{CausalReplicationFramework,
  title = {A {{Causal Replication Framework}} for {{Designing}} and {{Assessing Replication Efforts}} {\textbar} {{Zeitschrift}} F{\"u}r {{Psychologie}}},
  urldate = {2023-02-21},
  howpublished = {https://econtent-hogrefe-com.tilburguniversity.idm.oclc.org/doi/full/10.1027/2151-2604/a000385}
}

@article{chambersIntroducingTransparencyOpenness2018,
  title = {Introducing the Transparency and Openness Promotion ({{TOP}}) Guidelines and Badges for Open Practices at {{Cortex}}},
  author = {Chambers, Christopher D.},
  year = {2018},
  month = sep,
  journal = {Cortex},
  volume = {106},
  pages = {316--318},
  issn = {00109452},
  doi = {10.1016/j.cortex.2018.08.001},
  urldate = {2023-02-21},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\XDBJ6I4A\Chambers - 2018 - Introducing the transparency and openness promotio.pdf}
}

@article{chatzisarantisPrimerUnderstandingMetaanalysis2009,
  title = {A Primer on the Understanding of Meta-Analysis},
  author = {Chatzisarantis, Nikos L.D. and Stoica, Alina},
  year = {2009},
  month = sep,
  journal = {Psychology of Sport and Exercise},
  volume = {10},
  number = {5},
  pages = {498--501},
  issn = {14690292},
  doi = {10.1016/j.psychsport.2009.04.003},
  urldate = {2023-02-22},
  abstract = {Objectives: The purpose of the present paper is to provide a primer on the understanding of meta-analysis. Design and method: After presenting the rationale behind meta-analysis, the present paper defines statistical artifacts of sampling error and measurement. Findings: Examples show that statistical artifacts influence the correlation coefficient. The paper also explains the notions of confidence intervals and credibility intervals and how correlations corrected for sampling error and measurement error are calculated.},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\KZ78IYRP\Chatzisarantis and Stoica - 2009 - A primer on the understanding of meta-analysis.pdf}
}

@article{cheungDirectComparisonApproach2012,
  title = {A {{Direct Comparison Approach}} for {{Testing Measurement Invariance}}},
  author = {Cheung, Gordon W. and Lau, Rebecca S.},
  year = {2012},
  month = apr,
  journal = {Organizational Research Methods},
  volume = {15},
  number = {2},
  pages = {167--198},
  publisher = {SAGE Publications Inc},
  issn = {1094-4281},
  doi = {10.1177/1094428111421987},
  urldate = {2025-08-19},
  abstract = {Measurement equivalence/invariance (ME/I) is a condition that should be met before meaningful comparisons of survey results across groups can be made. As an alternative to the likelihood ratio test (LRT), the change in comparative fit index ({$\Delta$}CFI) rules of thumb, and the modification index (MI), this teaching note demonstrates the procedures for establishing bias-corrected (BC) bootstrap confidence intervals for testing ME/I. Unlike the LRT and {$\Delta$}CFI methods, which need a different model estimation per item, the BC bootstrap confidence intervals approach can examine item-level ME/I tests using a single model. This method greatly simplifies the search for an invariant item as the reference indicator in the factor-ratio test. Also demonstrated here is how the factor-ratio test and the list-and-delete method can be extended from the metric invariance test to the scalar invariance test. Finally, the BC bootstrap confidence interval procedures for comparing uniqueness variances, factor variances, factor covariances, and latent means across groups are shown.},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\7423ESF5\Cheung_Lau_2012_A Direct Comparison Approach for Testing Measurement Invariance.pdf}
}

@article{cochranCombinationEstimatesDifferent1954,
  title = {The {{Combination}} of {{Estimates}} from {{Different Experiments}}},
  author = {Cochran, William G.},
  year = {1954},
  journal = {Biometrics},
  volume = {10},
  number = {1},
  eprint = {3001666},
  eprinttype = {jstor},
  pages = {101--129},
  publisher = {[Wiley, International Biometric Society]},
  issn = {0006-341X},
  doi = {10.2307/3001666},
  urldate = {2025-08-19},
  file = {C:\Users\u686785\Zotero\storage\SGTZYPW9\Cochran_1954_The Combination of Estimates from Different Experiments.pdf}
}

@article{coppockGeneralizabilityHeterogeneousTreatment2018,
  title = {Generalizability of Heterogeneous Treatment Effect Estimates across Samples},
  author = {Coppock, Alexander and Leeper, Thomas J. and Mullinix, Kevin J.},
  year = {2018},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {49},
  pages = {12441--12446},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1808083115},
  urldate = {2023-02-16},
  abstract = {The extent to which survey experiments conducted with nonrepresentative convenience samples are generalizable to target populations depends critically on the degree of treatment effect heterogeneity. Recent inquiries have found a strong correspondence between sample average treatment effects estimated in nationally representative experiments and in replication studies conducted with convenience samples. We consider here two possible explanations: low levels of effect heterogeneity or high levels of effect heterogeneity that are unrelated to selection into the convenience sample. We analyze subgroup conditional average treatment effects using 27 original--replication study pairs (encompassing 101,745 individual survey responses) to assess the extent to which subgroup effect estimates generalize. While there are exceptions, the overwhelming pattern that emerges is one of treatment effect homogeneity, providing a partial explanation for strong correspondence across both unconditional and conditional average treatment effect estimates.},
  file = {C:\Users\u686785\Zotero\storage\3QKXTKCU\Coppock et al. - 2018 - Generalizability of heterogeneous treatment effect.pdf}
}

@article{cortinaWhatCoefficientAlpha1993,
  title = {What Is Coefficient Alpha? {{An}} Examination of Theory and Applications},
  shorttitle = {What Is Coefficient Alpha?},
  author = {Cortina, Jose M.},
  year = {1993},
  journal = {Journal of Applied Psychology},
  volume = {78},
  number = {1},
  pages = {98--104},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1854},
  doi = {10.1037/0021-9010.78.1.98},
  abstract = {Psychological research involving scale construction has been hindered considerably by a widespread lack of understanding of coefficient alpha and reliability theory in general. A discussion of the assumptions and meaning of coefficient alpha is presented. This discussion is followed by a demonstration of the effects of test length and dimensionality on alpha by calculating the statistic for hypothetical tests with varying numbers of items, numbers of orthogonal dimensions, and average item intercorrelations. Recommendations for the proper use of coefficient alpha are offered. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C:\Users\u686785\Zotero\storage\5X7LYBA4\Cortina_1993_What is coefficient alpha.pdf}
}

@article{cribari-netoBetaRegression2010,
  title = {Beta {{Regression}} in {{{\emph{R}}}}},
  author = {{Cribari-Neto}, Francisco and Zeileis, Achim},
  year = {2010},
  journal = {Journal of Statistical Software},
  volume = {34},
  number = {2},
  issn = {1548-7660},
  doi = {10.18637/jss.v034.i02},
  urldate = {2023-03-25},
  abstract = {The class of beta regression models is commonly used by practitioners to model variables that assume values in the standard unit interval (0, 1). It is based on the assumption that the dependent variable is beta-distributed and that its mean is related to a set of regressors through a linear predictor with unknown coefficients and a link function. The model also includes a precision parameter which may be constant or depend on a (potentially different) set of regressors through a link function as well. This approach naturally incorporates features such as heteroskedasticity or skewness which are commonly observed in data taking values in the standard unit interval, such as rates or proportions. This paper describes the betareg package which provides the class of beta regressions in the R system for statistical computing. The underlying theory is briefly outlined, the implementation discussed and illustrated in various replication exercises.},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\C98UYVA9\Cribari-Neto and Zeileis - 2010 - Beta Regression in R.pdf}
}

@article{cronbachCoefficientAlphaInternal1951,
  title = {Coefficient Alpha and the Internal Structure of Tests},
  author = {Cronbach, Lee J.},
  year = {1951},
  month = sep,
  journal = {Psychometrika},
  volume = {16},
  number = {3},
  pages = {297--334},
  issn = {1860-0980},
  doi = {10.1007/BF02310555},
  urldate = {2025-08-12},
  abstract = {A general formula ({$\alpha$}) of which a special case is the Kuder-Richardson coefficient of equivalence is shown to be the mean of all split-half coefficients resulting from different splittings of a test. {$\alpha$} is therefore an estimate of the correlation between two random samples of items from a universe of items like those in the test. {$\alpha$} is found to be an appropriate index of equivalence and, except for very short tests, of the first-factor concentration in the test. Tests divisible into distinct subtests should be so divided before using the formula. The index\$\${\textbackslash}bar r\_\{ij\} \$\$, derived from {$\alpha$}, is shown to be an index of inter-item homogeneity. Comparison is made to the Guttman and Loevinger approaches. Parallel split coefficients are shown to be unnecessary for tests of common types. In designing tests, maximum interpretability of scores is obtained by increasing the first-factor concentration in any separately-scored subtest and avoiding substantial group-factor clusters within a subtest. Scalability is not a requisite.},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\RW775B2D\Cronbach_1951_Coefficient alpha and the internal structure of tests.pdf}
}

@article{cronbachConstructValidityPsychological1955,
  title = {Construct Validity in Psychological Tests},
  author = {Cronbach, L. J. and Meehl, P. E.},
  year = {1955},
  month = jul,
  journal = {Psychological Bulletin},
  volume = {52},
  number = {4},
  pages = {281--302},
  issn = {0033-2909},
  doi = {10.1037/h0040957},
  langid = {english},
  pmid = {13245896},
  file = {C:\Users\u686785\Zotero\storage\STQ7JCRA\Cronbach_Meehl_1955_Construct validity in psychological tests.pdf}
}

@article{crosbyWhereWeLook2008a,
  title = {Where {{Do We Look During Potentially Offensive Behavior}}?},
  author = {Crosby, Jennifer Randall and Monin, Beno{\^i}t and Richardson, Daniel},
  year = {2008},
  month = mar,
  journal = {Psychological Science},
  volume = {19},
  number = {3},
  pages = {226--228},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1111/j.1467-9280.2008.02072.x},
  urldate = {2025-08-19},
  langid = {english}
}

@article{crutzenScaleQualityAlpha2017b,
  title = {Scale Quality: Alpha Is an Inadequate Estimate and Factor-Analytic Evidence Is Needed First of All},
  shorttitle = {Scale Quality},
  author = {Crutzen, Rik and {and Peters}, Gjalt-Jorn Ygram},
  year = {2017},
  month = jul,
  journal = {Health Psychology Review},
  volume = {11},
  number = {3},
  pages = {242--247},
  publisher = {Routledge},
  issn = {1743-7199},
  doi = {10.1080/17437199.2015.1124240},
  urldate = {2025-05-22},
  file = {C:\Users\u686785\Zotero\storage\X7HX8LKL\Crutzen_and Peters_2017_Scale quality.pdf}
}

@article{cummingNewStatisticsWhy2014,
  title = {The {{New Statistics}}: {{Why}} and {{How}}},
  shorttitle = {The {{New Statistics}}},
  author = {Cumming, Geoff},
  year = {2014},
  month = jan,
  journal = {Psychological Science},
  volume = {25},
  number = {1},
  pages = {7--29},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1177/0956797613504966},
  urldate = {2023-02-21},
  abstract = {We need to make substantial changes to how we conduct research. First, in response to heightened concern that our published research literature is incomplete and untrustworthy, we need new requirements to ensure research integrity. These include prespecification of studies whenever possible, avoidance of selection and other inappropriate data-analytic practices, complete reporting, and encouragement of replication. Second, in response to renewed recognition of the severe flaws of null-hypothesis significance testing (NHST), we need to shift from reliance on NHST to estimation and other preferred techniques. The new statistics refers to recommended practices, including estimation based on effect sizes, confidence intervals, and meta-analysis. The techniques are not new, but adopting them widely would be new for many researchers, as well as highly beneficial. This article explains why the new statistics are important and offers guidance for their use. It describes an eight-step new-statistics strategy for research with integrity, which starts with formulation of research questions in estimation terms, has no place for NHST, and is aimed at building a cumulative quantitative discipline.}
}

@article{defruytCloningersPsychobiologicalModel2000,
  title = {Cloninger's {{Psychobiological Model}} of {{Temperament}} and {{Character}} and the {{Five-Factor Model}} of {{Personality}}},
  author = {De Fruyt, F and Van De Wiele, L and Van Heeringen, C},
  year = {2000},
  month = sep,
  journal = {Personality and Individual Differences},
  volume = {29},
  number = {3},
  pages = {441--452},
  issn = {0191-8869},
  doi = {10.1016/S0191-8869(99)00204-4},
  urldate = {2024-07-01},
  abstract = {The relationships between Cloninger's Temperament and Character dimensions [Cloninger, C. R. (1987). A systematic method for clinical description and classification of personality variants. Archives of General Psychiatry, 44 573--588; Cloninger, C. R., Svrakic, D. M., \& Przybeck, T. R. (1993). A psychobiological model of temperament and character. Archives of General Psychiatry, 50, 975--990] and the Five-Factor Model (FFM) of personality are investigated in a randomised sample of 130 patients admitted to the Emergency Psychiatric Unit of a large university hospital. Cloninger's psychobiological model identifies four dimensions of temperament (Novelty seeking, Harm avoidance, Reward dependence and Persistence) and three dimensions of character (Self-directedness, Cooperativeness and Self-transcendence). The FFM proposes the domains of Extraversion, Agreeableness, Conscientiousness, Neuroticism and Openness as the basic dimensions underlying individual differences. Five-factor scores are obtained with the NEO-PI-R [Costa, P. T., Jr., \& McCrae, R. R. (1992). NEO-PI-R. Professional manual. Odessa, FL: Psychological Assessment Resources]; Cloninger's personality dimensions are assessed with the Temperament and Character Inventory (Cloninger et al., 1993). The present study primarily focuses on the direct equivalence of Cloninger's scales with the NEO-PI-R domains and facets. Considerable overlap with the FFM dimensions is demonstrated and the results show that each TCI factor is substantially covered by the FFM.},
  file = {C:\Users\u686785\Zotero\storage\B6GUM6ES\S0191886999002044.html}
}

@article{dengTestingDifferenceReliability2017,
  title = {Testing the {{Difference Between Reliability Coefficients Alpha}} and {{Omega}}},
  author = {Deng, Lifang and Chan, Wai},
  year = {2017},
  month = apr,
  journal = {Educational and Psychological Measurement},
  volume = {77},
  number = {2},
  pages = {185--203},
  issn = {0013-1644},
  doi = {10.1177/0013164416658325},
  urldate = {2025-05-22},
  abstract = {Reliable measurements are key to social science research. Multiple measures of reliability of the total score have been developed, including coefficient alpha, coefficient omega, the greatest lower bound reliability, and others. Among these, the coefficient alpha has been most widely used, and it is reported in nearly every study involving the measure of a construct through multiple items in social and behavioral research. However, it is known that coefficient alpha underestimates the true reliability unless the items are tau-equivalent, and coefficient omega is deemed as a practical alternative to coefficient alpha in estimating measurement reliability of the total score. However, many researchers noticed that the difference between alpha and omega is minor in applications. Since the observed differences in alpha and omega can be due to sampling errors, the purpose of the present study, therefore, is to propose a method to evaluate the difference of coefficient alpha ({$\alpha$}{\textasciicircum}) and omega ({$\omega$}{\textasciicircum}) statistically. In particular, the current article develops a procedure to estimate the SE of ({$\omega$}{\textasciicircum}-{$\alpha$}{\textasciicircum}) and consequently the confidence interval (CI) for ({$\omega-\alpha$}). This procedure allows us to test whether the observed difference ({$\omega$}{\textasciicircum}-{$\alpha$}{\textasciicircum}) is due to sample error or {$\omega$}{\textasciicircum} is significantly greater than {$\alpha$}{\textasciicircum}. The developed procedure is then applied to multiple real data sets from well-known scales to empirically verify the values of ({$\omega$}{\textasciicircum}-{$\alpha$}{\textasciicircum}) in practice. Results showed that in most of the comparisons the differences are significantly above zero but cases also exist where the CIs contain zero. An R program for calculating {$\omega$}{\textasciicircum}, {$\alpha$}{\textasciicircum}, and the SE of ({$\omega$}{\textasciicircum}-{$\alpha$}{\textasciicircum}) is also included in the present study so that the developed procedure is easily accessible to applied researchers.},
  pmcid = {PMC5965544},
  pmid = {29795909},
  file = {C:\Users\u686785\Zotero\storage\L7YXJUU7\Deng_Chan_2017_Testing the Difference Between Reliability Coefficients Alpha and Omega.pdf}
}

@article{diamantopoulosGuidelinesChoosingMultiItem2012,
  title = {Guidelines for {{Choosing Between Multi-Item}} and {{Single-Item Scales}} for {{Construct Measurement}}: {{A Predictive Validity Perspective}}},
  shorttitle = {Guidelines for {{Choosing Between Multi-Item}} and {{Single-Item Scales}} for {{Construct Measurement}}},
  author = {Diamantopoulos, Adamantios and Sarstedt, Marko and Fuchs, M. and Wilczynski, Petra and Kaiser, Sebastian},
  year = {2012},
  month = may,
  journal = {Journal of the Academy of Marketing Science},
  volume = {40},
  pages = {434--449},
  doi = {10.1007/s11747-011-0300-3},
  abstract = {Establishing predictive validity of measures is a major concern in marketing research. This paper investigates the conditions favoring the use of single items versus multi-item scales in terms of predictive validity. A series of complementary studies reveals that the predictive validity of single items varies considerably across different (concrete) constructs and stimuli objects. In an attempt to explain the observed instability, a comprehensive simulation study is conducted aimed at identifying the influence of different factors on the predictive validity of single versus multi-item measures. These include the average inter-item correlations in the predictor and criterion constructs, the number of items measuring these constructs, as well as the correlation patterns of multiple and single items between the predictor and criterion constructs. The simulation results show that, under most conditions typically encountered in practical applications, multi-item scales clearly outperform single items in terms of predictive validity. Only under very specific conditions do single items perform equally well as multi-item scales. Therefore, the use of single-item measures in empirical research should be approached with caution, and the use of such measures should be limited to special circumstances.},
  file = {C:\Users\u686785\Zotero\storage\3GCFGQIV\Diamantopoulos et al. - 2012 - Guidelines for Choosing Between Multi-Item and Sin.pdf}
}

@article{dreberUsingPredictionMarkets2015,
  title = {Using Prediction Markets to Estimate the Reproducibility of Scientific Research},
  author = {Dreber, Anna and Pfeiffer, Thomas and Almenberg, Johan and Isaksson, Siri and Wilson, Brad and Chen, Yiling and Nosek, Brian A. and Johannesson, Magnus},
  year = {2015},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {112},
  number = {50},
  pages = {15343--15347},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1516179112},
  urldate = {2023-02-21},
  abstract = {Concerns about a lack of reproducibility of statistically significant results have recently been raised in many fields, and it has been argued that this lack comes at substantial economic costs. We here report the results from prediction markets set up to quantify the reproducibility of 44 studies published in prominent psychology journals and replicated in the Reproducibility Project: Psychology. The prediction markets predict the outcomes of the replications well and outperform a survey of market participants' individual forecasts. This shows that prediction markets are a promising tool for assessing the reproducibility of published scientific results. The prediction markets also allow us to estimate probabilities for the hypotheses being true at different testing stages, which provides valuable information regarding the temporal dynamics of scientific discovery. We find that the hypotheses being tested in psychology typically have low prior probabilities of being true (median, 9\%) and that a ``statistically significant'' finding needs to be confirmed in a well-powered replication to have a high probability of being true. We argue that prediction markets could be used to obtain speedy information about reproducibility at low cost and could potentially even be used to determine which studies to replicate to optimally allocate limited resources into replications.},
  file = {C:\Users\u686785\Zotero\storage\NHU2I922\Dreber et al. - 2015 - Using prediction markets to estimate the reproduci.pdf}
}

@article{duhachekAlphasStandardError2004a,
  title = {Alpha's Standard Error ({{ASE}}): An Accurate and Precise Confidence Interval Estimate},
  shorttitle = {Alpha's Standard Error ({{ASE}})},
  author = {Duhachek, Adam and Lacobucci, Dawn},
  year = {2004},
  month = oct,
  journal = {The Journal of Applied Psychology},
  volume = {89},
  number = {5},
  pages = {792--808},
  issn = {0021-9010},
  doi = {10.1037/0021-9010.89.5.792},
  abstract = {This research presents the inferential statistics for Cronbach's coefficient alpha on the basis of the standard statistical assumption of multivariate normality. The estimation of alpha's standard error (ASE) and confidence intervals are described, and the authors analytically and empirically investigate the effects of the components of these equations. The authors then demonstrate the superiority of this estimate compared with previous derivations of ASE in a separate Monte Carlo simulation. The authors also present a sampling error and test statistic for a test of independent sample alphas. They conclude with a recommendation that all alpha coefficients be reported in conjunction with standard error or confidence interval estimates and offer SAS and SPSS programming codes for easy implementation.},
  langid = {english},
  pmid = {15506861}
}

@article{ebersoleManyLabs32016,
  title = {Many {{Labs}} 3: {{Evaluating}} Participant Pool Quality across the Academic Semester via Replication},
  shorttitle = {Many {{Labs}} 3},
  author = {Ebersole, Charles R. and Atherton, Olivia E. and Belanger, Aimee L. and Skulborstad, Hayley M. and Allen, Jill M. and Banks, Jonathan B. and Baranski, Erica and Bernstein, Michael J. and Bonfiglio, Diane B. V. and Boucher, Leanne and Brown, Elizabeth R. and Budiman, Nancy I. and Cairo, Athena H. and Capaldi, Colin A. and Chartier, Christopher R. and Chung, Joanne M. and Cicero, David C. and Coleman, Jennifer A. and Conway, John G. and Davis, William E. and Devos, Thierry and Fletcher, Melody M. and German, Komi and Grahe, Jon E. and Hermann, Anthony D. and Hicks, Joshua A. and Honeycutt, Nathan and Humphrey, Brandon and Janus, Matthew and Johnson, David J. and {Joy-Gaba}, Jennifer A. and Juzeler, Hannah and Keres, Ashley and Kinney, Diana and Kirshenbaum, Jacqeline and Klein, Richard A. and Lucas, Richard E. and Lustgraaf, Christopher J. N. and Martin, Daniel and Menon, Madhavi and Metzger, Mitchell and Moloney, Jaclyn M. and Morse, Patrick J. and Prislin, Radmila and Razza, Timothy and Re, Daniel E. and Rule, Nicholas O. and Sacco, Donald F. and Sauerberger, Kyle and Shrider, Emily and Shultz, Megan and Siemsen, Courtney and Sobocko, Karin and Weylin Sternglanz, R. and Summerville, Amy and Tskhay, Konstantin O. and {van Allen}, Zack and Vaughn, Leigh Ann and Walker, Ryan J. and Weinberg, Ashley and Wilson, John Paul and Wirth, James H. and Wortman, Jessica and Nosek, Brian A.},
  year = {2016},
  month = nov,
  journal = {Journal of Experimental Social Psychology},
  series = {Special {{Issue}}: {{Confirmatory}}},
  volume = {67},
  pages = {68--82},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2015.10.012},
  urldate = {2023-03-18},
  abstract = {The university participant pool is a key resource for behavioral research, and data quality is believed to vary over the course of the academic semester. This crowdsourced project examined time of semester variation in 10 known effects, 10 individual differences, and 3 data quality indicators over the course of the academic semester in 20 participant pools (N=2696) and with an online sample (N=737). Weak time of semester effects were observed on data quality indicators, participant sex, and a few individual differences---conscientiousness, mood, and stress. However, there was little evidence for time of semester qualifying experimental or correlational effects. The generality of this evidence is unknown because only a subset of the tested effects demonstrated evidence for the original result in the whole sample. Mean characteristics of pool samples change slightly during the semester, but these data suggest that those changes are mostly irrelevant for detecting effects.},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\Y3FBDUB7\Ebersole et al. - 2016 - Many Labs 3 Evaluating participant pool quality a.pdf}
}

@article{ebersoleManyLabs52020d,
  title = {Many {{Labs}} 5: {{Testing Pre-Data-Collection Peer Review}} as an {{Intervention}} to {{Increase Replicability}}},
  shorttitle = {Many {{Labs}} 5},
  author = {Ebersole, Charles R. and Mathur, Maya B. and Baranski, Erica and {Bart-Plange}, Diane-Jo and Buttrick, Nicholas R. and Chartier, Christopher R. and Corker, Katherine S. and Corley, Martin and Hartshorne, Joshua K. and IJzerman, Hans and Lazarevi{\'c}, Ljiljana B. and Rabagliati, Hugh and Ropovik, Ivan and Aczel, Balazs and Aeschbach, Lena F. and Andrighetto, Luca and Arnal, Jack D. and Arrow, Holly and Babincak, Peter and Bakos, Bence E. and Ban{\'i}k, Gabriel and Baskin, Ernest and Belopavlovi{\'c}, Radomir and Bernstein, Michael H. and Bia{\l}ek, Micha{\l} and Bloxsom, Nicholas G. and Bodro{\v z}a, Bojana and Bonfiglio, Diane B. V. and Boucher, Leanne and Br{\"u}hlmann, Florian and Brumbaugh, Claudia C. and Casini, Erica and Chen, Yiling and Chiorri, Carlo and Chopik, William J. and Christ, Oliver and Ciunci, Antonia M. and Claypool, Heather M. and Coary, Sean and {\v C}oli{\'c}, Marija V. and Collins, W. Matthew and Curran, Paul G. and Day, Chris R. and Dering, Benjamin and Dreber, Anna and Edlund, John E. and Falc{\~a}o, Filipe and Fedor, Anna and Feinberg, Lily and Ferguson, Ian R. and Ford, M{\'a}ire and Frank, Michael C. and Fryberger, Emily and Garinther, Alexander and Gawryluk, Katarzyna and Ashbaugh, Kayla and Giacomantonio, Mauro and Giessner, Steffen R. and Grahe, Jon E. and Guadagno, Rosanna E. and Ha{\l}asa, Ewa and Hancock, Peter J. B. and Hilliard, Rias A. and H{\"u}ffmeier, Joachim and Hughes, Sean and Idzikowska, Katarzyna and Inzlicht, Michael and Jern, Alan and {Jim{\'e}nez-Leal}, William and Johannesson, Magnus and {Joy-Gaba}, Jennifer A. and Kauff, Mathias and Kellier, Danielle J. and Kessinger, Grecia and Kidwell, Mallory C. and Kimbrough, Amanda M. and King, Josiah P. J. and Kolb, Vanessa S. and Ko{\l}odziej, Sabina and Kovacs, Marton and Krasuska, Karolina and Kraus, Sue and Krueger, Lacy E. and Kuchno, Katarzyna and Lage, Caio Ambrosio and Langford, Eleanor V. and Levitan, Carmel A. and {de Lima}, Tiago Jess{\'e} Souza and Lin, Hause and Lins, Samuel and Loy, Jia E. and Manfredi, Dylan and Markiewicz, {\L}ukasz and Menon, Madhavi and Mercier, Brett and Metzger, Mitchell and Meyet, Venus and Millen, Ailsa E. and Miller, Jeremy K. and Montealegre, Andres and Moore, Don A. and Muda, Rafa{\l} and Nave, Gideon and Nichols, Austin Lee and Novak, Sarah A. and Nunnally, Christian and Orli{\'c}, Ana and Palinkas, Anna and Panno, Angelo and Parks, Kimberly P. and Pedovi{\'c}, Ivana and P{\k e}kala, Emilian and Penner, Matthew R. and Pessers, Sebastiaan and Petrovi{\'c}, Boban and Pfeiffer, Thomas and Pie{\'n}kosz, Damian and Preti, Emanuele and Puri{\'c}, Danka and Ramos, Tiago and Ravid, Jonathan and Razza, Timothy S. and Rentzsch, Katrin and Richetin, Juliette and Rife, Sean C. and Rosa, Anna Dalla and Rudy, Kaylis Hase and Salamon, Janos and Saunders, Blair and Sawicki, Przemys{\l}aw and Schmidt, Kathleen and Schuepfer, Kurt and Schultze, Thomas and {Schulz-Hardt}, Stefan and Sch{\"u}tz, Astrid and Shabazian, Ani N. and Shubella, Rachel L. and Siegel, Adam and Silva, R{\'u}ben and Sioma, Barbara and Skorb, Lauren and {de Souza}, Luana Elayne Cunha and Steegen, Sara and Stein, L. A. R. and Sternglanz, R. Weylin and Stojilovi{\'c}, Darko and Storage, Daniel and Sullivan, Gavin Brent and Szaszi, Barnabas and Szecsi, Peter and Sz{\"o}ke, Orsolya and Szuts, Attila and Thomae, Manuela and Tidwell, Natasha D. and Tocco, Carly and Torka, Ann-Kathrin and Tuerlinckx, Francis and Vanpaemel, Wolf and Vaughn, Leigh Ann and Vianello, Michelangelo and Viganola, Domenico and Vlachou, Maria and Walker, Ryan J. and Weissgerber, Sophia C. and Wichman, Aaron L. and Wiggins, Bradford J. and Wolf, Daniel and Wood, Michael J. and Zealley, David and {\v Z}e{\v z}elj, Iris and Zrubka, Mark and Nosek, Brian A.},
  year = {2020},
  month = sep,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {3},
  number = {3},
  pages = {309--331},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245920958687},
  urldate = {2023-03-18},
  abstract = {Replication studies in psychological science sometimes fail to reproduce prior findings. If these studies use methods that are unfaithful to the original study or ineffective in eliciting the phenomenon of interest, then a failure to replicate may be a failure of the protocol rather than a challenge to the original finding. Formal pre-data-collection peer review by experts may address shortcomings and increase replicability rates. We selected 10 replication studies from the Reproducibility Project: Psychology (RP:P; Open Science Collaboration, 2015) for which the original authors had expressed concerns about the replication designs before data collection; only one of these studies had yielded a statistically significant effect (p {$<$} .05). Commenters suggested that lack of adherence to expert review and low-powered tests were the reasons that most of these RP:P studies failed to replicate the original effects. We revised the replication protocols and received formal peer review prior to conducting new replication studies. We administered the RP:P and revised protocols in multiple laboratories (median number of laboratories per original study = 6.5, range = 3?9; median total sample = 1,279.5, range = 276?3,512) for high-powered tests of each original finding with both protocols. Overall, following the preregistered analysis plan, we found that the revised protocols produced effect sizes similar to those of the RP:P protocols (?r = .002 or .014, depending on analytic approach). The median effect size for the revised protocols (r = .05) was similar to that of the RP:P protocols (r = .04) and the original RP:P replications (r = .11), and smaller than that of the original studies (r = .37). Analysis of the cumulative evidence across the original studies and the corresponding three replication attempts provided very precise estimates of the 10 tested effects and indicated that their effect sizes (median r = .07, range = .00?.15) were 78\% smaller, on average, than the original effect sizes (median r = .37, range = .19?.50).},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\XDDSQIPY\Ebersole et al. - 2020 - Many Labs 5 Testing Pre-Data-Collection Peer Revi.pdf}
}

@article{elsonPsychologicalMeasuresArent2023,
  title = {Psychological Measures Aren't Toothbrushes},
  author = {Elson, Malte and Hussey, Ian and Alsalti, Taym and Arslan, Ruben C.},
  year = {2023},
  month = oct,
  journal = {Communications Psychology},
  volume = {1},
  number = {1},
  pages = {25},
  publisher = {Nature Publishing Group},
  issn = {2731-9121},
  doi = {10.1038/s44271-023-00026-9},
  urldate = {2025-08-13},
  abstract = {Most psychological measures are used only once or twice. This proliferation and variability threaten the credibility of research. The Standardisation Of BEhavior Research (SOBER) guidelines aim to ensure that psychological measures are standardised and, unlike toothbrushes, reused by others.},
  copyright = {2023 The Author(s)},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\RDHATQMB\Elson et al_2023_Psychological measures aren’t toothbrushes.pdf}
}

@article{elsonPsychologicalMeasuresArent2023a,
  title = {Psychological Measures Aren't Toothbrushes},
  author = {Elson, Malte and Hussey, Ian and Alsalti, Taym and Arslan, Ruben C.},
  year = {2023},
  month = oct,
  journal = {Communications Psychology},
  volume = {1},
  number = {1},
  pages = {25},
  publisher = {Nature Publishing Group},
  issn = {2731-9121},
  doi = {10.1038/s44271-023-00026-9},
  urldate = {2025-08-19},
  abstract = {Most psychological measures are used only once or twice. This proliferation and variability threaten the credibility of research. The Standardisation Of BEhavior Research (SOBER) guidelines aim to ensure that psychological measures are standardised and, unlike toothbrushes, reused by others.},
  copyright = {2023 The Author(s)},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\SQINGENN\Elson et al_2023_Psychological measures aren’t toothbrushes.pdf}
}

@article{eronenTheoryCrisisPsychology2021,
  title = {The {{Theory Crisis}} in {{Psychology}}: {{How}} to {{Move Forward}}},
  shorttitle = {The {{Theory Crisis}} in {{Psychology}}},
  author = {Eronen, Markus I. and Bringmann, Laura F.},
  year = {2021},
  month = jul,
  journal = {Perspectives on Psychological Science},
  volume = {16},
  number = {4},
  pages = {779--788},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691620970586},
  urldate = {2023-03-17},
  abstract = {Meehl argued in 1978 that theories in psychology come and go, with little cumulative progress. We believe that this assessment still holds, as also evidenced by increasingly common claims that psychology is facing a ?theory crisis? and that psychologists should invest more in theory building. In this article, we argue that the root cause of the theory crisis is that developing good psychological theories is extremely difficult and that understanding the reasons why it is so difficult is crucial for moving forward in the theory crisis. We discuss three key reasons based on philosophy of science for why developing good psychological theories is so hard: the relative lack of robust phenomena that impose constraints on possible theories, problems of validity of psychological constructs, and obstacles to discovering causal relationships between psychological variables. We conclude with recommendations on how to move past the theory crisis.},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\2DTU39SE\Eronen and Bringmann - 2021 - The Theory Crisis in Psychology How to Move Forwa.pdf}
}

@article{fabrigarValidityBasedFrameworkUnderstanding2020,
  title = {A {{Validity-Based Framework}} for {{Understanding Replication}} in {{Psychology}}},
  author = {Fabrigar, Leandre R. and Wegener, Duane T. and Petty, Richard E.},
  year = {2020},
  month = nov,
  journal = {Personality and Social Psychology Review},
  volume = {24},
  number = {4},
  pages = {316--344},
  publisher = {Sage Publications Inc},
  address = {Thousand Oaks},
  issn = {1088-8683},
  doi = {10.1177/1088868320931366},
  urldate = {2023-02-16},
  abstract = {In recent years, psychology has wrestled with the broader implications of disappointing rates of replication of previously demonstrated effects. This article proposes that many aspects of this pattern of results can be understood within the classic framework of four proposed forms of validity: statistical conclusion validity, internal validity, construct validity, and external validity. The article explains the conceptual logic for how differences in each type of validity across an original study and a subsequent replication attempt can lead to replication "failure." Existing themes in the replication literature related to each type of validity are also highlighted. Furthermore, empirical evidence is considered for the role of each type of validity in non-replication. The article concludes with a discussion of broader implications of this classic validity framework for improving replication rates in psychological research.},
  langid = {english},
  annotation = {WOS:000553227600001}
}

@article{flakeConstructValidationSocial2017,
  title = {Construct {{Validation}} in {{Social}} and {{Personality Research}}: {{Current Practice}} and {{Recommendations}}},
  shorttitle = {Construct {{Validation}} in {{Social}} and {{Personality Research}}},
  author = {Flake, Jessica K. and Pek, Jolynn and Hehman, Eric},
  year = {2017},
  month = may,
  journal = {Social Psychological and Personality Science},
  volume = {8},
  number = {4},
  pages = {370--378},
  publisher = {SAGE Publications Inc},
  issn = {1948-5506},
  doi = {10.1177/1948550617693063},
  urldate = {2023-03-09},
  abstract = {The verity of results about a psychological construct hinges on the validity of its measurement, making construct validation a fundamental methodology to the scientific process. We reviewed a representative sample of articles published in the Journal of Personality and Social Psychology for construct validity evidence. We report that latent variable measurement, in which responses to items are used to represent a construct, is pervasive in social and personality research. However, the field does not appear to be engaged in best practices for ongoing construct validation. We found that validity evidence of existing and author-developed scales was lacking, with coefficient {$\alpha$} often being the only psychometric evidence reported. We provide a discussion of why the construct validation framework is important for social and personality researchers and recommendations for improving practice.},
  file = {C:\Users\u686785\Zotero\storage\AS6QKE7D\Flake et al_2017_Construct Validation in Social and Personality Research.pdf}
}

@article{flakeConstructValidityValidity2022,
  title = {Construct Validity and the Validity of Replication Studies: {{A}} Systematic Review},
  shorttitle = {Construct Validity and the Validity of Replication Studies},
  author = {Flake, Jessica Kay and Davidson, Ian J. and Wong, Octavia and Pek, Jolynn},
  year = {2022},
  month = may,
  journal = {American Psychologist},
  volume = {77},
  number = {4},
  pages = {576--588},
  publisher = {American Psychological Association},
  issn = {0003-066X},
  doi = {10.1037/amp0001006},
  urldate = {2023-02-16},
  abstract = {Currently, there is little guidance for navigating measurement challenges that threaten construct validity in replication research. To identify common challenges and ultimately strengthen replication research, we conducted a systematic review of the measures used in the 100 original and replication studies from the Reproducibility Project: Psychology (Open Science Collaboration, 2015). Results indicate that it was common for scales used in the original studies to have little or no validity evidence. Our systematic review demonstrates and corroborates evidence that issues of construct validity are sorely neglected in original and replicated research. We identify four measurement challenges replicators are likely to face: a lack of essential measurement information, a lack of validity evidence, measurement differences, and translation. Next, we offer solutions for addressing these challenges that will improve measurement practices in original and replication research. Finally, we close with a discussion of the need to develop measurement methodologies for the next generation of replication research. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  file = {C:\Users\u686785\Zotero\storage\I635FMUJ\Flake et al_2022_Construct validity and the validity of replication studies.pdf}
}

@article{flakeMeasurementSchmeasurementQuestionable2020,
  title = {Measurement {{Schmeasurement}}: {{Questionable Measurement Practices}} and {{How}} to {{Avoid Them}}},
  shorttitle = {Measurement {{Schmeasurement}}},
  author = {Flake, Jessica Kay and Fried, Eiko I.},
  year = {2020},
  month = dec,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {3},
  number = {4},
  pages = {456--465},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245920952393},
  urldate = {2023-02-21},
  abstract = {In this article, we define questionable measurement practices (QMPs) as decisions researchers make that raise doubts about the validity of the measures, and ultimately the validity of study conclusions. Doubts arise for a host of reasons, including a lack of transparency, ignorance, negligence, or misrepresentation of the evidence. We describe the scope of the problem and focus on how transparency is a part of the solution. A lack of measurement transparency makes it impossible to evaluate potential threats to internal, external, statistical-conclusion, and construct validity. We demonstrate that psychology is plagued by a measurement schmeasurement attitude: QMPs are common, hide a stunning source of researcher degrees of freedom, and pose a serious threat to cumulative psychological science, but are largely ignored. We address these challenges by providing a set of questions that researchers and consumers of scientific research can consider to identify and avoid QMPs. Transparent answers to these measurement questions promote rigorous research, allow for thorough evaluations of a study?s inferences, and are necessary for meaningful replication studies.},
  file = {C:\Users\u686785\Zotero\storage\ZMH3Q7NV\Flake_Fried_2020_Measurement Schmeasurement.pdf}
}

@article{frenchMeasurementInvarianceTechniques2016,
  title = {Measurement Invariance Techniques to Enhance Measurement Sensitivity},
  author = {French, Brian F. and Finch, W. Holmes and Randel, Bruce and Hand, Brian and Gotch, Chad M.},
  year = {2016},
  month = jan,
  journal = {International Journal of Quantitative Research in Education},
  volume = {3},
  number = {1-2},
  pages = {79--93},
  publisher = {Inderscience Publishers},
  issn = {2049-5986},
  doi = {10.1504/IJQRE.2016.073672},
  urldate = {2025-08-19},
  abstract = {Rigorous evidence supporting the effectiveness of interventions is needed to inform teaching practice and improve educational outcomes. In many instances, gathering such evidence includes cluster randomised control trials estimating the effectiveness of educational treatments. Such studies often require the collection of data from large samples in order to accurately detect intervention effects. A failure to detect these effects could be due to the inability of the intervention to produce effects or due to a lack of measurement sensitivity to the intervention itself. The current study outlines a two-stage method for evaluating measurement sensitivity by first conducting content analysis to align items with hypothesised intervention effects, followed by the use of differential item functioning analyses to detect intervention effects more precisely, and thereby test for measurement sensitivity. Increasing measurement sensitivity could lead to increased effect sizes, increased statistical power, reduced sample sizes and reduced costs.}
}

@article{gardinerEditorialMethodsPapers2019,
  title = {Editorial: {{Methods}} Papers},
  shorttitle = {Editorial},
  author = {Gardiner, Elizabeth},
  year = {2019},
  month = jan,
  journal = {Platelets},
  volume = {30},
  number = {1},
  pages = {2--2},
  publisher = {Taylor \& Francis},
  issn = {0953-7104},
  doi = {10.1080/09537104.2018.1529865},
  urldate = {2024-05-15},
  abstract = {The utility of a methods section of a research paper is often tempered by the brevity demanded by manuscript word limitations. Whilst word limits help streamline a paper, a Methods section often bears the brunt of the editorial scalpel, resulting in only brief sketches of experimental protocols and consignment of methodology to online supplementary information files. To retain a place for important detailed methodology, and to encapsulate and highlight new and existing important techniques for platelet and megakaryocyte biology, the Platelets Journal Editorial board now accept Methods manuscripts.},
  pmid = {30346864},
  file = {C:\Users\u686785\Zotero\storage\48ECS2N6\Gardiner - 2019 - Editorial Methods papers.pdf}
}

@article{gehlbachMeasureTwiceCut2011,
  title = {Measure {{Twice}}, {{Cut}} down {{Error}}: {{A Process}} for {{Enhancing}} the {{Validity}} of {{Survey Scales}}},
  shorttitle = {Measure {{Twice}}, {{Cut}} down {{Error}}},
  author = {Gehlbach, Hunter and Brinkworth, Maureen E.},
  year = {2011},
  month = dec,
  journal = {Review of General Psychology},
  volume = {15},
  number = {4},
  pages = {380--387},
  publisher = {SAGE Publications Inc},
  issn = {1089-2680},
  doi = {10.1037/a0025704},
  urldate = {2025-08-12},
  abstract = {For years psychologists across many subfields have undertaken the formidable challenge of designing survey scales to assess attitudes, opinions, and behaviors. Correspondingly, scholars have written much to guide researchers in this undertaking. Yet, many new scales violate established best practices in survey design, suggesting the need for a new approach to designing surveys. This article presents 6 steps to facilitate the construction of questionnaire scales. Unlike previous processes, this one front loads input from other academics and potential respondents in the item-development and revision phase with the goal of achieving credibility across both populations. Specifically, the article describes how (a) a literature review and (b) focus group--interview data can be (c) synthesized into a comprehensive list to facilitate (d) the development of items. Next, survey designers can subject the items to (e) an expert review and (f) cognitive pretesting before executing a pilot test.},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\7NYZZFTI\Gehlbach_Brinkworth_2011_Measure Twice, Cut down Error.pdf}
}

@book{gorsuchFactorAnalysis2013,
  title = {Factor {{Analysis}}},
  author = {Gorsuch, Richard L.},
  year = {2013},
  month = may,
  edition = {2},
  publisher = {Psychology Press},
  address = {New York},
  doi = {10.4324/9780203781098},
  abstract = {Comprehensive and comprehensible, this classic covers the basic and advanced topics essential for using factor analysis as a scientific tool in psychology, education, sociology, and related areas. Emphasizing the usefulness of the techniques, it presents sufficient mathematical background for understanding and sufficient discussion of applications for effective use. This includes not only theory but also the empirical evaluations of the importance of mathematical distinctions for applied scientific analysis.},
  isbn = {978-0-203-78109-8}
}

@article{goslingVeryBriefMeasure2003,
  title = {A Very Brief Measure of the {{Big-Five}} Personality Domains},
  author = {Gosling, Samuel D. and Rentfrow, Peter J. and Swann Jr., William B.},
  year = {2003},
  journal = {Journal of Research in Personality},
  volume = {37},
  number = {6},
  pages = {504--528},
  publisher = {Elsevier Science},
  address = {Netherlands},
  issn = {1095-7251},
  doi = {10.1016/S0092-6566(03)00046-1},
  abstract = {When time is limited, researchers may be faced with the choice of using an extremely brief measure of the Big-Five personality dimensions or using no measure at all. To meet the need for a very brief measure, 5 and 10-item inventories were developed and evaluated in 2 studies with undergraduate students as Ss. Although somewhat inferior to standard multi-item instruments, the instruments reached adequate levels in terms of: (a) convergence with widely used Big-Five measures in self, observer, and peer re- reports, (b) test-retest reliability, (c) patterns of predicted external correlates, and (d) convergence between self and observer ratings. On the basis of these tests, a 10-item measure of the Big-Five dimensions is offered for situations where very short measures are needed, personality is not the primary topic of interest, or researchers can tolerate the somewhat diminished psychometric properties associated with very brief measures. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  file = {C:\Users\u686785\Zotero\storage\W22HBVYE\2003-09807-003.html}
}

@article{groszTabooExplicitCausal2020,
  title = {The {{Taboo Against Explicit Causal Inference}} in {{Nonexperimental Psychology}}},
  author = {Grosz, Michael P. and Rohrer, Julia M. and Thoemmes, Felix},
  year = {2020},
  month = sep,
  journal = {Perspectives on Psychological Science},
  volume = {15},
  number = {5},
  pages = {1243--1255},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691620921521},
  urldate = {2023-02-21},
  abstract = {Causal inference is a central goal of research. However, most psychologists refrain from explicitly addressing causal research questions and avoid drawing causal inference on the basis of nonexperimental evidence. We argue that this taboo against causal inference in nonexperimental psychology impairs study design and data analysis, holds back cumulative research, leads to a disconnect between original findings and how they are interpreted in subsequent work, and limits the relevance of nonexperimental psychology for policymaking. At the same time, the taboo does not prevent researchers from interpreting findings as causal effects?the inference is simply made implicitly, and assumptions remain unarticulated. Thus, we recommend that nonexperimental psychologists begin to talk openly about causal assumptions and causal effects. Only then can researchers take advantage of recent methodological advances in causal reasoning and analysis and develop a solid understanding of the underlying causal mechanisms that can inform future research, theory, and policymakers.},
  file = {C:\Users\u686785\Zotero\storage\YY2HWIYS\Grosz et al. - 2020 - The Taboo Against Explicit Causal Inference in Non.pdf}
}

@article{hamiltonPrevalencePredictorsData2023a,
  title = {Prevalence and Predictors of Data and Code Sharing in the Medical and Health Sciences: Systematic Review with Meta-Analysis of Individual Participant Data},
  shorttitle = {Prevalence and Predictors of Data and Code Sharing in the Medical and Health Sciences},
  author = {Hamilton, Daniel G. and Hong, Kyungwan and Fraser, Hannah and {Rowhani-Farid}, Anisa and Fidler, Fiona and Page, Matthew J.},
  year = {2023},
  month = jul,
  journal = {BMJ},
  volume = {382},
  pages = {e075767},
  publisher = {British Medical Journal Publishing Group},
  issn = {1756-1833},
  doi = {10.1136/bmj-2023-075767},
  urldate = {2024-07-29},
  abstract = {Objectives To synthesise research investigating data and code sharing in medicine and health to establish an accurate representation of the prevalence of sharing, how this frequency has changed over time, and what factors influence availability. Design Systematic review with meta-analysis of individual participant data. Data sources Ovid Medline, Ovid Embase, and the preprint servers medRxiv, bioRxiv, and MetaArXiv were searched from inception to 1 July 2021. Forward citation searches were also performed on 30 August 2022. Review methods Meta-research studies that investigated data or code sharing across a sample of scientific articles presenting original medical and health research were identified. Two authors screened records, assessed the risk of bias, and extracted summary data from study reports when individual participant data could not be retrieved. Key outcomes of interest were the prevalence of statements that declared that data or code were publicly or privately available (declared availability) and the success rates of retrieving these products (actual availability). The associations between data and code availability and several factors (eg, journal policy, type of data, trial design, and human participants) were also examined. A two stage approach to meta-analysis of individual participant data was performed, with proportions and risk ratios pooled with the Hartung-Knapp-Sidik-Jonkman method for random effects meta-analysis. Results The review included 105 meta-research studies examining 2 121 580 articles across 31 specialties. Eligible studies examined a median of 195 primary articles (interquartile range 113-475), with a median publication year of 2015 (interquartile range 2012-2018). Only eight studies (8\%) were classified as having a low risk of bias. Meta-analyses showed a prevalence of declared and actual public data availability of 8\% (95\% confidence interval 5\% to 11\%) and 2\% (1\% to 3\%), respectively, between 2016 and 2021. For public code sharing, both the prevalence of declared and actual availability were estimated to be {$<$}0.5\% since 2016. Meta-regressions indicated that only declared public data sharing prevalence estimates have increased over time. Compliance with mandatory data sharing policies ranged from 0\% to 100\% across journals and varied by type of data. In contrast, success in privately obtaining data and code from authors historically ranged between 0\% and 37\% and 0\% and 23\%, respectively. Conclusions The review found that public code sharing was persistently low across medical research. Declarations of data sharing were also low, increasing over time, but did not always correspond to actual sharing of data. The effectiveness of mandatory data sharing policies varied substantially by journal and type of data, a finding that might be informative for policy makers when designing policies and allocating resources to audit compliance. Systematic review registration Open Science Framework doi:10.17605/OSF.IO/7SX8U.},
  chapter = {Research},
  copyright = {{\copyright} Author(s) (or their employer(s)) 2019. Re-use permitted under CC BY-NC. No commercial re-use. See rights and permissions. Published by BMJ.. http://creativecommons.org/licenses/by-nc/4.0/This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/4.0/.},
  langid = {english},
  pmid = {37433624},
  keywords = {found:lit_search,open_science,systematic_reviews},
  file = {C:\Users\u686785\Zotero\storage\CIP8TN8L\Hamilton et al_2023_Prevalence and predictors of data and code sharing in the medical and health.pdf}
}

@article{hardwickeEstimatingPrevalenceTransparency2022,
  title = {Estimating the {{Prevalence}} of {{Transparency}} and {{Reproducibility-Related Research Practices}} in {{Psychology}} (2014--2017)},
  author = {Hardwicke, Tom E. and Thibault, Robert T. and Kosie, Jessica E. and Wallach, Joshua D. and Kidwell, Mallory C. and Ioannidis, John P. A.},
  year = {2022},
  month = jan,
  journal = {Perspectives on Psychological Science},
  volume = {17},
  number = {1},
  pages = {239--251},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691620979806},
  urldate = {2024-01-15},
  abstract = {Psychologists are navigating an unprecedented period of introspection about the credibility and utility of their discipline. Reform initiatives emphasize the benefits of transparency and reproducibility-related research practices; however, adoption across the psychology literature is unknown. Estimating the prevalence of such practices will help to gauge the collective impact of reform initiatives, track progress over time, and calibrate future efforts. To this end, we manually examined a random sample of 250 psychology articles published between 2014 and 2017. Over half of the articles were publicly available (154/237, 65\%, 95\% confidence interval [CI] = [59\%, 71\%]); however, sharing of research materials (26/183; 14\%, 95\% CI = [10\%, 19\%]), study protocols (0/188; 0\%, 95\% CI = [0\%, 1\%]), raw data (4/188; 2\%, 95\% CI = [1\%, 4\%]), and analysis scripts (1/188; 1\%, 95\% CI = [0\%, 1\%]) was rare. Preregistration was also uncommon (5/188; 3\%, 95\% CI = [1\%, 5\%]). Many articles included a funding disclosure statement (142/228; 62\%, 95\% CI = [56\%, 69\%]), but conflict-of-interest statements were less common (88/228; 39\%, 95\% CI = [32\%, 45\%]). Replication studies were rare (10/188; 5\%, 95\% CI = [3\%, 8\%]), and few studies were included in systematic reviews (21/183; 11\%, 95\% CI = [8\%, 16\%]) or meta-analyses (12/183; 7\%, 95\% CI = [4\%, 10\%]). Overall, the results suggest that transparency and reproducibility-related research practices were far from routine. These findings establish baseline prevalence estimates against which future progress toward increasing the credibility and utility of psychology research can be compared.},
  langid = {english},
  keywords = {found:start,method:systematic_review,open_science,reproducibility,transparency,view(PhD):criteria},
  file = {C:\Users\u686785\Zotero\storage\8383K9VA\Hardwicke et al. - 2022 - Estimating the Prevalence of Transparency and Repr.pdf}
}

@article{hasselmanGoingSquaresTheorybased2023,
  title = {Going Round in Squares: {{Theory-based}} Measurement Requires a Theory of Measurement},
  shorttitle = {Going Round in Squares},
  author = {Hasselman, Fred},
  year = {2023},
  month = feb,
  journal = {Theory \& Psychology},
  volume = {33},
  number = {1},
  pages = {145--152},
  publisher = {SAGE Publications Ltd},
  issn = {0959-3543},
  doi = {10.1177/09593543221131511},
  urldate = {2025-08-12},
  abstract = {In their article on theory-based measurement, Borgstede and Eggert (2023) argue that a substantive formal psychological theory that is capable of predicting expected measurement outcomes for the theoretical objects of measurement it posits to exist is both necessary and sufficient for psychological measurement. They reveal that measurement in psychology mostly concerns the estimation of latent variables and compares unfavorably to the development of measurement in the history of physics. They, however, fail to include a comparison with the great advances in theory-based measurement achieved in modern physics. In this commentary, I describe how measurement is formalized in classical physics and examine what would be required to formalize the physical measurement of psychological phenomena. I conclude that, without an examination of the theoretical assumptions underlying current measurement procedures and a formal notion of psychological measurement, it is unlikely that psychological science will be able to generate the substantive theories suggested by Borgstede and Eggert.},
  langid = {english},
  keywords = {measurement theory},
  file = {C:\Users\u686785\Zotero\storage\BHJ6QUDY\Hasselman_2023_Going round in squares.pdf}
}

@article{hoaglinMisunderstandingsCochransTest2016,
  title = {Misunderstandings about {{Q}} and `{{Cochran}}'s {{Q}} Test' in Meta-Analysis},
  author = {Hoaglin, David C.},
  year = {2016},
  journal = {Statistics in Medicine},
  volume = {35},
  number = {4},
  pages = {485--495},
  issn = {1097-0258},
  doi = {10.1002/sim.6632},
  urldate = {2025-08-08},
  abstract = {Many meta-analyses report using `Cochran's Q test' to assess heterogeneity of effect-size estimates from the individual studies. Some authors cite work by W. G. Cochran, without realizing that Cochran deliberately did not use Q itself to test for heterogeneity. Further, when heterogeneity is absent, the actual null distribution of Q is not the chi-squared distribution assumed for `Cochran's Q test'. This paper reviews work by Cochran related to Q. It then discusses derivations of the asymptotic approximation for the null distribution of Q, as well as work that has derived finite-sample moments and corresponding approximations for the cases of specific measures of effect size. Those results complicate implementation and interpretation of the popular heterogeneity index I2. Also, it turns out that the test-based confidence intervals used with I2 are based on a fallacious approach. Software that outputs Q and I2 should use the appropriate reference value of Q for the particular measure of effect size and the current meta-analysis. Q is a key element of the popular DerSimonian--Laird procedure for random-effects meta-analysis, but the assumptions of that procedure and related procedures do not reflect the actual behavior of Q and may introduce bias. The DerSimonian--Laird procedure should be regarded as unreliable. Copyright {\copyright} 2015 John Wiley \& Sons, Ltd.},
  copyright = {Copyright {\copyright} 2015 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {Meta-analysis heterogeneity},
  file = {C:\Users\u686785\Zotero\storage\66EFYWKY\Hoaglin_2016_Misunderstandings about Q and ‘Cochran's Q test' in meta-analysis.pdf}
}

@article{hoganEmpiricalStudyReporting2004,
  title = {An {{Empirical Study}} of {{Reporting Practices Concerning Measurement Validity}}},
  author = {Hogan, Thomas P. and Agnello, Jessica},
  year = {2004},
  month = oct,
  journal = {Educational and Psychological Measurement},
  volume = {64},
  number = {5},
  pages = {802--812},
  publisher = {SAGE Publications Inc},
  issn = {0013-1644},
  doi = {10.1177/0013164404264120},
  urldate = {2024-06-26},
  abstract = {This study investigates the current research practice concerning reporting measurement validity evidence based on a sample of 696 research reports listed in the American Psychological Association's Directory of Unpublished Experimental Mental Measures. Only 55\% of the reports included any type of validity evidence. This was a substantially lower percentage than the percentage for reports of reliability found in an earlier study. Of those entries that included validity evidence, the vast majority reported correlations with other variables. Little use was made of the numerous other types of validation approaches described in measurement textbooks and in the American Educational Research Association, American Psychological Association, and National Council on Measurement in Education's Standards for Educational and Psychological Testing. Inconsistent reports of validity characterized nearly all journals covered in the study.},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\XDQ2BMJV\Hogan and Agnello - 2004 - An Empirical Study of Reporting Practices Concerni.pdf}
}

@article{hornPracticalTheoreticalGuide1992,
  title = {A Practical and Theoretical Guide to Measurement Invariance in Aging Research},
  author = {Horn, John L. and Mcardle, J. J.},
  year = {1992},
  month = sep,
  journal = {Experimental Aging Research},
  volume = {18},
  number = {3},
  pages = {117--144},
  publisher = {Routledge},
  issn = {0361-073X},
  doi = {10.1080/03610739208253916},
  urldate = {2025-08-19},
  abstract = {We describe mathematical and statistical models for factor invariance. We demonstrate that factor invariance is a condition of measurement invariance. In any study of change (as over age) measurement invariance is necessary for valid inference and interpretation. Two important forms of factorial invariance are distinguished: ``configural'' and ``metric.'' Tests for factorial invariance and the range of tests from strong to weak are illustrated with multiple group factor and structural equation modeling analyses (with programs such as LISREL, COSAN, and RAM). The tests are for models of the organization and age changes of intellectual abilities. The models are derived from current theory of fluid (Gf) and crystallized (Gc) abilities. The models are made manifest with measurements of the WAIS-R in the standardization sample. Although this is a methodological paper, the key issues and major principles and conclusions are presented in basic English, devoid of technical details and obscure notation. Conceptual principles of multivariate methods of data analysis are presented in terms of substantive issues of importance for the science of the psychology of aging.},
  pmid = {1459160},
  file = {C:\Users\u686785\Zotero\storage\ITSW8XBI\Horn_Mcardle_1992_A practical and theoretical guide to measurement invariance in aging research.pdf}
}

@article{humphryPsychologicalMeasurementTheory2017,
  title = {Psychological Measurement: {{Theory}}, Paradoxes, and Prototypes},
  shorttitle = {Psychological Measurement},
  author = {Humphry, Stephen M.},
  year = {2017},
  month = jun,
  journal = {Theory \& Psychology},
  volume = {27},
  number = {3},
  pages = {407--418},
  publisher = {SAGE Publications Ltd},
  issn = {0959-3543},
  doi = {10.1177/0959354317699099},
  urldate = {2025-08-12},
  abstract = {The article focuses on the ongoing debate regarding the measurement of psychological attributes. The aim is to clarify different uses of the term theory and key points of agreement and disagreement among participants. In addition, the article addresses misinterpretations of key points in recent articles and notes an apparent paradox arising in the representational theory of measurement. Substantive theory is contrasted with both item response theory and the representational theory of measurement. Emphasis is placed on the direct dependence of the measurement of physical attributes on substantive quantitative theory as opposed to any form of separate measurement theory. It is concluded that the primary challenge faced in quantitative psychology is to posit testable substantive theories or laws which form a foundation for measurement.},
  langid = {english},
  keywords = {measurement theory},
  file = {C:\Users\u686785\Zotero\storage\Z7CS38PC\Humphry_2017_Psychological measurement.pdf}
}

@misc{husseyAberrantAbundanceCronbachs2023,
  title = {An Aberrant Abundance of {{Cronbach}}'s Alpha Values at .70},
  author = {Hussey, Ian and Alsalti, Taym and Bosco, Frank and Elson, Malte and Arslan, Ruben},
  year = {2023},
  month = feb,
  publisher = {OSF},
  doi = {10.31234/osf.io/dm8xn},
  urldate = {2025-06-16},
  abstract = {Cronbach's alpha ({$\alpha$}) is the most widely reported metric of the reliability of psychological measures. Decisions about an observed {$\alpha$}'s adequacy are often made using rule-of-thumb thresholds, such as {$\alpha$} of at least .70. Such thresholds can put pressure on researchers to make their measures meet these criteria, similar to the pressure to meet the significance threshold with p values. We examined whether {$\alpha$} values reported in the psychology literature are inflated at the rule-of-thumb thresholds ({$\alpha$} = .70, .80, .90) due to, for example, overfitting to in-sample data ({$\alpha$}-hacking) or publication bias. We extracted reported {$\alpha$} values from three very large datasets covering the general psychology literature (\&gt;30,000 {$\alpha$} values taken from \&gt;74,000 published articles in APA journals), the Industrial and Organizational psychology literature (\&gt;89,000 {$\alpha$} values taken from \&gt;14,000 published articles in I/O journals), and the APA's PsycTests database which aims to cover all psychological measures published since 1894 (\&gt;67,000 {$\alpha$} values taken from \&gt;60,000 measures). The distributions of these values show robust evidence of excesses at the {$\alpha$} = .70 rule-of-thumb threshold which cannot be explained by justifiable measurement practices. We discuss the scope, causes, and consequences of {$\alpha$}-hacking and how increased transparency, preregistration of measurement strategy, and standardized protocols could mitigate this problem. Code and data are available at osf.io/pe3t7. Supplementary materials at osf.io/5xzy4.},
  archiveprefix = {OSF},
  langid = {american},
  keywords = {found:michele,measurement,method:systematic_review,reliability},
  file = {C:\Users\u686785\Zotero\storage\C73FURTI\Hussey et al_2023_An aberrant abundance of Cronbach’s alpha values at.pdf}
}

@article{ioannidisUncertaintyHeterogeneityEstimates2007,
  title = {Uncertainty in Heterogeneity Estimates in Meta-Analyses},
  author = {Ioannidis, John P. A. and Patsopoulos, Nikolaos A. and Evangelou, Evangelos},
  year = {2007},
  month = nov,
  journal = {BMJ (Clinical research ed.)},
  volume = {335},
  number = {7626},
  pages = {914--916},
  issn = {1756-1833},
  doi = {10.1136/bmj.39343.408449.80},
  abstract = {John Ioannidis, Nikolaos Patsopoulos, and Evangelos Evangelou argue that, although meta-analyses often measure heterogeneity between studies, these estimates can have large uncertainty, which must be taken into account when interpreting evidence},
  langid = {english},
  pmcid = {PMC2048840},
  pmid = {17974687},
  file = {C:\Users\u686785\Zotero\storage\HGDBZUDJ\Ioannidis et al. - 2007 - Uncertainty in heterogeneity estimates in meta-ana.pdf}
}

@techreport{isagerDecidingWhatReplicate2020,
  type = {Preprint},
  title = {Deciding What to Replicate: {{A}} Decision Model for Replication Study Selection under Resource and Knowledge Constraints.},
  shorttitle = {Deciding What to Replicate},
  author = {Isager, Peder Mortvedt and {van Aert}, Robbie Cornelis Maria and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Brandt, Mark John and DeSoto, Kurt Andrew and {Giner-Sorolla}, Roger and Krueger, Joachim and Perugini, Marco and Ropovik, Ivan and {van 't Veer}, Anna Elisabeth and Vranka, Marek Albert and Lakens, Daniel},
  year = {2020},
  month = sep,
  institution = {MetaArXiv},
  doi = {10.31222/osf.io/2gurz},
  urldate = {2023-02-21},
  abstract = {Robust scientific knowledge is contingent upon replication of original findings. However, replicating researchers are constrained by resources, and will almost always have to choose one replication effort to focus on from a set of potential candidates. To select a candidate efficiently in these cases, we need methods for deciding which out of all candidates considered would be the most useful to replicate, given some overall goal researchers wish to achieve. In this article we assume that the overall goal researchers wish to achieve is to maximize the utility gained by conducting the replication study. We then propose a general rule for study selection in replication research based on the replication value of the set of claims considered for replication. The replication value of a claim is defined as the maximum expected utility we could gain by conducting a replication of the claim, and is a function of (1) the value of being certain about the claim, and (2) uncertainty about the claim based on current evidence. We formalize this definition in terms of a causal decision model, utilizing concepts from decision theory and causal graph modeling. We discuss the validity of using replication value as a measure of expected utility gain, and we suggest approaches for deriving quantitative estimates of replication value. Our goal in this article is not to define concrete guidelines for study selection, but to provide the necessary theoretical foundations on which such concrete guidelines could be built.},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\UGEDN6GS\Isager et al. - 2020 - Deciding what to replicate A decision model for r.pdf}
}

@article{johnMeasuringPrevalenceQuestionable2012a,
  title = {Measuring the {{Prevalence}} of {{Questionable Research Practices With Incentives}} for {{Truth Telling}}},
  author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
  year = {2012},
  month = may,
  journal = {Psychological Science},
  volume = {23},
  number = {5},
  pages = {524--532},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1177/0956797611430953},
  urldate = {2025-08-19},
  abstract = {Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\UGJFLRVX\John et al_2012_Measuring the Prevalence of Questionable Research Practices With Incentives for.pdf}
}

@article{kalkbrennerChoosingCronbachsCoefficient2024,
  title = {Choosing {{Between Cronbach}}'s {{Coefficient Alpha}}, {{McDonald}}'s {{Coefficient Omega}}, and {{Coefficient H}}: {{Confidence Intervals}} and the {{Advantages}} and {{Drawbacks}} of {{Interpretive Guidelines}}},
  shorttitle = {Choosing {{Between Cronbach}}'s {{Coefficient Alpha}}, {{McDonald}}'s {{Coefficient Omega}}, and {{Coefficient H}}},
  author = {Kalkbrenner, Michael T.},
  year = {2024},
  month = apr,
  journal = {Measurement and Evaluation in Counseling and Development},
  volume = {57},
  number = {2},
  pages = {93--105},
  publisher = {Routledge},
  issn = {0748-1756},
  doi = {10.1080/07481756.2023.2283637},
  urldate = {2025-08-12},
  abstract = {The purpose of this instructional piece was to provide a nontechnical synthesis of common internal consistency reliability estimates used in professional counseling and in related fields. The article begins with an overview of coefficients alpha, omega, omega hierarchical, and H, with guidelines for their selection. Next, I provide recommendations for interpretive cutoff scores for higher and lower stakes testing followed by commentary on the limitations of relying too heavily on such guidelines. I discuss the importance of reporting confidence intervals (CIs) for reliability estimates to enhance reliability generalizations. When evaluating internal consistency reliability estimates of scores based on sample data counselors are advised to (a) determine the intended use of test scores in terms of higher or lower stakes testing, (b) take a multifaceted approach and consider the requirements of each reliability index, (c) include CIs, and (d) refer to interpretive cutoff scores as tentative general guidelines, not absolute standards.},
  keywords = {omega SE}
}

@article{kennyPerformanceRMSEAModels2015,
  title = {The Performance of {{RMSEA}} in Models with Small Degrees of Freedom},
  author = {Kenny, David A. and Kaniskan, Burcu and McCoach, D. Betsy},
  year = {2015},
  journal = {Sociological Methods \& Research},
  volume = {44},
  number = {3},
  pages = {486--507},
  publisher = {Sage Publications},
  address = {US},
  issn = {1552-8294},
  doi = {10.1177/0049124114543236},
  abstract = {Given that the root mean square error of approximation (RMSEA) is currently one of the most popular measures of goodness-of-model fit within structural equation modeling (SEM), it is important to know how well the RMSEA performs in models with small degrees of freedom (df). Unfortunately, most previous work on the RMSEA and its confidence interval has focused on models with a large df. Building on the work of Chen et al. to examine the impact of small df on the RMSEA, we conducted a theoretical analysis and a Monte Carlo simulation using correctly specified models with varying df and sample size. The results of our investigation indicate that when the cutoff values are used to assess the fit of the properly specified models with small df and small sample size, the RMSEA too often falsely indicates a poor fitting model. We recommend not computing the RMSEA for small df models, especially those with small sample sizes, but rather estimating parameters that were not originally specified in the model. (PsycINFO Database Record (c) 2016 APA, all rights reserved)}
}

@article{kleinInvestigatingVariationReplicability2014a,
  title = {Investigating Variation in Replicability: {{A}} ``Many Labs'' Replication Project},
  shorttitle = {Investigating Variation in Replicability},
  author = {Klein, Richard A. and Ratliff, Kate A. and Vianello, Michelangelo and Adams Jr., Reginald B. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Bernstein, Michael J. and Bocian, Konrad and Brandt, Mark J. and Brooks, Beach and Brumbaugh, Claudia Chloe and Cemalcilar, Zeynep and Chandler, Jesse and Cheong, Winnee and Davis, William E. and Devos, Thierry and Eisner, Matthew and Frankowska, Natalia and Furrow, David and Galliani, Elisa Maria and Hasselman, Fred and Hicks, Joshua A. and Hovermale, James F. and Hunt, S. Jane and Huntsinger, Jeffrey R. and IJzerman, Hans and John, Melissa-Sue and {Joy-Gaba}, Jennifer A. and Barry Kappes, Heather and Krueger, Lacy E. and Kurtz, Jaime and Levitan, Carmel A. and Mallett, Robyn K. and Morris, Wendy L. and Nelson, Anthony J. and Nier, Jason A. and Packard, Grant and Pilati, Ronaldo and Rutchick, Abraham M. and Schmidt, Kathleen and Skorinko, Jeanine L. and Smith, Robert and Steiner, Troy G. and Storbeck, Justin and Van Swol, Lyn M. and Thompson, Donna and {van 't Veer}, A. E. and Vaughn, Leigh Ann and Vranka, Marek and Wichman, Aaron L. and Woodzicka, Julie A. and Nosek, Brian A.},
  year = {2014},
  journal = {Social Psychology},
  volume = {45},
  number = {3},
  pages = {142--152},
  publisher = {Hogrefe Publishing},
  address = {Germany},
  issn = {2151-2590},
  doi = {10.1027/1864-9335/a000178},
  abstract = {[Correction Notice: An Erratum for this article was reported in Vol 50(3) of Social Psychology (see record 2019-34530-002). The article contained some errors. One line of code was incorrect in the script that generated results for Rugg (1941). Effectively, the authors failed to correctly invert two of the columns in Tables 2 and 3. The revised statistics do not alter the substantive conclusions for this effect (e.g., it remains a successful replication), however the correct effect size is much smaller and closer to the result reported in the original study. In addition, a typo was incorrect in Table 2, that led to the df and N reported for one of the anchoring studies to be slightly off. The corrections are included in the erratum.] Although replication is a central tenet of science, direct replications are rare in psychology. This research tested variation in the replicability of 13 classic and contemporary effects across 36 independent samples totaling 6,344 participants. In the aggregate, 10 effects replicated consistently. One effect---imagined contact reducing prejudice---showed weak support for replicability. And two effects---flag priming influencing conservatism and currency priming influencing system justification---did not replicate. We compared whether the conditions such as lab versus online or US versus international sample predicted effect magnitudes. By and large they did not. The results of this small sample of effects suggest that replicability is more dependent on the effect itself than on the sample and setting used to investigate the effect. (PsycInfo Database Record (c) 2024 APA, all rights reserved)},
  file = {C:\Users\u686785\Zotero\storage\MFEHBWHC\Klein et al_2014_Investigating variation in replicability.pdf}
}

@article{kleinManyLabs22018b,
  title = {Many {{Labs}} 2: {{Investigating Variation}} in {{Replicability Across Samples}} and {{Settings}}},
  shorttitle = {Many {{Labs}} 2},
  author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Adams, Reginald B. and Alper, Sinan and Aveyard, Mark and Axt, Jordan R. and Babalola, Mayowa T. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Batra, Rishtee and Berkics, Mih{\'a}ly and Bernstein, Michael J. and Berry, Daniel R. and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J. and Busching, Robert and R{\'e}dei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L. and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E. and Cheong, Winnee and Cicero, David C. and Coen, Sharon and Coleman, Jennifer A. and Collisson, Brian and Conway, Morgan A. and Corker, Katherine S. and Curran, Paul G. and Cushman, Fiery and Dagona, Zubairu K. and Dalgar, Ilker and Dalla Rosa, Anna and Davis, William E. and {de Bruijn}, Maaike and De Schutter, Leander and Devos, Thierry and {de Vries}, Marieke and Do{\u g}ulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R. and Edlund, John E. and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-{\'A}ngel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C. and Ghoshal, Tanuka and Giessner, Steffen R. and Gill, Tripat and Gnambs, Timo and G{\'o}mez, {\'A}ngel and Gonz{\'a}lez, Roberto and Graham, Jesse and Grahe, Jon E. and Grahek, Ivan and Green, Eva G. T. and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L. and Hall, Michael P. and Heffernan, Marie E. and Hicks, Joshua A. and Houdek, Petr and Huntsinger, Jeffrey R. and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and {Innes-Ker}, {\AA}se H. and {Jim{\'e}nez-Leal}, William and John, Melissa-Sue and {Joy-Gaba}, Jennifer A. and Kamilo{\u g}lu, Roza G. and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N. and Kende, Anna and Kervyn, Nicolas and Kne{\v z}evi{\'c}, Goran and Kovacs, Carrie and Krueger, Lacy E. and Kurapov, German and Kurtz, Jamie and Lakens, Dani{\"e}l and Lazarevi{\'c}, Ljiljana B. and Levitan, Carmel A. and Lewis, Neil A. and Lins, Samuel and Lipsey, Nikolette P. and Losee, Joy E. and Maassen, Esther and Maitner, Angela T. and Malingumu, Winfrida and Mallett, Robyn K. and Marotta, Satia A. and Me{\dj}edovi{\'c}, Janko and {Mena-Pacheco}, Fernando and Milfont, Taciano L. and Morris, Wendy L. and Murphy, Sean C. and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J. and Neto, F{\'e}lix and Lee Nichols, Austin and Ocampo, Aaron and O'Donnell, Susan L. and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, G{\'a}bor and Osowiecka, Malgorzata and Packard, Grant and {P{\'e}rez-S{\'a}nchez}, Rolando and Petrovi{\'c}, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M. H. and Rutchick, Abraham M. and Saavedra, Patricio and Saeri, Alexander K. and Salomon, Erika and Schmidt, Kathleen and Sch{\"o}nbrodt, Felix D. and Sekerdej, Maciej B. and Sirlop{\'u}, David and Skorinko, Jeanine L. M. and Smith, Michael A. and {Smith-Castro}, Vanessa and Smolders, Karin C. H. J. and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G. and Stouten, Jeroen and Street, Chris N. H. and Sundfelt, Oskar K. and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C. W. and Tanzer, Norbert and Tear, Morgan J. and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M. and Ujhelyi, Adrienn and {van Aert}, Robbie C. M. and {van Assen}, Marcel A. L. M. and {van der Hulst}, Marije and {van Lange}, Paul A. M. and {van 't Veer}, Anna Elisabeth and {V{\'a}squez- Echeverr{\'i}a}, Alejandro and Ann Vaughn, Leigh and V{\'a}zquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P. J. and Vranka, Marek A. and Welch, Cheryl and Wichman, Aaron L. and Williams, Lisa A. and Wood, Michael and Woodzicka, Julie A. and Wronska, Marta K. and Young, Liane and Zelenski, John M. and Zhijia, Zeng and Nosek, Brian A.},
  year = {2018},
  month = dec,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {4},
  pages = {443--490},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245918810225},
  urldate = {2023-04-03},
  abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p {$<$} .05), we found that 15 (54\%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p {$<$} .0001), 14 (50\%) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25\%) of the replications yielded effect sizes larger than the original ones, and 21 (75\%) yielded effect sizes smaller than the original ones. The median comparable Cohen?s ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small ({$<$} 0.20) in 16 of the replications (57\%), and 9 effects (32\%) were in the direction opposite the direction of the original effect. Across settings, the Q statistic indicated significant heterogeneity in 11 (39\%) of the replication effects, and most of those were among the findings with the largest overall effect sizes; only 1 effect that was near zero in the aggregate showed significant heterogeneity according to this measure. Only 1 effect had a tau value greater than .20, an indication of moderate heterogeneity. Eight others had tau values near or slightly above .10, an indication of slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied.},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\3XTK5TJI\Klein et al. - 2018 - Many Labs 2 Investigating Variation in Replicabil.pdf}
}

@article{kleinManyLabs42022,
  title = {Many {{Labs}} 4: {{Failure}} to {{Replicate Mortality Salience Effect With}} and {{Without Original Author Involvement}}},
  shorttitle = {Many {{Labs}} 4},
  author = {Klein, Richard A and Cook, Corey L. and Ebersole, Charles R. and Vitiello, Christine and Nosek, Brian A. and Hilgard, Joseph and Ahn, Paul Hangsan and Brady, Abbie J. and Chartier, Christopher R. and Christopherson, Cody D. and Clay, Samuel and Collisson, Brian and Crawford, Jarret T. and Cromar, Ryan and Gardiner, Gwendolyn and Gosnell, Courtney L. and Grahe, Jon and Hall, Calvin and Howard, Irene and {Joy-Gaba}, Jennifer A. and Kolb, Miranda and Legg, Angela M. and Levitan, Carmel A. and Mancini, Anthony D. and Manfredi, Dylan and Miller, Jason and Nave, Gideon and Redford, Liz and Schlitz, Ilaria and Schmidt, Kathleen and Skorinko, Jeanine L. M. and Storage, Daniel and Swanson, Trevor and Van Swol, Lyn M. and Vaughn, Leigh Ann and Vidamuerte, Devere and Wiggins, Brady and Ratliff, Kate A.},
  year = {2022},
  month = apr,
  journal = {Collabra: Psychology},
  volume = {8},
  number = {1},
  pages = {35271},
  issn = {2474-7394},
  doi = {10.1525/collabra.35271},
  urldate = {2023-03-18},
  abstract = {Interpreting a failure to replicate is complicated by the fact that the failure could be due to the original finding being a false positive, unrecognized moderating influences between the original and replication procedures, or faulty implementation of the procedures in the replication. One strategy to maximize replication quality is involving the original authors in study design. We (N = 17 Labs and N = 1,550 participants, after exclusions) experimentally tested whether original author involvement improved replicability of a classic finding from Terror Management Theory (Greenberg et al., 1994). Our results were non-diagnostic of whether original author involvement improves replicability because we were unable to replicate the finding under any conditions. This suggests that the original finding was either a false positive or the conditions necessary to obtain it are not fully understood or no longer exist. Data, materials, analysis code, preregistration, and supplementary documents can be found on the OSF page: https://osf.io/8ccnw/},
  file = {C:\Users\u686785\Zotero\storage\CDEX8HTU\Klein et al. - 2022 - Many Labs 4 Failure to Replicate Mortality Salien.pdf}
}

@article{koziolImpactModelParameterization2018,
  title = {The {{Impact}} of {{Model Parameterization}} and {{Estimation Methods}} on {{Tests}} of {{Measurement Invariance With Ordered Polytomous Data}}},
  author = {Koziol, Natalie A. and Bovaird, James A.},
  year = {2018},
  month = apr,
  journal = {Educational and Psychological Measurement},
  volume = {78},
  number = {2},
  pages = {272--296},
  publisher = {SAGE Publications Inc},
  issn = {0013-1644},
  doi = {10.1177/0013164416683754},
  urldate = {2025-08-19},
  abstract = {Evaluations of measurement invariance provide essential construct validity evidence---a prerequisite for seeking meaning in psychological and educational research and ensuring fair testing procedures in high-stakes settings. However, the quality of such evidence is partly dependent on the validity of the resulting statistical conclusions. Type I or Type II errors can render measurement invariance conclusions meaningless. The present study used Monte Carlo simulation methods to compare the effects of multiple model parameterizations (linear factor model, Tobit factor model, and categorical factor model) and estimators (maximum likelihood [ML], robust maximum likelihood [MLR], and weighted least squares mean and variance-adjusted [WLSMV]) on the performance of the chi-square test for the exact-fit hypothesis and chi-square and likelihood ratio difference tests for the equal-fit hypothesis for evaluating measurement invariance with ordered polytomous data. The test statistics were examined under multiple generation conditions that varied according to the degree of metric noninvariance, the size of the sample, the magnitude of the factor loadings, and the distribution of the observed item responses. The categorical factor model with WLSMV estimation performed best for evaluating overall model fit, and the categorical factor model with ML and MLR estimation performed best for evaluating change in fit. Results from this study should be used to inform the modeling decisions of applied researchers. However, no single analysis combination can be recommended for all situations. Therefore, it is essential that researchers consider the context and purpose of their study.},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\E3FN8ZY6\Koziol_Bovaird_2018_The Impact of Model Parameterization and Estimation Methods on Tests of.pdf}
}

@article{lanceSourcesFourCommonly2006,
  title = {The {{Sources}} of {{Four Commonly Reported Cutoff Criteria}}: {{What Did They Really Say}}?},
  shorttitle = {The {{Sources}} of {{Four Commonly Reported Cutoff Criteria}}},
  author = {Lance, Charles E. and Butts, Marcus M. and Michels, Lawrence C.},
  year = {2006},
  month = apr,
  journal = {Organizational Research Methods},
  volume = {9},
  number = {2},
  pages = {202--220},
  publisher = {SAGE Publications Inc},
  issn = {1094-4281},
  doi = {10.1177/1094428105284919},
  urldate = {2023-03-02},
  abstract = {Everyone can recite methodological ?urban legends? that were taught in graduate school, learned over the years through experience publishing, or perhaps just heard through the grapevine. In this article, the authors trace four widely cited and reported cutoff criteria to their (alleged) original sources to determine whether they really said what they are cited as having said about the cutoff criteria, and if not, what the original sources really said. The authors uncover partial truths in tracing the history of each cutoff criterion and in the end endorse a set of 12 specific guidelines for effective academic referencing provided by Harzing that, if adopted, should help prevent the further perpetuation of methodological urban legends.}
}

@article{lebretonCorrectionsCriterionReliability2014,
  title = {Corrections for {{Criterion Reliability}} in {{Validity Generalization}}: {{A False Prophet}} in a {{Land}} of {{Suspended Judgment}}},
  shorttitle = {Corrections for {{Criterion Reliability}} in {{Validity Generalization}}},
  author = {LeBreton, James M. and Scherer, Kelly T. and James, Lawrence R.},
  year = {2014},
  month = dec,
  journal = {Industrial and Organizational Psychology},
  volume = {7},
  number = {4},
  pages = {478--500},
  issn = {1754-9426, 1754-9434},
  doi = {10.1017/S1754942600006775},
  urldate = {2024-06-25},
  abstract = {The results of meta-analytic (MA) and validity generalization (VG) studies continue to be impressive. In contrast to earlier findings that capped the variance accounted for in job performance at roughly 16\%, many recent studies suggest that a single predictor variable can account for between 16 and 36\% of the variance in some aspect of job performance. This article argues that this ``enhancement'' in variance accounted for is often attributable not to improvements in science but to a dumbing down of the standards for the values of statistics used in correction equations. With rare exceptions, applied researchers have suspended judgment about what is and is not an acceptable threshold for criterion reliability in their quest for higher validities. We demonstrate a statistical dysfunction that is a direct result of using low criterion reliabilities in corrections for attenuation. Corrections typically applied to a single predictor in a VG study are instead applied to multiple predictors. A multiple correlation analysis is then conducted on corrected validity coefficients. It is shown that the corrections often used in single predictor studies yield a squared multiple correlation that appears suspect. Basically, the multiple predictor study exposes the tenuous statistical foundation of using abjectly low criterion reliabilities in single predictor VG studies. Recommendations for restoring scientific integrity to the meta-analyses that permeate industrial--organizational (I--O) psychology are offered.},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\9YDXBZBE\LeBreton et al. - 2014 - Corrections for Criterion Reliability in Validity .pdf}
}

@article{leppinkWeNeedMore2017,
  title = {We Need More Replication Research -- {{A}}~Case for Test-Retest Reliability},
  author = {Leppink, Jimmie and {P{\'e}rez-Fuster}, Patricia},
  year = {2017},
  month = jun,
  journal = {Perspectives on Medical Education},
  volume = {6},
  number = {3},
  pages = {158--164},
  issn = {2212-277X},
  doi = {10.1007/s40037-017-0347-z},
  urldate = {2023-02-22},
  abstract = {Following debates in psychology on the importance of replication research, we have also started to see pleas for a~more prominent role for replication research in medical education. To enable replication research, it is of paramount importance to carefully study the reliability of the instruments we use. Cronbach's alpha has been the most widely used estimator of reliability in the field of medical education, notably as some kind of quality label of test or questionnaire scores based on multiple items or of the reliability of assessment across exam stations. However, as this narrative review outlines, Cronbach's alpha or alternative reliability statistics may complement but not replace psychometric methods such as factor analysis. Moreover, multiple-item measurements should be preferred above single-item measurements, and when using single-item measurements, coefficients as Cronbach's alpha should not be interpreted as indicators of the reliability of a~single item when that item is administered after fundamentally different activities, such as learning tasks that differ in content. Finally, if we want to follow up on recent pleas for more replication research, we have to start studying the test-retest reliability of the instruments we use.},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\Z2BPHXUL\Leppink_Pérez-Fuster_2017_We need more replication research – A case for test-retest reliability.pdf}
}

@article{liDilemmaHeterogeneityTests2015,
  title = {The {{Dilemma}} of {{Heterogeneity Tests}} in {{Meta-Analysis}}: {{A Challenge}} from a {{Simulation Study}}},
  shorttitle = {The {{Dilemma}} of {{Heterogeneity Tests}} in {{Meta-Analysis}}},
  author = {Li, Shi-jun and Jiang, Hua and Yang, Hao and Chen, Wei and Peng, Jin and Sun, Ming-wei and Lu, Charles Damien and Peng, Xi and Zeng, Jun},
  year = {2015},
  month = may,
  journal = {PLoS ONE},
  volume = {10},
  number = {5},
  pages = {e0127538},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0127538},
  urldate = {2024-03-11},
  abstract = {Introduction After several decades' development, meta-analysis has become the pillar of evidence-based medicine. However, heterogeneity is still the threat to the validity and quality of such studies. Currently, Q and its descendant I2 (I square) tests are widely used as the tools for heterogeneity evaluation. The core mission of this kind of test is to identify data sets from similar populations and exclude those are from different populations. Although Q and I2 are used as the default tool for heterogeneity testing, the work we present here demonstrates that the robustness of these two tools is questionable. Methods and Findings We simulated a strictly normalized population S. The simulation successfully represents randomized control trial data sets, which fits perfectly with the theoretical distribution (experimental group: p = 0.37, control group: p = 0.88). And we randomly generate research samples Si that fits the population with tiny distributions. In short, these data sets are perfect and can be seen as completely homogeneous data from the exactly same population. If Q and I2 are truly robust tools, the Q and I2 testing results on our simulated data sets should not be positive. We then synthesized these trials by using fixed model. Pooled results indicated that the mean difference (MD) corresponds highly with the true values, and the 95\% confidence interval (CI) is narrow. But, when the number of trials and sample size of trials enrolled in the meta-analysis are substantially increased; the Q and I2 values also increase steadily. This result indicates that I2 and Q are only suitable for testing heterogeneity amongst small sample size trials, and are not adoptable when the sample sizes and the number of trials increase substantially. Conclusions Every day, meta-analysis studies which contain flawed data analysis are emerging and passed on to clinical practitioners as ``updated evidence''. Using this kind of evidence that contain heterogeneous data sets leads to wrong conclusion, makes chaos in clinical practice and weakens the foundation of evidence-based medicine. We suggest more strict applications of meta-analysis: it should only be applied to those synthesized trials with small sample sizes. We call upon that the tools of evidence-based medicine should keep up-to-dated with the cutting-edge technologies in data science. Clinical research data should be made available publicly when there is any relevant article published so the research community could conduct in-depth data mining, which is a better alternative for meta-analysis in many instances.},
  pmcid = {PMC4449216},
  pmid = {26023932},
  file = {C:\Users\u686785\Zotero\storage\V8ZWNWS4\Li et al. - 2015 - The Dilemma of Heterogeneity Tests in Meta-Analysi.pdf}
}

@article{loganDataStudentsCrowdsourced2023,
  title = {Data from Students and Crowdsourced Online Platforms Do Not Often Measure the Same Thing},
  author = {Logan, Corina},
  year = {2023},
  month = nov,
  journal = {Peer Community in Registered Reports},
  volume = {1},
  pages = {100551},
  publisher = {Peer Community In},
  issn = {2823-9393},
  doi = {10.24072/pci.rr.100551},
  urldate = {2025-08-11},
  abstract = {A recommendation of: Lindsay J. Alley, Jordan Axt, Jessica Kay Flake Convenience Samples and Measurement Equivalence in Replication Research https://osf.io/s5t3v},
  copyright = {(C) Peer Community in Registered Reports, 2023},
  langid = {english},
  file = {C\:\\Users\\u686785\\Zotero\\storage\\A2YQPANC\\Logan_2023_Data from students and crowdsourced online platforms do not often measure the.pdf;C\:\\Users\\u686785\\Zotero\\storage\\VJ75Q5V9\\PCI Stage 1 Preprint_Convenience Samples and Measurement Equivalence Manuscript.docx}
}

@article{lopez-ibanezReliabilityGeneralizationMetaanalysis2024,
  title = {Reliability Generalization Meta-Analysis: Comparing Different Statistical Methods},
  shorttitle = {Reliability Generalization Meta-Analysis},
  author = {{L{\'o}pez-Ib{\'a}{\~n}ez}, Carmen and {L{\'o}pez-Nicol{\'a}s}, Rub{\'e}n and {Bl{\'a}zquez-Rinc{\'o}n}, Desir{\'e}e M. and {S{\'a}nchez-Meca}, Julio},
  year = {2024},
  month = jan,
  journal = {Current Psychology},
  issn = {1936-4733},
  doi = {10.1007/s12144-023-05604-y},
  urldate = {2024-03-12},
  abstract = {Reliability generalization (RG) is a kind of meta-analysis that aims to characterize how reliability varies from one test application to the next. A wide variety of statistical methods have typically been applied in RG meta-analyses, regarding statistical model (ordinary least squares, fixed-effect, random effects, varying-coefficient models), weighting scheme (inverse variance, sample size, not weighting), and transformation method (raw, Fisher's Z, Hakstian and Whalen's and Bonett's transformation) of reliability coefficients. This variety of methods compromise the comparability of RG meta-analyses results and their reproducibility. With the purpose of examining the influence of the different statistical methods applied, a methodological review was conducted on 138 published RG meta-analyses of psychological tests, amounting to a total of 4,350 internal consistency coefficients. Among all combinations of procedures that made theoretical sense, we compared thirteen strategies for calculating the average coefficient, eighteen for calculating the confidence intervals of the average coefficient and calculated the heterogeneity indices for the different transformations of the coefficients. Our findings showed that transformation methods of the reliability coefficients improved the normality adjustment of the coefficient distribution. Regarding the average reliability coefficient and the width of confidence intervals, clear differences among methods were found. The largest discrepancies were found between the different strategies for calculating confidence intervals. Our findings point towards the need for the meta-analyst to justify the statistical model assumed, as well as the transformation method of the reliability coefficients and the weighting scheme.},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\98R7VYMS\López-Ibáñez et al. - 2024 - Reliability generalization meta-analysis comparin.pdf}
}

@article{luijkenImpactPredictorMeasurement2019,
  title = {Impact of Predictor Measurement Heterogeneity across Settings on the Performance of Prediction Models: {{A}} Measurement Error Perspective},
  shorttitle = {Impact of Predictor Measurement Heterogeneity across Settings on the Performance of Prediction Models},
  author = {Luijken, K. and Groenwold, R. H. H. and Van Calster, B. and Steyerberg, E. W. and {van Smeden}, M.},
  year = {2019},
  journal = {Statistics in Medicine},
  volume = {38},
  number = {18},
  pages = {3444--3459},
  issn = {1097-0258},
  doi = {10.1002/sim.8183},
  urldate = {2023-02-20},
  abstract = {It is widely acknowledged that the predictive performance of clinical prediction models should be studied in patients that were not part of the data in which the model was derived. Out-of-sample performance can be hampered when predictors are measured differently at derivation and external validation. This may occur, for instance, when predictors are measured using different measurement protocols or when tests are produced by different manufacturers. Although such heterogeneity in predictor measurement between derivation and validation data is common, the impact on the out-of-sample performance is not well studied. Using analytical and simulation approaches, we examined out-of-sample performance of prediction models under various scenarios of heterogeneous predictor measurement. These scenarios were defined and clarified using an established taxonomy of measurement error models. The results of our simulations indicate that predictor measurement heterogeneity can induce miscalibration of prediction and affects discrimination and overall predictive accuracy, to extents that the prediction model may no longer be considered clinically useful. The measurement error taxonomy was found to be helpful in identifying and predicting effects of heterogeneous predictor measurements between settings of prediction model derivation and validation. Our work indicates that homogeneity of measurement strategies across settings is of paramount importance in prediction research.},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\64XAEPJU\Luijken et al. - 2019 - Impact of predictor measurement heterogeneity acro.pdf}
}

@article{maccallumSampleSizeFactor1999,
  title = {Sample Size in Factor Analysis.},
  author = {MacCallum, Robert C. and Widaman, Keith F. and Zhang, Shaobo and Hong, Sehee},
  year = {1999},
  month = mar,
  journal = {Psychological Methods},
  volume = {4},
  number = {1},
  pages = {84--99},
  doi = {10.1037/1082-989X.4.1.84},
  urldate = {2025-08-12},
  abstract = {The factor analysis literature includes a range of recommendations regarding the minimum sample size necessary to obtain factor solutions that are adequately stable and that correspond closely to population factors. A fundamental misconception about this issue is that the minimum sample size, or the minimum ratio of sample size to the number of variables, is invariant across studies. In fact, necessary sample size is dependent on several aspects of any given study, including the level of communality of the variables and the level of overdetermination of the factors. The authors present a theoretical and mathematical framework that provides a basis for understanding and predicting these effects. The hypothesized effects are verified by a sampling study using artificial data. Results demonstrate the lack of validity of common rules of thumb and provide a basis for establishing guidelines for sample size in factor analysis. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\GGVATRVT\MacCallum et al_1999_Sample size in factor analysis.pdf}
}

@article{maireadshawMeasurementPracticesLargescale2020,
  title = {Measurement Practices in Large-Scale Replications: {{Insights}} from {{Many Labs}} 2},
  shorttitle = {Measurement Practices in Large-Scale Replications},
  author = {{Mairead Shaw} and Cloos, Leonie J. R. and Luong, Raymond and Elbaz, Sasha and Flake, Jessica Kay},
  year = {2020},
  month = nov,
  journal = {Canadian Psychology/Psychologie canadienne},
  series = {Building a {{Cumulative Psychological Science}} / {{La}} Psychologie - {{B{\^a}tir}} Une Science Cumulative},
  volume = {61},
  number = {4},
  pages = {289--298},
  publisher = {Educational Publishing Foundation},
  issn = {0708-5591},
  doi = {10.1037/cap0000220},
  abstract = {Validity of measurement is integral to the interpretability of research endeavours and any subsequent replication attempts. To assess current measurement practices and the construct validity of measures in large-scale replication studies, we conducted a systematic review of measures used in 'Many Labs 2: Investigating Variation in Replicability Across Samples and Settings' (Klein et al., 2018). To evaluate the psychometric properties of the scales used in 'Many Labs 2,' we conducted factor and reliability analyses on the publicly available data. We report that measures in 'Many Labs 2' were often short with little validity evidence reported in the original study, that measures with more validity evidence in the original study had stronger psychometric properties in the replication sample, and that translated versions of scales had lower reliability. We discuss the implications of these findings for interpreting replication results, and make recommendations to improve measurement practices in future replications. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  file = {C\:\\Users\\u686785\\Zotero\\storage\\JJGMXSBZ\\2020-58803-001.pdf;C\:\\Users\\u686785\\Zotero\\storage\\TIHAECGA\\Shaw et al. - 2020 - Measurement practices in large-scale replications.pdf}
}

@article{meadePowerPrecisionConfirmatory2007,
  title = {Power and {{Precision}} in {{Confirmatory Factor Analytic Tests}} of {{Measurement Invariance}}},
  author = {Meade, Adam W. and Bauer, Daniel J.},
  year = {2007},
  month = oct,
  journal = {Structural Equation Modeling: A Multidisciplinary Journal},
  volume = {14},
  number = {4},
  pages = {611--635},
  publisher = {Routledge},
  issn = {1070-5511},
  doi = {10.1080/10705510701575461},
  urldate = {2025-08-19},
  abstract = {This study investigates the effects of sample size, factor overdetermination, and communality on the precision of factor loading estimates and the power of the likelihood ratio test of factorial invariance in multigroup confirmatory factor analysis. Although sample sizes are typically thought to be the primary determinant of precision and power, the degree of factor overdetermination and the level of indicator communalities also play important roles. Based on these findings, no single rule of thumb regarding the ratio of sample size to number of indicators can ensure adequate power to detect a lack of measurement invariance.},
  file = {C:\Users\u686785\Zotero\storage\D2EGPKJC\Meade_Bauer_2007_Power and Precision in Confirmatory Factor Analytic Tests of Measurement.pdf}
}

@article{mcshaneLargeScaleReplicationProjects2019,
  title = {Large-{{Scale Replication Projects}} in {{Contemporary Psychological Research}}},
  author = {McShane, Blakeley B. and Tackett, Jennifer L. and B{\"o}ckenholt, Ulf and Gelman, Andrew},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {99--105},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2018.1505655},
  urldate = {2023-03-26},
  abstract = {Replication is complicated in psychological research because studies of a given psychological phenomenon can never be direct or exact replications of one another, and thus effect sizes vary from one study of the phenomenon to the next---an issue of clear importance for replication. Current large-scale replication projects represent an important step forward for assessing replicability, but provide only limited information because they have thus far been designed in a manner such that heterogeneity either cannot be assessed or is intended to be eliminated. Consequently, the nontrivial degree of heterogeneity found in these projects represents a lower bound on the true degree of heterogeneity. We recommend enriching largescale replication projects going forward by embracing heterogeneity. We argue this is the key for assessing replicability: if effect sizes are sufficiently heterogeneous---even if the sign of the effect is consistent---the phenomenon in question does not seem particularly replicable and the theory underlying it seems poorly constructed and in need of enrichment. Uncovering why and revising theory in light of it will lead to improved theory that explains heterogeneity and increases replicability. Given this, large-scale replication projects can play an important role not only in assessing replicability but also in advancing theory.},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\ZABUSFAS\large_scale.pdf}
}

@book{mellenberghConceptualIntroductionPsychometrics2011,
  title = {A Conceptual Introduction to Psychometrics: Development, Analysis and Application of Psychological and Educational Tests},
  shorttitle = {A Conceptual Introduction to Psychometrics},
  author = {Mellenbergh, G.J.},
  year = {2011},
  publisher = {Eleven international publishing},
  address = {The Hague},
  isbn = {978-94-90947-29-3},
  langid = {english},
  annotation = {OCLC: 761506735}
}

@article{nosekReplicabilityRobustnessReproducibility2022,
  title = {Replicability, {{Robustness}}, and {{Reproducibility}} in {{Psychological Science}}},
  author = {Nosek, Brian A. and Hardwicke, Tom E. and Moshontz, Hannah and Allard, Aur{\'e}lien and Corker, Katherine S. and Dreber, Anna and Fidler, Fiona and Hilgard, Joe and Kline Struhl, Melissa and Nuijten, Mich{\`e}le B. and Rohrer, Julia M. and Romero, Felipe and Scheel, Anne M. and Scherer, Laura D. and Sch{\"o}nbrodt, Felix D. and Vazire, Simine},
  year = {2022},
  journal = {Annual Review of Psychology},
  volume = {73},
  number = {1},
  pages = {719--748},
  doi = {10.1146/annurev-psych-020821-114157},
  urldate = {2023-02-23},
  abstract = {Replication---an important, uncommon, and misunderstood practice---is gaining appreciation in psychology. Achieving replicability is important for making research progress. If findings are not replicable, then prediction and theory development are stifled. If findings are replicable, then interrogation of their meaning and validity can advance knowledge. Assessing replicability can be productive for generating and testing hypotheses by actively confronting current understandings to identify weaknesses and spur innovation. For psychology, the 2010s might be characterized as a decade of active confrontation. Systematic and multi-site replication projects assessed current understandings and observed surprising failures to replicate many published findings. Replication efforts highlighted sociocultural challenges such as disincentives to conduct replications and a tendency to frame replication as a personal attack rather than a healthy scientific practice, and they raised awareness that replication contributes to self-correction. Nevertheless, innovation in doing and understanding replication and its cousins, reproducibility and robustness, has positioned psychology to improve research practices and accelerate progress.},
  pmid = {34665669},
  file = {C:\Users\u686785\Zotero\storage\2JTZXA5D\Nosek et al. - 2022 - Replicability, Robustness, and Reproducibility in .pdf}
}

@incollection{nuijtenAssessingImprovingRobustness2022b,
  title = {Assessing and {{Improving Robustness}} of {{Psychological Research Findings}} in {{Four Steps}}},
  booktitle = {Avoiding {{Questionable Research Practices}} in {{Applied Psychology}}},
  author = {Nuijten, Mich{\`e}le B.},
  editor = {O'Donohue, William and Masuda, Akihiko and Lilienfeld, Scott},
  year = {2022},
  pages = {379--400},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-04968-2_17},
  urldate = {2024-06-26},
  abstract = {Increasing evidence indicates that many published findings in psychology may be overestimated or even false. An often-heard response to this ``replication crisis'' is to replicate more: replication studies should weed out false positives over time and increase the robustness of psychological science. However, replications take time and money -- resources that are often scarce. In this chapter, I propose an efficient alternative strategy: a four-step robustness check that first focuses on verifying reported numbers through reanalysis before replicating studies in a new sample.},
  isbn = {978-3-031-04968-2},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\3VQGC8ZM\Nuijten - 2022 - Assessing and Improving Robustness of Psychologica.pdf}
}

@incollection{nunnallyOverviewPsychologicalMeasurement1978,
  title = {An {{Overview}} of {{Psychological Measurement}}},
  booktitle = {Clinical {{Diagnosis}} of {{Mental Disorders}}: {{A Handbook}}},
  author = {Nunnally, Jum C.},
  editor = {Wolman, Benjamin B.},
  year = {1978},
  pages = {97--146},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-1-4684-2490-4_4},
  urldate = {2024-03-15},
  abstract = {Because this book is being written for clinical psychologists, psychiatrists, and kindred professionals, in this chapter it will be assumed that the reader is already familiar with fundamental issues relating to behavioral measurement and, consequently, that there will be no need to discuss low-level principles. Rather, the discussion will center on controversial issues that are of immediate importance to the professional clinician or researcher in the behavioral sciences. Whereas the examples chosen for this chapter to illustrate principles of measurement are particularly applicable to clinical diagnosis, the principles are quite general to empirical science. Because some methods of statistical and mathematical analysis are intimately related to the development and use of measurement methods, critical comments will be made about some prominent approaches to statistical analysis, but details regarding their applications will be left to referenced sources rather than be discussed in detail here. (Any reader who is not already familiar with fundamental principles of psychometric theory and analysis, or would like a refresher course in that regard, might want to consult my book Psychometric Theory, 1978.)},
  isbn = {978-1-4684-2490-4},
  langid = {english}
}

@article{olsson-collentineHeterogeneityDirectReplications2020,
  title = {Heterogeneity in Direct Replications in Psychology and Its Association with Effect Size},
  author = {{Olsson-Collentine}, Anton and Wicherts, Jelte M. and {van Assen}, Marcel A. L. M.},
  year = {2020},
  journal = {Psychological Bulletin},
  volume = {146},
  pages = {922--940},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1455},
  doi = {10.1037/bul0000294},
  abstract = {We examined the evidence for heterogeneity (of effect sizes) when only minor changes to sample population and settings were made between studies and explored the association between heterogeneity and average effect size in a sample of 68 meta-analyses from 13 preregistered multilab direct replication projects in social and cognitive psychology. Among the many examined effects, examples include the Stroop effect, the ``verbal overshadowing'' effect, and various priming effects such as ``anchoring'' effects. We found limited heterogeneity; 48/68 (71\%) meta-analyses had nonsignificant heterogeneity, and most (49/68; 72\%) were most likely to have zero to small heterogeneity. Power to detect small heterogeneity (as defined by Higgins, Thompson, Deeks, \& Altman, 2003) was low for all projects (mean 43\%), but good to excellent for medium and large heterogeneity. Our findings thus show little evidence of widespread heterogeneity in direct replication studies in social and cognitive psychology, suggesting that minor changes in sample population and settings are unlikely to affect research outcomes in these fields of psychology. We also found strong correlations between observed average effect sizes (standardized mean differences and log odds ratios) and heterogeneity in our sample. Our results suggest that heterogeneity and moderation of effects is unlikely for a 0 average true effect size, but increasingly likely for larger average true effect size. (PsycInfo Database Record (c) 2021 APA, all rights reserved)},
  file = {C:\Users\u686785\Zotero\storage\IS99PZRY\Olsson-Collentine et al. - 2020 - Heterogeneity in direct replications in psychology.pdf}
}

@article{olsson-collentineUnreliableHeterogeneityHow2023,
  title = {Unreliable {{Heterogeneity}}: {{How Measurement Error Obscures Heterogeneity}} in {{Meta-analyses}} in {{Psychology}}},
  shorttitle = {Unreliable {{Heterogeneity}}},
  author = {{Olsson-Collentine}, Anton and Bakker, Marjan and Wicherts, Jelte},
  year = {2023},
  month = jun,
  publisher = {OSF},
  doi = {10.31234/osf.io/jnb6e},
  urldate = {2024-03-13},
  abstract = {Measurement error (imperfect reliability) is present in any empirical effect size estimate and systematically attenuates observed effect sizes compared to true underlying effect sizes. Yet there exist broad concerns that proper measurement tends to be neglected in much of psychological research. We examined how measurement error in primary studies affects meta-analytic heterogeneity estimates using Monte-Carlo simulations. Our results indicate that although measurement error in primary studies can both inflate and suppress heterogeneity, under most circumstances measurement error in primary studies leads to a severe underestimate of heterogeneity in meta-analysis. Our simulations showed expected heterogeneity to be underestimated by about 15\% - 60\% when considering a typical effect size around r = 0.2 and true heterogeneity levels that are common in the meta-analytic literature ({$\tau\&$}gt;0.1, in Pearson's r). The underestimate primarily depends on average reliability in primary studies (higher reliability leads to a smaller underestimate), but also worsens with smaller primary study sample sizes. We observed a positive bias in heterogeneity estimates due to measurement error only under specific and arguably uncommon circumstances of (1) actual zero heterogeneity, particularly when mean effect sizes are large, or (2) combinations of very small true heterogeneity, large variance in primary study reliabilities, large mean effect sizes, and a limited number of primary studies. Severe underestimates of heterogeneity due to measurement error may affect many meta-analyses in psychology and obscure true differences between studies that could be relevant for theory, practice, and future research efforts. Research on concrete guidance to applied meta-analysts is needed, as sophisticated methods for correcting measurement unreliability such as meta-analytic structural equation modeling (MASEM) are only applicable in exceptional cases and corrections based on classical test theory come with caveats and strong assumptions.},
  langid = {american},
  file = {C\:\\Users\\u686785\\Zotero\\storage\\FG26IETZ\\Olsson-Collentine et al. - 2023 - Unreliable Heterogeneity How Measurement Error Ob.pdf;C\:\\Users\\u686785\\Zotero\\storage\\UKNPASD4\\jnb6e.html}
}

@article{opensciencecollaborationEstimatingReproducibilityPsychological2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{OPEN SCIENCE COLLABORATION}},
  year = {2015},
  month = aug,
  journal = {Science},
  volume = {349},
  number = {6251},
  pages = {aac4716},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aac4716},
  urldate = {2023-02-21},
  abstract = {Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
  file = {C:\Users\u686785\Zotero\storage\85CY7SWF\OPEN SCIENCE COLLABORATION - 2015 - Estimating the reproducibility of psychological sc.pdf}
}

@article{opensciencecollaborationEstimatingReproducibilityPsychological2015a,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  year = {2015},
  month = aug,
  journal = {Science},
  volume = {349},
  number = {6251},
  pages = {aac4716},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aac4716},
  urldate = {2025-08-19},
  abstract = {Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
  file = {C:\Users\u686785\Zotero\storage\IH2JXSXA\Open Science Collaboration_2015_Estimating the reproducibility of psychological science.pdf}
}

@article{oswaldImperfectCorrectionsCorrect2015,
  title = {Imperfect {{Corrections}} or {{Correct Imperfections}}? {{Psychometric Corrections}} in {{Meta-Analysis}}},
  shorttitle = {Imperfect {{Corrections}} or {{Correct Imperfections}}?},
  author = {Oswald, Frederick L. and Ercan, Seydahmet and McAbee, Samuel T. and Ock, Jisoo and Shaw, Amy},
  year = {2015},
  month = jun,
  journal = {Industrial and Organizational Psychology},
  volume = {8},
  number = {2},
  pages = {e1-e4},
  issn = {1754-9426, 1754-9434},
  doi = {10.1017/iop.2015.17},
  urldate = {2024-06-25},
  abstract = {There is understandable concern by LeBreton, Scherer, and James (2014) that psychometric corrections in organizational research are nothing more than a form of statistical hydraulics. Statistical corrections for measurement error variance and range restriction might inappropriately ratchet observed effects upward into regions of practical significance and publication glory---at the expense of highly questionable results.},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\PRX8LE92\Oswald et al. - 2015 - Imperfect Corrections or Correct Imperfections Ps.pdf}
}

@article{patacchiniUnobservedHeterogeneityMeasurement2007,
  title = {Unobserved {{Heterogeneity}} or {{Measurement Errors}}? {{Testing}} for {{Correlated Effects}} with {{Measurement Errors}} *},
  shorttitle = {Unobserved {{Heterogeneity}} or {{Measurement Errors}}?},
  author = {Patacchini, Eleonora},
  year = {2007},
  journal = {Oxford Bulletin of Economics and Statistics},
  volume = {69},
  number = {6},
  pages = {867--880},
  issn = {1468-0084},
  doi = {10.1111/j.1468-0084.2007.00485.x},
  urldate = {2023-02-20},
  abstract = {This paper addresses the problem of endogenous regressors due to the presence of unobserved heterogeneity, when this is correlated with the regressors, and caused by regressors' measurement errors. A simple two-stage testing procedure is proposed for the identification of the underlying cause of correlation between regressors and the error term. The statistical performance of the resulting sequential test is assessed using simulated data.},
  langid = {english}
}

@article{patilWhatShouldResearchers2016,
  title = {What {{Should Researchers Expect When They Replicate Studies}}? {{A Statistical View}} of {{Replicability}} in {{Psychological Science}}},
  shorttitle = {What {{Should Researchers Expect When They Replicate Studies}}?},
  author = {Patil, Prasad and Peng, Roger D. and Leek, Jeffrey T.},
  year = {2016},
  month = jul,
  journal = {Perspectives on Psychological Science},
  volume = {11},
  number = {4},
  pages = {539--544},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691616646366},
  urldate = {2023-02-20},
  abstract = {A recent study of the replicability of key psychological findings is a major contribution toward understanding the human side of the scientific process. Despite the careful and nuanced analysis reported, the simple narrative disseminated by the mass, social, and scientific media was that in only 36\% of the studies were the original results replicated. In the current study, however, we showed that 77\% of the replication effect sizes reported were within a 95\% prediction interval calculated using the original effect size. Our analysis suggests two critical issues in understanding replication of psychological studies. First, researchers? intuitive expectations for what a replication should show do not always match with statistical estimates of replication. Second, when the results of original studies are very imprecise, they create wide prediction intervals?and a broad range of replication effects that are consistent with the original estimates. This may lead to effects that replicate successfully, in that replication results are consistent with statistical expectations, but do not provide much information about the size (or existence) of the true effect. In this light, the results of the Reproducibility Project: Psychology can be viewed as statistically consistent with what one might expect when performing a large-scale replication experiment.},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\XIAX3X55\Patil et al. - 2016 - What Should Researchers Expect When They Replicate.pdf}
}

@article{pereiraCriticalInterpretationCochrans2010,
  title = {Critical Interpretation of {{Cochran}}'s {{Q}} Test Depends on Power and Prior Assumptions about Heterogeneity},
  author = {Pereira, Tiago V. and Patsopoulos, Nikolaos A. and Salanti, Georgia and Ioannidis, John P. A.},
  year = {2010},
  journal = {Research Synthesis Methods},
  volume = {1},
  number = {2},
  pages = {149--161},
  issn = {1759-2887},
  doi = {10.1002/jrsm.13},
  urldate = {2025-08-08},
  abstract = {We describe how an appropriate interpretation of the Q-test depends on its power to detect a given typical amount of between-study variance ({$\tau$}2) as well as prior beliefs on heterogeneity. We illustrate these concepts in an evaluation of 1011 meta-analyses of clinical trials with ⩾4 studies and binary outcomes. These concepts can be seen as an application of the Bayes theorem. Across the 1011 meta-analyses, power to detect typical heterogeneity was low in most situations. Thus, usually a non-significant Q test did not change perceptibly prior convictions on heterogeneity. Conversely, significant results for the Q test typically augmented considerably the probability of heterogeneity. The posterior probability of heterogeneity depends on what {$\tau$}2 we want to detect. With the same approach, one may also estimate the posterior probability for the presence of heterogeneity that is large enough to annul statistically significant summary effects; that is half the average within-study variance of the combined studies; and that is able to change the summary effect estimate of the meta-analysis by 20\%. The discussed analyses are exploratory, and may depend heavily on prior assumptions when power for the Q-test is low. Statistical heterogeneity in meta-analyses should be cautiously interpreted considering the power to detect a specific {$\tau$}2 and prior assumptions about the presence of heterogeneity. Copyright {\copyright} 2010 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {Meta-analysis heterogeneity},
  file = {C:\Users\u686785\Zotero\storage\7B6W7FBZ\Pereira et al_2010_Critical interpretation of Cochran's Q test depends on power and prior.pdf}
}

@article{pikeReliabilityMeasurementError1998a,
  title = {Reliability and {{Measurement Error}} in the {{Presence}} of {{Homogeneity}}},
  author = {Pike, Cathy King and Hudson, Walter W.},
  year = {1998},
  month = may,
  journal = {Journal of Social Service Research},
  volume = {24},
  number = {1-2},
  pages = {149--163},
  publisher = {Routledge},
  issn = {0148-8376},
  doi = {10.1300/J079v24n01_07},
  urldate = {2024-06-26},
  abstract = {This paper describes a limitation of using Cronbach's Alpha to estimate reliability when using a sample with homogeneous responses in the measured construct. More specifically, it describes the risk of falsely concluding that a new instrument may have poor reliability and demonstrates the use of an alternate statistic that may serve as a cushion against such errors. Data from two validation studies are used to illustrate the utility of the new statistic, referred to as R-Alpha or Relative Alpha. Included is a discussion of the limitations and appropriate use of the statistic in validating multi-item tests, assessment scales, and inventories.},
  file = {C:\Users\u686785\Zotero\storage\EKS82J44\Pike and Hudson - 1998 - Reliability and Measurement Error in the Presence .pdf}
}

@misc{PsychmetaPackagePsychometric,
  title = {Psychmeta: {{An R Package}} for {{Psychometric Meta-Analysis}}},
  shorttitle = {Psychmeta},
  doi = {10.1177/0146621618795933},
  urldate = {2023-02-22},
  howpublished = {https://journals.sagepub.com/doi/epub/10.1177/0146621618795933},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\UNT72VV9\psychmeta An R Package for Psychometric Meta-Anal.pdf}
}

@article{revelleCoefficientsAlphaBeta2009a,
  title = {Coefficients {{Alpha}}, {{Beta}}, {{Omega}}, and the Glb: {{Comments}} on {{Sijtsma}}},
  shorttitle = {Coefficients {{Alpha}}, {{Beta}}, {{Omega}}, and the Glb},
  author = {Revelle, William and Zinbarg, Richard E.},
  year = {2009},
  month = mar,
  journal = {Psychometrika},
  volume = {74},
  number = {1},
  pages = {145--154},
  publisher = {Springer-Verlag},
  issn = {1860-0980},
  doi = {10.1007/s11336-008-9102-z},
  urldate = {2025-05-22},
  abstract = {There are three fundamental problems in Sijtsma (Psychometrika, 2008): (1) contrary to the name, the glb is not the greatest lower bound of reliability but rather is systematically less than {$\omega$} t (McDonald, Test theory: A unified treatment, Erlbaum, Hillsdale, 1999), (2) we agree with Sijtsma that when considering how well a test measures one concept, {$\alpha$} is not appropriate, but recommend {$\omega$} t rather than the glb, and (3) the end user needs procedures that are readily available in open source software.},
  copyright = {2008 The Psychometric Society},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\JIMZKA46\Revelle_Zinbarg_2009_Coefficients Alpha, Beta, Omega, and the glb.pdf}
}

@article{rohrerThinkingClearlyCorrelations2018,
  title = {Thinking {{Clearly About Correlations}} and {{Causation}}: {{Graphical Causal Models}} for {{Observational Data}}},
  shorttitle = {Thinking {{Clearly About Correlations}} and {{Causation}}},
  author = {Rohrer, Julia M.},
  year = {2018},
  month = mar,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {1},
  pages = {27--42},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245917745629},
  urldate = {2024-05-22},
  abstract = {Correlation does not imply causation; but often, observational data are the only option, even though the research question at hand involves causality. This article discusses causal inference based on observational data, introducing readers to graphical causal models that can provide a powerful tool for thinking more clearly about the interrelations between variables. Topics covered include the rationale behind the statistical control of third variables, common procedures for statistical control, and what can go wrong during their implementation. Certain types of third variables---colliders and mediators---should not be controlled for because that can actually move the estimate of an association away from the value of the causal effect of interest. More subtle variations of such harmful control include using unrepresentative samples, which can undermine the validity of causal conclusions, and statistically controlling for mediators. Drawing valid causal inferences on the basis of observational data is not a mechanistic procedure but rather always depends on assumptions that require domain knowledge and that can be more or less plausible. However, this caveat holds not only for research based on observational data, but for all empirical research endeavors.},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\55BI25V7\Rohrer - 2018 - Thinking Clearly About Correlations and Causation.pdf}
}

@article{sarstedtSelectingSingleItems2016,
  title = {Selecting Single Items to Measure Doubly Concrete Constructs: {{A}} Cautionary Tale},
  shorttitle = {Selecting Single Items to Measure Doubly Concrete Constructs},
  author = {Sarstedt, Marko and Diamantopoulos, Adamantios and Salzberger, Thomas and Baumgartner, Petra},
  year = {2016},
  month = aug,
  journal = {Journal of Business Research},
  volume = {69},
  number = {8},
  pages = {3159--3167},
  issn = {0148-2963},
  doi = {10.1016/j.jbusres.2015.12.004},
  urldate = {2025-08-12},
  abstract = {Single-item measures have recently become more en vogue due to studies arguing in favor of their psychometric properties vis-{\`a}-vis multi-item scales. However, their effective use requires (1) expert raters to designate the focal construct as being doubly concrete and (2) researchers to find a good single item to represent the construct. This study examines whether expert raters identify the doubly concrete nature of constructs that prior research presents as exemplary in this respect. Furthermore, the study compares the efficacy of a broad range of selection mechanisms based on expert judgment and statistical criteria for identifying the best item in a scale. The results show that expert raters do not share the commonly held belief that researchers can validly measure constructs such as attitude toward the ad, or brand, with single items. Further analyses show that neither rater assessments nor statistical criteria prove valuable regarding identifying an appropriate single item from a set of candidate items.},
  keywords = {single-item measure},
  file = {C:\Users\u686785\Zotero\storage\NLXZ37EK\Sarstedt et al_2016_Selecting single items to measure doubly concrete constructs.pdf}
}

@article{schmittUsesAbusesCoefficient1996,
  title = {Uses and Abuses of Coefficient Alpha},
  author = {Schmitt, Neal},
  year = {1996},
  journal = {Psychological Assessment},
  volume = {8},
  number = {4},
  pages = {350--353},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-134X},
  doi = {10.1037/1040-3590.8.4.350},
  abstract = {The article addresses some concerns about how coefficient alpha is reported and used. It also shows that alpha is not a measure of homogeneity or unidimensionality. This fact and the finding that test length is related to reliability may cause significant misinterpretations of measures when alpha is used as evidence that a measure is unidimensional. For multidimensional measures, use of alpha as the basis for corrections for attenuation causes overestimates of true correlation. Satisfactory levels of alpha depend on test use and interpretation. Even relatively low (e.g., .50) levels of criterion reliability do not seriously attenuate validity coefficients. When reporting intercorrelations among measures that should be discriminable, it is important to present observed correlations, appropriate measures of reliability, and correlations corrected for unreliability. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  file = {C:\Users\u686785\Zotero\storage\L5J83ZGR\Schmitt_1996_Uses and abuses of coefficient alpha.pdf}
}

@article{sijtsmaUseMisuseVery2009,
  title = {On the {{Use}}, the {{Misuse}}, and the {{Very Limited Usefulness}} of~{{Cronbach}}'s {{Alpha}}},
  author = {Sijtsma, Klaas},
  year = {2009},
  journal = {Psychometrika},
  volume = {74},
  number = {1},
  pages = {107--120},
  issn = {0033-3123},
  doi = {10.1007/s11336-008-9101-0},
  urldate = {2025-08-19},
  abstract = {This discussion paper argues that both the use of Cronbach's alpha as a reliability estimate and as a measure of internal consistency suffer from major problems. First, alpha always has a value, which cannot be equal to the test score's reliability given the interitem covariance matrix and the usual assumptions about measurement error. Second, in practice, alpha is used more often as a measure of the test's internal consistency than as an estimate of reliability. However, it can be shown easily that alpha is unrelated to the internal structure of the test. It is further discussed that statistics based on a single test administration do not convey much information about the accuracy of individuals' test performance. The paper ends with a list of conclusions about the usefulness of alpha.},
  pmcid = {PMC2792363},
  pmid = {20037639},
  file = {C:\Users\u686785\Zotero\storage\2YZR5S3D\Sijtsma_2009_On the Use, the Misuse, and the Very Limited Usefulness of Cronbach’s Alpha.pdf}
}

@article{simmonsFalsePositivePsychologyUndisclosed2011a,
  title = {False-{{Positive Psychology}}: {{Undisclosed Flexibility}} in {{Data Collection}} and {{Analysis Allows Presenting Anything}} as {{Significant}}},
  shorttitle = {False-{{Positive Psychology}}},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  year = {2011},
  month = nov,
  journal = {Psychological Science},
  volume = {22},
  number = {11},
  pages = {1359--1366},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1177/0956797611417632},
  urldate = {2024-07-17},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings ({$\leq$} .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  langid = {english},
  keywords = {found:lit_search,method:simulation,replicability,transparency},
  file = {C:\Users\u686785\Zotero\storage\87S3K5U5\Simmons et al_2011_False-Positive Psychology.pdf}
}

@article{soderbergUsingOSFShare2018,
  title = {Using {{OSF}} to {{Share Data}}: {{A Step-by-Step Guide}}},
  shorttitle = {Using {{OSF}} to {{Share Data}}},
  author = {Soderberg, Courtney},
  year = {2018},
  month = feb,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  pages = {251524591875768},
  doi = {10.1177/2515245918757689},
  abstract = {Sharing data, materials, and analysis scripts with reviewers and readers is valued in psychological science. To facilitate this sharing, files should be stored in a stable location, referenced with unique identifiers, and cited in published work associated with them. This Tutorial provides a step-by-step guide to using OSF to meet the needs for sharing psychological data.},
  file = {C:\Users\u686785\Zotero\storage\CWLLDPUA\Soderberg - 2018 - Using OSF to Share Data A Step-by-Step Guide.pdf}
}

@article{spearmanProofMeasurementAssociation1904,
  title = {The Proof and Measurement of Association between Two Things},
  author = {Spearman, C.},
  year = {1904},
  journal = {The American Journal of Psychology},
  volume = {15},
  number = {1},
  pages = {72--101},
  publisher = {Univ of Illinois Press},
  address = {US},
  issn = {1939-8298},
  doi = {10.2307/1412159},
  abstract = {Attempted to study the scientific correlation between two things. Any correlational experiment can only be regarded as a sample and presents a certain amount of accidental deviation from the real general tendency. Accidental deviation can be measured by the 'probable error.' Accidental deviation depends on the number or cases, and on the largeness of existing correspondence. Probable error varies according to the method of calculation. While the number of Ss helps to reduce accidental deviation, it has no effect upon systematic deviation, except that it indirectly leads to an augmentation. Therefore, the number of cases should be determined by the principle that the measurements to be aggregated together should have their error brought to the same general order of magnitude. Suggests that probable errors must be kept down to limits small enough for the particular object of investigation to be proved. (PsycINFO Database Record (c) 2017 APA, all rights reserved)},
  file = {C\:\\Users\\u686785\\Zotero\\storage\\GCFH7JF4\\Spearman - 1904 - The proof and measurement of association between t.pdf;C\:\\Users\\u686785\\Zotero\\storage\\4A5T7EBW\\1926-00292-001.html}
}

@article{stanleyExpectationsReplicationsAre2014,
  title = {Expectations for {{Replications}}: {{Are Yours Realistic}}?},
  shorttitle = {Expectations for {{Replications}}},
  author = {Stanley, David J. and Spence, Jeffrey R.},
  year = {2014},
  month = may,
  journal = {Perspectives on Psychological Science},
  volume = {9},
  number = {3},
  pages = {305--318},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691614528518},
  urldate = {2023-02-21},
  abstract = {Failures to replicate published psychological research findings have contributed to a ?crisis of confidence.? Several reasons for these failures have been proposed, the most notable being questionable research practices and data fraud. We examine replication from a different perspective and illustrate that current intuitive expectations for replication are unreasonable. We used computer simulations to create thousands of ideal replications, with the same participants, wherein the only difference across replications was random measurement error. In the first set of simulations, study results differed substantially across replications as a result of measurement error alone. This raises questions about how researchers should interpret failed replication attempts, given the large impact that even modest amounts of measurement error can have on observed associations. In the second set of simulations, we illustrated the difficulties that researchers face when trying to interpret and replicate a published finding. We also assessed the relative importance of both sampling error and measurement error in producing variability in replications. Conventionally, replication attempts are viewed through the lens of verifying or falsifying published findings. We suggest that this is a flawed perspective and that researchers should adjust their expectations concerning replications and shift to a meta-analytic mind-set.}
}

@article{stanleyWhatMetaanalysesReveal2018,
  title = {What Meta-Analyses Reveal about the Replicability of Psychological Research},
  author = {Stanley, T. D. and Carter, Evan C. and Doucouliagos, Hristos},
  year = {2018},
  month = dec,
  journal = {Psychological Bulletin},
  volume = {144},
  number = {12},
  pages = {1325--1346},
  publisher = {American Psychological Association},
  issn = {0033-2909},
  doi = {10.1037/bul0000169},
  urldate = {2023-02-20},
  abstract = {[Correction Notice: An Erratum for this article was reported in Vol 145(7) of Psychological Bulletin (see record [rid]2019-32510-001[/rid]). In the article, the Open Science Framework (OSF) URL for the data has now been included in the author note. The online version of this article has been corrected.] Can recent failures to replicate psychological research be explained by typical magnitudes of statistical power, bias or heterogeneity? A large survey of 12,065 estimated effect sizes from 200 meta-analyses and nearly 8,000 papers is used to assess these key dimensions of replicability. First, our survey finds that psychological research is, on average, afflicted with low statistical power. The median of median power across these 200 areas of research is about 36\%, and only about 8\% of studies have adequate power (using Cohen's 80\% convention). Second, the median proportion of the observed variation among reported effect sizes attributed to heterogeneity is 74\% (I2). Heterogeneity of this magnitude makes it unlikely that the typical psychological study can be closely replicated when replication is defined as study-level null hypothesis significance testing. Third, the good news is that we find only a small amount of average residual reporting bias, allaying some of the often-expressed concerns about the reach of publication bias and questionable research practices. Nonetheless, the low power and high heterogeneity that our survey finds fully explain recent difficulties to replicate highly regarded psychological studies and reveal challenges for scientific progress in psychology. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  file = {C:\Users\u686785\Zotero\storage\4RAV4LEI\Stanley et al. - 2018 - What meta-analyses reveal about the replicability .pdf}
}

@article{stoevenbeltRRRJohnsSchmader2018,
  title = {{{RRR}}: {{Johns}}, {{Schmader}} and {{Martens}} (2005)},
  shorttitle = {{{RRR}}},
  author = {Stoevenbelt, Andrea H. and Wicherts, Jelte and Flore, Paulette and Torka, Ann-Kathrin and Gomez, Luis Angel},
  year = {2018},
  month = nov,
  publisher = {OSF},
  doi = {10.17605/OSF.IO/ZWNXY},
  urldate = {2024-06-26},
  abstract = {This is the Registered Replication Report project to replicate the stereotype threat study by Johns, Schmader, \& Martens (2005).      Hosted on the Open Science Framework},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\5NWA9Z9F\zwnxy.html}
}

@article{sundieMovingUnwiseReplication2019,
  title = {Moving beyond Unwise Replication Practices: {{The}} Case of Romantic Motivation},
  shorttitle = {Moving beyond Unwise Replication Practices},
  author = {Sundie, Jill M. and Beal, Daniel J. and Neuberg, Steven L. and Kenrick, Douglas T.},
  year = {2019},
  month = apr,
  journal = {Journal of Experimental Psychology: General},
  volume = {148},
  number = {4},
  pages = {e1-e11},
  publisher = {American Psychological Association},
  issn = {0096-3445},
  doi = {10.1037/xge0000527},
  urldate = {2023-02-20},
  abstract = {Replication research holds an increasingly important place in modern psychological science. If such work is to improve the state of knowledge rather than add confusion, however, replication attempts must be held to high standards of rigor. As an example of how replication attempts can add confusion rather than clarity, we consider an article by Shanks and colleagues (2015). They conducted a meta-analysis of studies examining romantic motivation, using problematic criteria for the inclusion of effects and reached conclusions of a null effect that were unwarranted. A more rigorous and defensible approach, relying on a representative analysis of effects and p-curves, suggests a different, more positive conclusion with no evidence of p-hacking. Shanks et al. also conducted several experiments that suffered from numerous issues, such as relying on inappropriate subject samples (e.g., older adults likely to be less sensitive to mating manipulations than college students used in previous research), altered research methods, and demonstrably weak manipulations, among other problems. We discuss the broader implications of this case, to illustrate both the opportunities and the pitfalls inherent in attempts to replicate contextually sensitive research. (PsycINFO Database Record (c) 2019 APA, all rights reserved)}
}

@article{suttonEmpiricalAssessmentEffect2000,
  title = {Empirical Assessment of Effect of Publication Bias on Meta-Analyses},
  author = {Sutton, A. J. and Duval, S. J. and Tweedie, R. L. and Abrams, K. R. and Jones, D. R.},
  year = {2000},
  month = jun,
  journal = {BMJ (Clinical research ed.)},
  volume = {320},
  number = {7249},
  pages = {1574--1577},
  issn = {0959-8138},
  doi = {10.1136/bmj.320.7249.1574},
  abstract = {OBJECTIVE: To assess the effect of publication bias on the results and conclusions of systematic reviews and meta-analyses. DESIGN: Analysis of published meta-analyses by trim and fill method. STUDIES: 48 reviews in Cochrane Database of Systematic Reviews that considered a binary endpoint and contained 10 or more individual studies. MAIN OUTCOME MEASURES: Number of reviews with missing studies and effect on conclusions of meta-analyses. RESULTS: The trim and fill fixed effects analysis method estimated that 26 (54\%) of reviews had missing studies and in 10 the number missing was significant. The corresponding figures with a random effects model were 23 (48\%) and eight. In four cases, statistical inferences regarding the effect of the intervention were changed after the overall estimate for publication bias was adjusted for. CONCLUSIONS: Publication or related biases were common within the sample of meta-analyses assessed. In most cases these biases did not affect the conclusions. Nevertheless, researchers should check routinely whether conclusions of systematic reviews are robust to possible non-random selection mechanisms.},
  langid = {english},
  pmcid = {PMC27401},
  pmid = {10845965},
  file = {C:\Users\u686785\Zotero\storage\BUDSVJ8I\Sutton et al. - 2000 - Empirical assessment of effect of publication bias.pdf}
}

@article{vacha-haaseReliabilityGeneralizationExploring1998,
  title = {Reliability {{Generalization}}: {{Exploring Variance}} in {{Measurement Error Affecting Score Reliability Across Studies}}},
  shorttitle = {Reliability {{Generalization}}},
  author = {{Vacha-Haase}, Tammi},
  year = {1998},
  month = feb,
  journal = {Educational and Psychological Measurement},
  volume = {58},
  number = {1},
  pages = {6--20},
  publisher = {SAGE Publications Inc},
  issn = {0013-1644},
  doi = {10.1177/0013164498058001002},
  urldate = {2024-04-29},
  abstract = {Because tests are not reliable, it is important to explore score reliability in virtually all studies. The present article proposes and illustrates a new method-reliability generalization-that can be used in a meta-analysis application similar to validity generalization. Reliability generalization characterizes (a) the typical reliability of scores for a given test across studies, (b) the amount of variability in reliability coefficients for given measures, and (c) the sources of variability in reliability coefficients across studies. The use of reliability generalization is illustrated here by analyzing 87 reliability coefficients reported for the two scales of the Bem Sex Role Inventory (BSRI).},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\Z98DQZFX\Vacha-Haase - 1998 - Reliability Generalization Exploring Variance in .pdf}
}

@article{vazireCredibilityReplicabilityImproving2022,
  title = {Credibility {{Beyond Replicability}}: {{Improving}} the {{Four Validities}} in {{Psychological Science}}},
  shorttitle = {Credibility {{Beyond Replicability}}},
  author = {Vazire, Simine and Schiavone, Sarah R. and Bottesini, Julia G.},
  year = {2022},
  month = apr,
  journal = {Current Directions in Psychological Science},
  volume = {31},
  number = {2},
  pages = {162--168},
  publisher = {SAGE Publications Inc},
  issn = {0963-7214},
  doi = {10.1177/09637214211067779},
  urldate = {2024-05-17},
  abstract = {Psychological science's ``credibility revolution'' has produced an explosion of metascientific work on improving research practices. Although much attention has been paid to replicability (reducing false positives), improving credibility depends on addressing a wide range of problems afflicting psychological science, beyond simply making psychology research more replicable. Here we focus on the ``four validities'' and highlight recent developments---many of which have been led by early-career researchers---aimed at improving these four validities in psychology research. We propose that the credibility revolution in psychology, which has its roots in replicability, can be harnessed to improve psychology's validity more broadly.},
  langid = {english}
}

@article{wanousEstimatingReliabilitySingleItem1996a,
  title = {Estimating the {{Reliability}} of a {{Single-Item Measure}}},
  author = {Wanous, John P. and Reichers, Arnon E.},
  year = {1996},
  month = apr,
  journal = {Psychological Reports},
  volume = {78},
  number = {2},
  pages = {631--634},
  publisher = {SAGE Publications Inc},
  issn = {0033-2941},
  doi = {10.2466/pr0.1996.78.2.631},
  urldate = {2024-06-26},
  abstract = {Single-item measures of employees' attitudes and beliefs are generally discouraged because their (internal consistency) reliability cannot be estimated. This results in the concern that reliability may be unacceptably low, particularly when compared to scales used to measure the same construct. A method for estimating the reliability of a single-item measure is demonstrated on original data that included both a single-item and a multiple-item measure of three constructs, namely, Over-all Job Satisfaction, Perceived Amount of Participation, and Desired Amount of Participation in decision-making. The average minimum estimated reliability for these single-item measures is .57; however, a realistic yet conservative estimate of their likely minimum reliability is at least .70.},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\YMETIND2\Wanous and Reichers - 1996 - Estimating the Reliability of a Single-Item Measur.pdf}
}

@article{wassersteinASAStatementPValues2016,
  title = {The {{ASA Statement}} on P-{{Values}}: {{Context}}, {{Process}}, and {{Purpose}}},
  shorttitle = {The {{ASA Statement}} on P-{{Values}}},
  author = {Wasserstein, Ronald L. and Lazar, Nicole A.},
  year = {2016},
  month = apr,
  journal = {The American Statistician},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  urldate = {2025-08-19},
  abstract = {Published in The American Statistician (Vol. 70, No. 2, 2016)},
  copyright = {{\copyright} 2016 American Statistical Association},
  langid = {english}
}

@article{wichertsDegreesFreedomPlanning2016a,
  title = {Degrees of {{Freedom}} in {{Planning}}, {{Running}}, {{Analyzing}}, and {{Reporting Psychological Studies}}: {{A Checklist}} to {{Avoid}} p-{{Hacking}}},
  shorttitle = {Degrees of {{Freedom}} in {{Planning}}, {{Running}}, {{Analyzing}}, and {{Reporting Psychological Studies}}},
  author = {Wicherts, Jelte M. and Veldkamp, Coosje L. S. and Augusteijn, Hilde E. M. and Bakker, Marjan and {van Aert}, Robbie C. M. and {van Assen}, Marcel A. L. M.},
  year = {2016},
  month = nov,
  journal = {Frontiers in Psychology},
  volume = {7},
  publisher = {Frontiers},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2016.01832},
  urldate = {2024-07-17},
  abstract = {{$<$}p{$>$}The designing, collecting, analyzing, and reporting of psychological studies entail many choices that are often arbitrary. The opportunistic use of these so-called researcher degrees of freedom aimed at obtaining statistically significant results is problematic because it enhances the chances of false positive results and may inflate effect size estimates. In this review article, we present an extensive list of 34 degrees of freedom that researchers have in formulating hypotheses, and in designing, running, analyzing, and reporting of psychological research. The list can be used in research methods education, and as a checklist to assess the quality of preregistrations and to determine the potential for bias due to (arbitrary) choices in unregistered studies.{$<$}/p{$>$}},
  langid = {english},
  keywords = {bias,found:start,method:proposal,pre-registration,transparency},
  file = {C:\Users\u686785\Zotero\storage\SI367BQW\Wicherts et al_2016_Degrees of Freedom in Planning, Running, Analyzing, and Reporting Psychological.pdf}
}

@article{wichertsWillingnessShareResearch2011a,
  title = {Willingness to {{Share Research Data Is Related}} to the {{Strength}} of the {{Evidence}} and the {{Quality}} of {{Reporting}} of {{Statistical Results}}},
  author = {Wicherts, Jelte M. and Bakker, Marjan and Molenaar, Dylan},
  year = {2011},
  month = nov,
  journal = {PLOS ONE},
  volume = {6},
  number = {11},
  pages = {e26828},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0026828},
  urldate = {2024-05-16},
  abstract = {Background The widespread reluctance to share published research data is often hypothesized to be due to the authors' fear that reanalysis may expose errors in their work or may produce conclusions that contradict their own. However, these hypotheses have not previously been studied systematically. Methods and Findings We related the reluctance to share research data for reanalysis to 1148 statistically significant results reported in 49 papers published in two major psychology journals. We found the reluctance to share data to be associated with weaker evidence (against the null hypothesis of no effect) and a higher prevalence of apparent errors in the reporting of statistical results. The unwillingness to share data was particularly clear when reporting errors had a bearing on statistical significance. Conclusions Our findings on the basis of psychological papers suggest that statistical results are particularly hard to verify when reanalysis is more likely to lead to contrasting conclusions. This highlights the importance of establishing mandatory data archiving policies.},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\CBZU7LEU\Wicherts et al. - 2011 - Willingness to Share Research Data Is Related to t.pdf}
}

@article{willrothBestLaidPlans2024,
  title = {Best {{Laid Plans}}: {{A Guide}} to {{Reporting Preregistration Deviations}}},
  shorttitle = {Best {{Laid Plans}}},
  author = {Willroth, Emily C. and Atherton, Olivia E.},
  year = {2024},
  month = jan,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {7},
  number = {1},
  pages = {25152459231213802},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/25152459231213802},
  urldate = {2024-03-21},
  abstract = {Psychological scientists are increasingly using preregistration as a tool to increase the credibility of research findings. Many of the benefits of preregistration rest on the assumption that preregistered plans are followed perfectly. However, research suggests that this is the exception rather than the norm, and there are many reasons why researchers may deviate from their preregistered plans. Preregistration can still be a valuable tool, even in the presence of deviations, as long as those deviations are well documented and transparently reported. Unfortunately, most preregistration deviations in psychology go unreported or are reported in unsystematic ways. In the current article, we offer a solution to this problem by providing a framework for transparent and standardized reporting of preregistration deviations, which was developed by drawing on our own experiences with preregistration, existing unpublished templates, feedback from colleagues and reviewers, and the results of a survey of 34 psychology-journal editors. This framework provides a clear template for what to do when things do not go as planned. We conclude by encouraging researchers to adopt this framework in their own preregistered research and by suggesting that journals implement structural policies around the transparent reporting of preregistration deviations.},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\WW645SU6\Willroth and Atherton - 2024 - Best Laid Plans A Guide to Reporting Preregistrat.pdf}
}

@article{willsonResearchTechniquesAERJ1980,
  title = {Research {{Techniques}} in {{AERJ Articles}}: 1969 to 1978},
  shorttitle = {Research {{Techniques}} in {{AERJ Articles}}},
  author = {WILLSON, VICTOR L.},
  year = {1980},
  month = jun,
  journal = {Educational Researcher},
  volume = {9},
  number = {6},
  pages = {5--10},
  publisher = {American Educational Research Association},
  issn = {0013-189X},
  doi = {10.3102/0013189X009006005},
  urldate = {2024-06-25},
  langid = {english},
  file = {C:\Users\u686785\Zotero\storage\T8Y6C9DS\WILLSON - 1980 - Research Techniques in AERJ Articles 1969 to 1978.pdf}
}

@article{yangEstimatingDeepReplicability2020,
  title = {Estimating the Deep Replicability of Scientific Findings Using Human and Artificial Intelligence},
  author = {Yang, Yang and Youyou, Wu and Uzzi, Brian},
  year = {2020},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {20},
  pages = {10762--10768},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1909046117},
  urldate = {2023-02-21},
  abstract = {Replicability tests of scientific papers show that the majority of papers fail replication. Moreover, failed papers circulate through the literature as quickly as replicating papers. This dynamic weakens the literature, raises research costs, and demonstrates the need for new approaches for estimating a study's replicability. Here, we trained an artificial intelligence model to estimate a paper's replicability using ground truth data on studies that had passed or failed manual replication tests, and then tested the model's generalizability on an extensive set of out-of-sample studies. The model predicts replicability better than the base rate of reviewers and comparably as well as prediction markets, the best present-day method for predicting replicability. In out-of-sample tests on manually replicated papers from diverse disciplines and methods, the model had strong accuracy levels of 0.65 to 0.78. Exploring the reasons behind the model's predictions, we found no evidence for bias based on topics, journals, disciplines, base rates of failure, persuasion words, or novelty words like ``remarkable'' or ``unexpected.'' We did find that the model's accuracy is higher when trained on a paper's text rather than its reported statistics and that n-grams, higher order word combinations that humans have difficulty processing, correlate with replication. We discuss how combining human and machine intelligence can raise confidence in research, provide research self-assessment techniques, and create methods that are scalable and efficient enough to review the ever-growing numbers of publications---a task that entails extensive human resources to accomplish with prediction markets and manual replication alone.},
  file = {C:\Users\u686785\Zotero\storage\8MMZ5RV9\Yang et al. - 2020 - Estimating the deep replicability of scientific fi.pdf}
}

@article{youyouDisciplinewideInvestigationReplicability2023,
  title = {A Discipline-Wide Investigation of the Replicability of {{Psychology}} Papers over the Past Two Decades},
  author = {Youyou, Wu and Yang, Yang and Uzzi, Brian},
  year = {2023},
  month = feb,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {120},
  number = {6},
  pages = {e2208863120},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2208863120},
  urldate = {2023-02-21},
  abstract = {Conjecture about the weak replicability in social sciences has made scholars eager to quantify the scale and scope of replication failure for a discipline. Yet small-scale manual replication methods alone are ill-suited to deal with this big data problem. Here, we conduct a discipline-wide replication census in science. Our sample (N = 14,126 papers) covers nearly all papers published in the six top-tier Psychology journals over the past 20 y. Using a validated machine learning model that estimates a paper's likelihood of replication, we found evidence that both supports and refutes speculations drawn from a relatively small sample of manual replications. First, we find that a single overall replication rate of Psychology poorly captures the varying degree of replicability among subfields. Second, we find that replication rates are strongly correlated with research methods in all subfields. Experiments replicate at a significantly lower rate than do non-experimental studies. Third, we find that authors' cumulative publication number and citation impact are positively related to the likelihood of replication, while other proxies of research quality and rigor, such as an author's university prestige and a paper's citations, are unrelated to replicability. Finally, contrary to the ideal that media attention should cover replicable research, we find that media attention is positively related to the likelihood of replication failure. Our assessments of the scale and scope of replicability are important next steps toward broadly resolving issues of replicability.}
}

@article{yuanRobusinessNormalTheoryBased2002,
  title = {On {{Robusiness}} of the {{Normal-Theory Based Asymptotic Distributions}} of {{Three Reliability Coefficient Estimates}}},
  author = {Yuan, Ke-Hai and Bentler, Peter M.},
  year = {2002},
  month = jun,
  journal = {Psychometrika},
  volume = {67},
  number = {2},
  pages = {251--259},
  issn = {0033-3123, 1860-0980},
  doi = {10.1007/BF02294845},
  urldate = {2025-08-12},
  abstract = {This paper studies the asymptotic distributions of three reliability coefficient estimates: Sample coefficient alpha, the reliability estimate of a composite score following a factor analysis, and the estimate of the maximal reliability of a linear combination of item scores following a factor analysis. Results indicate that the asymptotic distribution for each of the coefficient estimates, obtained based on a normal sampling distribution, is still valid within a large class of nonnormal distributions. Therefore, a formula for calculating the standard error of the sample coefficient alpha, recently obtained by van Zyl, Neudecker and Nel, applies to other reliability coefficients and can still be used even with skewed and kurtotic data such as are typical in the social and behavioral sciences.},
  langid = {english},
  keywords = {omega SE}
}

@misc{ZenodoOpenData,
  title = {Zenodo Open Data Repository ({{CERN}})},
  journal = {European University Institute},
  urldate = {2024-05-23},
  howpublished = {https://www.eui.eu/Research/Library/ResearchGuides/Economics/Statistics/DataPortal/Zenodo},
  langid = {british}
}

@article{zhangMetaanalysisCorrelationCoefficients2022,
  title = {Meta-Analysis of Correlation Coefficients: {{A}} Cautionary Tale on Treating Measurement Error},
  shorttitle = {Meta-Analysis of Correlation Coefficients},
  author = {Zhang, Qian},
  year = {2022},
  month = may,
  journal = {Psychological Methods},
  publisher = {American Psychological Association},
  issn = {1082-989X},
  doi = {10.1037/met0000498},
  urldate = {2023-02-22},
  abstract = {A scale to measure a psychological construct is subject to measurement error. When meta-analyzing correlations obtained from scale scores, many researchers recommend correcting for measurement error. I considered three caveats when correcting for measurement error in meta-analysis of correlations: (a) the distribution of true scores can be non-normal, resulting in violation of the normality assumption for raw correlations and Fisher's z transformed correlations; (b) coefficient alpha is often used as the reliability, but correlations corrected for measurement error using alpha can be inaccurate when some assumptions of alpha (e.g., tau-equivalence) are violated; and (c) item scores are often ordinal, making the disattenuation formula potentially problematic. Via three simulation studies, I examined the performance of two meta-analysis approaches---with raw correlations and z scores. In terms of estimation accuracy and coverage probability of the mean correlation, results showed that (a) considering the true-score distribution alone, estimation of the mean correlation was slightly worse when true scores of the constructs were skewed rather than normal; (b) when the tau-equivalence assumption was violated and coefficient alpha was used for correcting measurement error, the mean correlation estimates can be biased and coverage probabilities can be low; and (c) discretization of continuous items can result in biased estimates and undercoverage of the mean correlations even when tau-equivalence was satisfied. With more categories and/or items on a scale, results can improve whether tau-equivalence was met or not. Based on these findings, I gave recommendations for conducting meta-analyses of correlations. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  file = {C:\Users\u686785\Zotero\storage\WELST2NX\Zhang - 2022 - Meta-analysis of correlation coefficients A cauti.pdf}
}

@article{zhangMetaanalysisCorrelationCoefficients2024,
  title = {Meta-Analysis of Correlation Coefficients: {{A}} Cautionary Tale on Treating Measurement Error},
  shorttitle = {Meta-Analysis of Correlation Coefficients},
  author = {Zhang, Qian},
  year = {2024},
  month = apr,
  journal = {Psychological Methods},
  volume = {29},
  number = {2},
  pages = {308--330},
  issn = {1939-1463},
  doi = {10.1037/met0000498},
  abstract = {A scale to measure a psychological construct is subject to measurement error. When meta-analyzing correlations obtained from scale scores, many researchers recommend correcting for measurement error. I considered three caveats when correcting for measurement error in meta-analysis of correlations: (a) the distribution of true scores can be non-normal, resulting in violation of the normality assumption for raw correlations and Fisher's z transformed correlations; (b) coefficient alpha is often used as the reliability, but correlations corrected for measurement error using alpha can be inaccurate when some assumptions of alpha (e.g., tau-equivalence) are violated; and (c) item scores are often ordinal, making the disattenuation formula potentially problematic. Via three simulation studies, I examined the performance of two meta-analysis approaches-with raw correlations and z scores. In terms of estimation accuracy and coverage probability of the mean correlation, results showed that (a) considering the true-score distribution alone, estimation of the mean correlation was slightly worse when true scores of the constructs were skewed rather than normal; (b) when the tau-equivalence assumption was violated and coefficient alpha was used for correcting measurement error, the mean correlation estimates can be biased and coverage probabilities can be low; and (c) discretization of continuous items can result in biased estimates and undercoverage of the mean correlations even when tau-equivalence was satisfied. With more categories and/or items on a scale, results can improve whether tau-equivalence was met or not. Based on these findings, I gave recommendations for conducting meta-analyses of correlations. (PsycInfo Database Record (c) 2024 APA, all rights reserved).},
  langid = {english},
  pmid = {35604700},
  file = {C:\Users\u686785\Zotero\storage\SFKZWEKI\Zhang - 2024 - Meta-analysis of correlation coefficients A cauti.pdf}
}

@article{zogmaisterAssessingTransparencyMethods2024,
  title = {Assessing the {{Transparency}} of {{Methods}} in {{Scientific Reporting}}},
  author = {Zogmaister, Cristina and Vezzoli, Michela and Facchin, Alessio and Conte, Federica Paola and Rizzi, Ezia and Giaquinto, Francesco and Cavicchiolo, Elisa and Fusco, Gabriele and Pegoraro, Sara and Simioni, Maura},
  editor = {van Ravenzwaaij, Don},
  year = {2024},
  month = aug,
  journal = {Collabra: Psychology},
  volume = {10},
  number = {1},
  pages = {121243},
  issn = {2474-7394},
  doi = {10.1525/collabra.121243},
  urldate = {2025-08-19},
  abstract = {Enhancing transparency in scientific reports is crucial to foster trust, facilitate reproducibility, and ensure the integrity of research, ultimately advancing the progress of knowledge and innovation. To devise strategies for enhancing transparency in scientific reports, the initial step is to assess the current state: to objectively measure the current level of transparency, identifying its shortcomings and associated factors, and to gauge improvements, for instance, following interventions. Here we present a new tool and a proof of concept to this endeavor.Using a checklist, we evaluated the methods transparency of a corpus of 180 papers published in 2011 and 2021 in five top-tier psychology journals. We specifically focused on the materials, procedures, and characteristics of population and sample. We summarized the level of transparency in the methods of each paper with the Transparency Of Methods (TOM) score. This score consists of the proportion of relevant information regarding the method that is available to the reader of a scientific report, either in the main text of the paper, or the appendixes, supplementary materials, and online open repository. It ranges from 0 (i.e., no transparency in the relevant aspects of the methods and materials) to 1 (i.e., the scientific report is fully transparent in all the relevant materials and methods).The results affirm TOM's potential for assessing the transparency of scientific reporting and offer two snapshots of transparency in the methods of published papers, a decade apart. While they indicate progress has been made, there remains room for further enhancements and highlight specific areas that require attention.In conclusion, this work underscores the ongoing need for improvement in methods' transparency and introduces a valuable tool, demonstrating its applicability as a means to evaluate the transparency of scientific reports.},
  file = {C:\Users\u686785\Zotero\storage\ZGS3IIM8\Zogmaister et al_2024_Assessing the Transparency of Methods in Scientific Reporting.pdf}
}
